{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ee2b575-1a0f-4d68-8702-b2a1812ef9c7",
   "metadata": {},
   "source": [
    "### Day 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc444f36-d592-405e-8acd-6140020fb430",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f402b900-ca23-4951-9c20-1f1d5cfeeb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://codeload.github.com/DataTalksClub/faq/zip/refs/heads/main'\n",
    "resp = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfce6e13-33d0-4122-a890-1a77c5eb9d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    # Only process markdown files\n",
    "    if not filename.endswith('.md'):\n",
    "        continue\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea40a8d8-0b45-43f4-81bc-2524d3d3a0c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1228\n",
      "{'content': '# DataTalks.Club FAQ\\n\\nA static site generator for DataTalks.Club course FAQs with automated AI-powered FAQ maintenance.\\n\\n## Features\\n\\n- **Static Site Generation**: Converts markdown FAQs to a beautiful, searchable HTML site\\n- **Automated FAQ Management**: AI-powered bot that processes new FAQ proposals\\n- **Intelligent Triage**: Automatically determines if proposals should create new entries, update existing ones, or are duplicates\\n- **GitHub Integration**: Seamless workflow via GitHub Issues and Pull Requests\\n\\n## Project Structure\\n\\n```\\nfaq/\\n├── _questions/              # FAQ content organized by course\\n│   ├── machine-learning-zoomcamp/\\n│   │   ├── _metadata.yaml   # Course configuration\\n│   │   ├── general/         # General course questions\\n│   │   ├── module-1/        # Module-specific questions\\n│   │   └── ...\\n│   ├── data-engineering-zoomcamp/\\n│   └── ...\\n├── _layouts/                # Jinja2 HTML templates\\n│   ├── base.html\\n│   ├── course.html\\n│   └── index.html\\n├── assets/                  # CSS and static assets\\n├── faq_automation/          # FAQ automation module\\n│   ├── core.py             # Core FAQ processing functions\\n│   ├── rag_agent.py        # AI-powered decision agent\\n│   ├── actions.py          # GitHub Actions integration\\n│   └── cli.py              # Command-line interface\\n├── tests/                   # Test suite\\n├── generate_website.py      # Main site generator\\n└── Makefile                # Build commands\\n```\\n\\n## Contributing FAQ Entries\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions.\\n\\n\\n## Development\\n\\n### Setup\\n\\n```bash\\n# Install dependencies\\nuv sync --dev\\n```\\n\\nFor testing the FAQ automation locally, you\\'ll need to set your OpenAI API key:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key-here\\'\\n```\\n\\nOr add it to your shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`).\\n\\n### Running Locally\\n\\nTo test the FAQ automation locally, create a `test_issue.txt` file:\\n\\n```bash\\ncat > test_issue.txt << \\'EOF\\'\\n### Course\\nmachine-learning-zoomcamp\\n\\n### Question\\nHow do I check my Python version?\\n\\n### Answer\\nRun `python --version` in your terminal.\\nEOF\\n```\\n\\nThen process the FAQ proposal:\\n\\n```bash\\nuv run python -m faq_automation.cli \\\\\\n  --issue-body \"$(cat test_issue.txt)\" \\\\\\n  --issue-number 42\\n```\\n\\n### Testing\\n\\n```bash\\n# Generate static website\\nmake website\\n\\n# Run all tests\\nmake test\\n\\n# Run unit tests only\\nmake test-unit\\n\\n# Run integration tests only\\nmake test-int\\n```\\n\\nSee [testing documentation](tests/README.md) for detailed information about the test suite, including how to run specific test files or methods, test coverage details, and guidelines for adding new tests.\\n\\n## Architecture\\n\\n### Site Generation Pipeline\\n\\n1. **Collection** (`collect_questions()`):\\n   - Reads all markdown files from `_questions/`\\n   - Parses YAML frontmatter\\n   - Loads course metadata for section ordering\\n\\n2. **Processing** (`process_markdown()`):\\n   - Converts markdown to HTML\\n   - Applies syntax highlighting (Pygments)\\n   - Auto-links plain text URLs\\n   - Handles image placeholders\\n\\n3. **Sorting** (`sort_sections_and_questions()`):\\n   - Orders sections per `_metadata.yaml`\\n   - Sorts questions by `sort_order` field\\n\\n4. **Rendering** (`generate_site()`):\\n   - Applies Jinja2 templates\\n   - Generates course pages and index\\n   - Copies assets to `_site/`', 'filename': 'faq-main/readme.md'}\n"
     ]
    }
   ],
   "source": [
    "print(len(repository_data))\n",
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09e19ccf-9502-40e4-bd83-7e2c25d98eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "repository_data = []\n",
    "\n",
    "# Create a ZipFile object from the downloaded content\n",
    "zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "\n",
    "for file_info in zf.infolist():\n",
    "    filename = file_info.filename.lower()\n",
    "\n",
    "    if not (filename.endswith('.md') or filename.endswith('.mdx')):\n",
    "        continue\n",
    "\n",
    "\n",
    "    # Read and parse each file\n",
    "    with zf.open(file_info) as f_in:\n",
    "        content = f_in.read()\n",
    "        post = frontmatter.loads(content)\n",
    "        data = post.to_dict()\n",
    "        data['filename'] = filename\n",
    "        repository_data.append(data)\n",
    "\n",
    "zf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e38e257b-84e2-474b-b67d-1acff3f7d9ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '# DataTalks.Club FAQ\\n\\nA static site generator for DataTalks.Club course FAQs with automated AI-powered FAQ maintenance.\\n\\n## Features\\n\\n- **Static Site Generation**: Converts markdown FAQs to a beautiful, searchable HTML site\\n- **Automated FAQ Management**: AI-powered bot that processes new FAQ proposals\\n- **Intelligent Triage**: Automatically determines if proposals should create new entries, update existing ones, or are duplicates\\n- **GitHub Integration**: Seamless workflow via GitHub Issues and Pull Requests\\n\\n## Project Structure\\n\\n```\\nfaq/\\n├── _questions/              # FAQ content organized by course\\n│   ├── machine-learning-zoomcamp/\\n│   │   ├── _metadata.yaml   # Course configuration\\n│   │   ├── general/         # General course questions\\n│   │   ├── module-1/        # Module-specific questions\\n│   │   └── ...\\n│   ├── data-engineering-zoomcamp/\\n│   └── ...\\n├── _layouts/                # Jinja2 HTML templates\\n│   ├── base.html\\n│   ├── course.html\\n│   └── index.html\\n├── assets/                  # CSS and static assets\\n├── faq_automation/          # FAQ automation module\\n│   ├── core.py             # Core FAQ processing functions\\n│   ├── rag_agent.py        # AI-powered decision agent\\n│   ├── actions.py          # GitHub Actions integration\\n│   └── cli.py              # Command-line interface\\n├── tests/                   # Test suite\\n├── generate_website.py      # Main site generator\\n└── Makefile                # Build commands\\n```\\n\\n## Contributing FAQ Entries\\n\\nSee [CONTRIBUTING.md](CONTRIBUTING.md) for detailed instructions.\\n\\n\\n## Development\\n\\n### Setup\\n\\n```bash\\n# Install dependencies\\nuv sync --dev\\n```\\n\\nFor testing the FAQ automation locally, you\\'ll need to set your OpenAI API key:\\n\\n```bash\\nexport OPENAI_API_KEY=\\'your-api-key-here\\'\\n```\\n\\nOr add it to your shell configuration file (e.g., `~/.bashrc`, `~/.zshrc`).\\n\\n### Running Locally\\n\\nTo test the FAQ automation locally, create a `test_issue.txt` file:\\n\\n```bash\\ncat > test_issue.txt << \\'EOF\\'\\n### Course\\nmachine-learning-zoomcamp\\n\\n### Question\\nHow do I check my Python version?\\n\\n### Answer\\nRun `python --version` in your terminal.\\nEOF\\n```\\n\\nThen process the FAQ proposal:\\n\\n```bash\\nuv run python -m faq_automation.cli \\\\\\n  --issue-body \"$(cat test_issue.txt)\" \\\\\\n  --issue-number 42\\n```\\n\\n### Testing\\n\\n```bash\\n# Generate static website\\nmake website\\n\\n# Run all tests\\nmake test\\n\\n# Run unit tests only\\nmake test-unit\\n\\n# Run integration tests only\\nmake test-int\\n```\\n\\nSee [testing documentation](tests/README.md) for detailed information about the test suite, including how to run specific test files or methods, test coverage details, and guidelines for adding new tests.\\n\\n## Architecture\\n\\n### Site Generation Pipeline\\n\\n1. **Collection** (`collect_questions()`):\\n   - Reads all markdown files from `_questions/`\\n   - Parses YAML frontmatter\\n   - Loads course metadata for section ordering\\n\\n2. **Processing** (`process_markdown()`):\\n   - Converts markdown to HTML\\n   - Applies syntax highlighting (Pygments)\\n   - Auto-links plain text URLs\\n   - Handles image placeholders\\n\\n3. **Sorting** (`sort_sections_and_questions()`):\\n   - Orders sections per `_metadata.yaml`\\n   - Sorts questions by `sort_order` field\\n\\n4. **Rendering** (`generate_site()`):\\n   - Applies Jinja2 templates\\n   - Generates course pages and index\\n   - Copies assets to `_site/`', 'filename': 'faq-main/readme.md'}\n"
     ]
    }
   ],
   "source": [
    "print(repository_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58357ddf-d1bb-4f97-86e1-ab893215949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "import requests\n",
    "import frontmatter\n",
    "\n",
    "def read_repo_data(repo_owner, repo_name):\n",
    "    \"\"\"\n",
    "    Download and parse all markdown files from a GitHub repository.\n",
    "    \n",
    "    Args:\n",
    "        repo_owner: GitHub username or organization\n",
    "        repo_name: Repository name\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries containing file content and metadata\n",
    "    \"\"\"\n",
    "    prefix = 'https://codeload.github.com' \n",
    "    url = f'{prefix}/{repo_owner}/{repo_name}/zip/refs/heads/main'\n",
    "    resp = requests.get(url)\n",
    "    \n",
    "    if resp.status_code != 200:\n",
    "        raise Exception(f\"Failed to download repository: {resp.status_code}\")\n",
    "\n",
    "    repository_data = []\n",
    "    zf = zipfile.ZipFile(io.BytesIO(resp.content))\n",
    "    \n",
    "    for file_info in zf.infolist():\n",
    "        filename = file_info.filename\n",
    "        filename_lower = filename.lower()\n",
    "\n",
    "        if not (filename_lower.endswith('.md') \n",
    "            or filename_lower.endswith('.mdx')):\n",
    "            continue\n",
    "    \n",
    "        try:\n",
    "            with zf.open(file_info) as f_in:\n",
    "                content = f_in.read().decode('utf-8', errors='ignore')\n",
    "                post = frontmatter.loads(content)\n",
    "                data = post.to_dict()\n",
    "                data['filename'] = filename\n",
    "                repository_data.append(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    zf.close()\n",
    "    return repository_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e72204ea-e62a-40df-9d9d-08f48aa17a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 1228\n",
      "<class 'list'> 95\n",
      "dict_keys(['content', 'filename'])\n"
     ]
    }
   ],
   "source": [
    "dtc_faq = read_repo_data(\"DataTalksClub\", \"faq\")\n",
    "evidently_docs = read_repo_data(\"evidentlyai\", \"docs\")\n",
    "\n",
    "print(type(dtc_faq), len(dtc_faq))\n",
    "print(type(evidently_docs), len(evidently_docs))\n",
    "print(dtc_faq[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55ad7016-8cfe-4175-8b11-de962bc86435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '1b03f3dadf', 'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine', 'sort_order': 45, 'content': 'This [tutorial](https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine?hl=en) shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/045_1b03f3dadf_set-up-chrome-remote-desktop-for-linux-on-compute.md'}\n"
     ]
    }
   ],
   "source": [
    "doc_45 = dtc_faq[45]\n",
    "\n",
    "print(doc_45)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6179e644-0145-4b88-b957-a1f433216c42",
   "metadata": {},
   "source": [
    "### Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "751426d8-8d81-4d63-b409-7de206bb647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(seq, size, step):\n",
    "    if size <= 0 or step <= 0:\n",
    "        raise ValueError(\"size and step must be positive\")\n",
    "\n",
    "    n = len(seq)\n",
    "    result = []\n",
    "    for i in range(0, n, step):\n",
    "        chunk = seq[i:i+size]\n",
    "        result.append({'start': i, 'chunk': chunk})\n",
    "        if i + size >= n:\n",
    "            break\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3135533-d86d-4b7b-8e23-ae55bba83b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    chunks = sliding_window(doc_content, 2000, 1000)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        chunk_dict = {\n",
    "            \"chunk\": ch,          # ch is the string\n",
    "            \"chunk_id\": i,\n",
    "            **doc_copy\n",
    "        }\n",
    "        evidently_chunks.append(chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10ba8180-96ec-4700-9d75-2b9943a056fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "text = evidently_docs[45]['content']\n",
    "paragraphs = re.split(r\"\\n\\s*\\n\", text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d42e672b-281b-40d1-b8bf-08803a2c99cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_markdown_by_level(text, level=2):\n",
    "    \"\"\"\n",
    "    Split markdown text by a specific header level.\n",
    "    \n",
    "    :param text: Markdown text as a string\n",
    "    :param level: Header level to split on\n",
    "    :return: List of sections as strings\n",
    "    \"\"\"\n",
    "    # This regex matches markdown headers\n",
    "    # For level 2, it matches lines starting with \"## \"\n",
    "    header_pattern = r'^(#{' + str(level) + r'} )(.+)$'\n",
    "    pattern = re.compile(header_pattern, re.MULTILINE)\n",
    "\n",
    "    # Split and keep the headers\n",
    "    parts = pattern.split(text)\n",
    "    \n",
    "    sections = []\n",
    "    for i in range(1, len(parts), 3):\n",
    "        # We step by 3 because regex.split() with\n",
    "        # capturing groups returns:\n",
    "        # [before_match, group1, group2, after_match, ...]\n",
    "        # here group1 is \"## \", group2 is the header text\n",
    "        header = parts[i] + parts[i+1]  # \"## \" + \"Title\"\n",
    "        header = header.strip()\n",
    "\n",
    "        # Get the content after this header\n",
    "        content = \"\"\n",
    "        if i+2 < len(parts):\n",
    "            content = parts[i+2].strip()\n",
    "\n",
    "        if content:\n",
    "            section = f'{header}\\n\\n{content}'\n",
    "        else:\n",
    "            section = header\n",
    "        sections.append(section)\n",
    "    \n",
    "    return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5987c5ef-984a-4aab-bb22-34f8053daece",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections = split_markdown_by_level(text, level=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e23b6567-be37-4bf8-b0cc-90e1a42d6c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidently_chunks = []\n",
    "\n",
    "for doc in evidently_docs:\n",
    "    doc_copy = doc.copy()\n",
    "    doc_content = doc_copy.pop('content')\n",
    "    sections = split_markdown_by_level(doc_content, level=2)\n",
    "    for i, section in enumerate(sections):\n",
    "        section_doc = doc_copy.copy()\n",
    "        section_doc[\"chunk\"] = section      # ✅ now you have chunk\n",
    "        section_doc[\"chunk_id\"] = i\n",
    "        section_doc[\"section\"] = section    # optional: keep old name too\n",
    "        evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f14b6f7-9e98-46c4-ae73-8abb024d0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# openai_client = OpenAI()\n",
    "\n",
    "\n",
    "# def llm(prompt, model='gpt-4o-mini'):\n",
    "#     messages = [\n",
    "#         {\"role\": \"user\", \"content\": prompt}\n",
    "#     ]\n",
    "\n",
    "#     response = openai_client.responses.create(\n",
    "#         model='gpt-4o-mini',\n",
    "#         input=messages\n",
    "#     )\n",
    "\n",
    "#     return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2d65778-67fa-4781-bc76-4a8207dd2971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt_template = \"\"\"\n",
    "# Split the provided document into logical sections\n",
    "# that make sense for a Q&A system.\n",
    "\n",
    "# Each section should be self-contained and cover\n",
    "# a specific topic or concept.\n",
    "\n",
    "# <DOCUMENT>\n",
    "# {document}\n",
    "# </DOCUMENT>\n",
    "\n",
    "# Use this format:\n",
    "\n",
    "# ## Section Name\n",
    "\n",
    "# Section content with all relevant details\n",
    "\n",
    "# ---\n",
    "\n",
    "# ## Another Section Name\n",
    "\n",
    "# Another section content\n",
    "\n",
    "# ---\n",
    "# \"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8432e03c-2700-4cbe-8840-05519ac0a01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def intelligent_chunking(text):\n",
    "#     prompt = prompt_template.format(document=text)\n",
    "#     response = llm(prompt)\n",
    "#     sections = response.split('---')\n",
    "#     sections = [s.strip() for s in sections if s.strip()]\n",
    "#     return sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6ce7e34-daf3-4db8-8173-7ae9b0bc555e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# evidently_chunks = []\n",
    "\n",
    "# for doc in tqdm(evidently_docs):\n",
    "#     doc_copy = doc.copy()\n",
    "#     doc_content = doc_copy.pop('content')\n",
    "\n",
    "#     sections = intelligent_chunking(doc_content)\n",
    "#     for section in sections:\n",
    "#         section_doc = doc_copy.copy()\n",
    "#         section_doc['section'] = section\n",
    "#         evidently_chunks.append(section_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d161fba-28fd-43d4-a1fd-06889eddc7f7",
   "metadata": {},
   "source": [
    "### Day 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bec96823-4f85-4fa5-b76b-f6b219e0286e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x107825fa0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import Index\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"chunk\", \"title\", \"description\", \"filename\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "index.fit(evidently_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db4df452-842f-48a3-b72b-0b10cf7c1742",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What should be in a test dataset for AI evaluation?'\n",
    "results = index.search(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c15bd3f-c1db-48fc-a6c5-7d84f3fa270a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x1078dbec0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc_faq = read_repo_data('DataTalksClub', 'faq')\n",
    "\n",
    "de_dtc_faq = [d for d in dtc_faq if 'data-engineering' in d['filename']]\n",
    "\n",
    "faq_index = Index(\n",
    "    text_fields=[\"question\", \"content\"],\n",
    "    keyword_fields=[]\n",
    ")\n",
    "\n",
    "faq_index.fit(de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9bd2de2b-4217-45af-b589-90794cb43ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': '9e508f2212',\n",
       "  'question': 'Course: When does the course start?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'},\n",
       " {'id': 'bfafa427b3',\n",
       "  'question': 'Course: What are the prerequisites for this course?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'To get the most out of this course, you should have:\\n\\n- Basic coding experience\\n- Familiarity with SQL\\n- Experience with Python (helpful but not required)\\n\\nNo prior data engineering experience is necessary. See [Readme on GitHub](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/README.md#prerequisites).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/002_bfafa427b3_course-what-are-the-prerequisites-for-this-course.md'},\n",
       " {'id': '3f1424af17',\n",
       "  'question': 'Course: Can I still join the course after the start date?',\n",
       "  'sort_order': 3,\n",
       "  'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'},\n",
       " {'id': '52217fc51b',\n",
       "  'question': 'Course: I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       "  'sort_order': 4,\n",
       "  'content': \"You don't need a confirmation email. You're accepted. You can start learning and submitting homework without registering. Registration was just to gauge interest before the start date.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/004_52217fc51b_course-i-have-registered-for-the-data-engineering.md'},\n",
       " {'id': '33fc260cd8',\n",
       "  'question': 'Course: What can I do before the course starts?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'},\n",
       " {'id': 'b71fb3b195',\n",
       "  'question': 'Course: how many Zoomcamps in a year?',\n",
       "  'sort_order': 6,\n",
       "  'content': \"There are multiple Zoomcamps in a year, as of 2025. More info at [DTC Article](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\nThey are five separate courses, estimated to be during these months:\\n\\n- **Data-Engineering**: Jan - Apr\\n- **Stock Market Analytics**: Apr - May\\n- **MLOps**: May - Aug\\n- **LLM**: June - Sep\\n- **Machine Learning**: Sep - Jan\\n\\nThere's only one Data-Engineering Zoomcamp “live” cohort per year for the certification, similar to the other Zoomcamps. They follow pretty much the same schedule for each cohort. For Data-Engineering, it is generally from Jan-Apr of the year. \\n\\nIf you’re not interested in the Certificate, you can take any Zoomcamp at any time, at your own pace, out of sync with any “live” cohort.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/006_b71fb3b195_course-how-many-zoomcamps-in-a-year.md'},\n",
       " {'id': 'e499535e82',\n",
       "  'question': 'Course: Is the current cohort going to be different from the previous cohort?',\n",
       "  'sort_order': 7,\n",
       "  'content': 'For the 2025 edition, we are using Kestra (see [Demo](https://www.youtube.com/watch?v=R0JAFvDCmSY)) instead of MageAI (Module 2). Look out for new videos. See [Playlist](https://www.youtube.com/playlist?list=PLEK3H8YwZn1oPPShk2p5k3E9vO-gPnUCf).\\n\\nFor the 2024 edition, we used Mage AI instead of Prefect and re-recorded the Terraform videos. For 2023, we used Prefect instead of Airflow. See playlists on YouTube and the [cohorts folder in the GitHub repo](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/cohorts).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/007_e499535e82_course-is-the-current-cohort-going-to-be-different.md'},\n",
       " {'id': '068529125b',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'},\n",
       " {'id': 'c207b8614e',\n",
       "  'question': 'Course: Can I get support if I take the course in the self-paced mode?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'},\n",
       " {'id': '8ac65b2225',\n",
       "  'question': 'Course: Which playlist on YouTube should I refer to?',\n",
       "  'sort_order': 10,\n",
       "  'content': 'All the main videos are stored in the \"DATA ENGINEERING ZOOMCAMP\" main playlist. The GitHub repository is updated to include each video with a thumbnail, linking directly to the relevant playlist.\\n\\nRefer to the Main Playlist for the core content, and then check specific year playlists for additional videos such as office hours.\\n\\n- [Data Engineering Zoomcamp](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb)\\n- [Data Engineering Zoomcamp 2022](https://www.youtube.com/playlist?list=PL3MmuxUbc_hKVX8VnwWCPaWlIHf1qmg8s)\\n- [Data Engineering Zoomcamp 2023](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJjEePXIdE-LVUx_1ZZjYGW)\\n- Data Engineering Bootcamp 2024\\n- [Data Engineering Bootcamp 2025](https://www.youtube.com/playlist?list=PL3MmuxUbc_hJZdpLpRHp7dg6EOx828q6y)\\n- [DE Zoomcamp 2025 (Module 2 Kestra)](https://www.youtube.com/playlist?list=PLEK3H8YwZn1oPPShk2p5k3E9vO-gPnUCf)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/010_8ac65b2225_course-which-playlist-on-youtube-should-i-refer-to.md'},\n",
       " {'id': '316180784f',\n",
       "  'question': 'Course: How many hours per week am I expected to spend on this course?',\n",
       "  'sort_order': 11,\n",
       "  'content': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week.\\n\\nYou can also calculate it yourself using [this data](https://github.com/DataTalksClub/zoomcamp-analytics/tree/main/data/de-zoomcamp-2023) and then update this answer.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/011_316180784f_course-how-many-hours-per-week-am-i-expected-to-sp.md'},\n",
       " {'id': 'a411de5004',\n",
       "  'question': 'Office Hours: I can’t attend the “Office hours” / workshop, will it be recorded?',\n",
       "  'sort_order': 12,\n",
       "  'content': 'Yes! Every \"Office Hours\" will be recorded and available a few minutes after the live session is over; so you can view (or rewatch) whenever you want.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/012_a411de5004_office-hours-i-cant-attend-the-office-hours-worksh.md'},\n",
       " {'id': '16005581f2',\n",
       "  'question': 'Edit Course Profile.',\n",
       "  'sort_order': 13,\n",
       "  'content': 'The display name listed on the leaderboard is an auto-generated randomized name. You can edit it to be a nickname or your real name if you prefer. Your entry on the Leaderboard is the one highlighted in light green.\\n\\nThe Certificate name should be your actual name that you want to appear on your certificate after completing the course.\\n\\nThe \"Display on Leaderboard\" option indicates whether you want your name to be listed on the course leaderboard.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/013_16005581f2_edit-course-profile.md'},\n",
       " {'id': '3774a79c13',\n",
       "  'question': 'Certificate: Do I need to do the homeworks to get the certificate?',\n",
       "  'sort_order': 14,\n",
       "  'content': 'No, as long as you complete the peer-reviewed capstone projects on time, you can receive the certificate. You do not need to do the homeworks if you join late, for example.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md'},\n",
       " {'id': '900f60fd25',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md'},\n",
       " {'id': 'd3f485cd10',\n",
       "  'question': 'Homework: What are homework and project deadlines?',\n",
       "  'sort_order': 16,\n",
       "  'content': '2025 deadlines will be announced on [the course website](https://courses.datatalks.club/de-zoomcamp-2025/) and in [Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ).\\n\\nYou can find the 2024 deadlines here: [2024 Deadlines Spreadsheet](https://docs.google.com/spreadsheets/d/e/2PACX-1vQACMLuutV5rvXg5qICuJGL-yZqIV0FBD84CxPdC5eZHf8TfzB-CJT_3Mo7U7oGVTXmSihPgQxuuoku/pubhtml).\\n\\nAlso, take note of announcements from @Au-Tomator for any extensions or other news. The form may also show the updated deadline if the instructor(s) have updated it.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/016_d3f485cd10_homework-what-are-homework-and-project-deadlines.md'},\n",
       " {'id': '4dbd2eea47',\n",
       "  'question': 'Homework: Are late submissions of homework allowed?',\n",
       "  'sort_order': 17,\n",
       "  'content': 'No, late submissions are not allowed. However, if the form is still open after the due date, you can still submit the homework. Confirm your submission by checking the date-timestamp on the Course page. Ensure you are logged in.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/017_4dbd2eea47_homework-are-late-submissions-of-homework-allowed.md'},\n",
       " {'id': '9d89b52976',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_eaa6f559.png'}],\n",
       "  'question': 'Homework: What is the homework URL in the homework link?',\n",
       "  'sort_order': 19,\n",
       "  'content': '<{IMAGE:image_1}>\\n\\nAnswer: In short, it’s your repository on GitHub, GitLab, Bitbucket, etc.\\n\\nIn long, your repository or any other location where you have your code, and a reasonable person would look at it and think, yes, you went through the week and exercises. Think of it like a portfolio you could present to an employer.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/019_9d89b52976_homework-what-is-the-homework-url-in-the-homework.md'},\n",
       " {'id': '7255a1c3bc',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_813348b4.png'}],\n",
       "  'question': 'Leaderboard: how do find myself on the leaderboard?',\n",
       "  'sort_order': 20,\n",
       "  'content': 'When you set up your account, you are automatically assigned a random name, such as \"Lucid Elbakyan.\" If you want to see what your display name is, follow these steps:\\n\\n- Go to your profile.\\n  - 2025: [https://courses.datatalks.club/de-zoomcamp-2025/enrollment](https://courses.datatalks.club/de-zoomcamp-2025/enrollment)\\n  - 2024: [https://courses.datatalks.club/de-zoomcamp-2024/enrollment](https://courses.datatalks.club/de-zoomcamp-2025/enrollment)\\n- Log in.\\n- Your display name is shown. You can also change it if you wish. \\n- Ensure your certificate name is correct, as this name will later be printed on your certificate.\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/020_7255a1c3bc_leaderboard-how-do-find-myself-on-the-leaderboard.md'},\n",
       " {'id': '29e58c5c37',\n",
       "  'question': 'Environment: Is Python 3.9 still the recommended version to use in 2024?',\n",
       "  'sort_order': 21,\n",
       "  'content': 'Yes, for simplicity and stability when troubleshooting against recorded videos.\\n\\nBut Python 3.10 and 3.11 should work fine.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/021_29e58c5c37_environment-is-python-39-still-the-recommended-ver.md'},\n",
       " {'id': '4f1fe161b1',\n",
       "  'question': 'Environment - Should I use my local machine, GCP, or GitHub Codespaces for my environment?',\n",
       "  'sort_order': 22,\n",
       "  'content': 'You can set it up on your laptop or PC if you prefer to work locally. However, Windows users might face some challenges.\\n\\nIf you prefer to work on the local machine, you can start with the Week 1 Introduction to Docker.\\n\\nAlternatively, if you prefer to set up a virtual machine, consider the following:\\n\\n- **Using GitHub Codespaces**\\n- **Setting up the environment on a cloud VM**: Refer to [this video](https://www.youtube.com/watch?v=ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb) for guidance.\\n\\nWorking on a virtual machine is beneficial if you have different devices for home and office, allowing you to work virtually anywhere.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/022_4f1fe161b1_environment-should-i-use-my-local-machine-gcp-or-g.md'},\n",
       " {'id': '5b4fb0c0a8',\n",
       "  'question': 'Environment - Is GitHub codespaces an alternative to using cli/git bash to ingest the data and create a docker file?',\n",
       "  'sort_order': 23,\n",
       "  'content': 'GitHub Codespaces offers you computing Linux resources with many pre-installed tools (Docker, Docker Compose, Python).\\n\\nYou can also open any GitHub repository in a GitHub Codespace.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/023_5b4fb0c0a8_environment-is-github-codespaces-an-alternative-to.md'},\n",
       " {'id': '070766ca79',\n",
       "  'question': 'Environment: Do we really have to use GitHub codespaces? I already have PostgreSQL & Docker installed.',\n",
       "  'sort_order': 24,\n",
       "  'content': \"It's up to you which platform and environment you use for the course.\\n\\nGitHub Codespaces or GCP VM are just possible options, but you can do the entire course from your laptop.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/024_070766ca79_environment-do-we-really-have-to-use-github-codesp.md'},\n",
       " {'id': 'ef96ec09e6',\n",
       "  'question': 'Environment - Do I need both GitHub Codespaces and GCP?',\n",
       "  'sort_order': 25,\n",
       "  'content': 'Choose the approach that aligns the most with your idea for the end project.\\n\\nOne should suffice; however, BigQuery, which is part of GCP, will be used, so learning that is probably a better option. Alternatively, you can set up a local environment for most of this course.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/025_ef96ec09e6_environment-do-i-need-both-github-codespaces-and-g.md'},\n",
       " {'id': 'b92da7c113',\n",
       "  'question': 'Environment - Could not establish connection to \"MyServerName\": Got bad result from install script',\n",
       "  'sort_order': 26,\n",
       "  'content': 'This issue occurs when attempting to connect to a GCP VM using VSCode on a Windows machine. You can resolve it by changing a registry value in the registry editor.\\n\\nOpen the Run command window:\\n- Use the shortcut keys `Windows + R`, or\\n- Right-click \"Start\" and click \"Run\".\\n\\nOpen the Registry Editor:\\n- Type `regedit` in the Run command window, then press Enter.\\n\\nChange the registry value:\\n- Navigate to `HKEY_CURRENT_USER\\\\Software\\\\Microsoft\\\\Command Processor`.\\n- Change the \"Autorun\" value from \"if exists\" to a blank.\\n\\nAlternatively, you can delete the saved fingerprint within the known_hosts file:\\n\\nIn Windows, locate the file at `C:\\\\Users\\\\<your_user_name>\\\\.ssh\\\\known_hosts` and remove the entry for the server.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/026_b92da7c113_environment-could-not-establish-connection-to-myse.md'},\n",
       " {'id': 'a8219681ec',\n",
       "  'question': 'Environment - Why are we using GCP and not other cloud providers?',\n",
       "  'sort_order': 27,\n",
       "  'content': 'For uniformity.\\n\\nYou can use other cloud platforms since you get every service provided by GCP in Azure and AWS. You’re not restricted to GCP and can use other platforms like AWS if you’re more comfortable.\\n\\nBecause everyone usually has a Google account, GCP offers a free trial period with $300 in credits for new users. Additionally, we are working with BigQuery, which is part of GCP.\\n\\nNote that to sign up for a free GCP account, you must have a valid credit card.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/027_a8219681ec_environment-why-are-we-using-gcp-and-not-other-clo.md'},\n",
       " {'id': 'e7738f47c8',\n",
       "  'question': 'Should I pay for cloud services?',\n",
       "  'sort_order': 28,\n",
       "  'content': \"It's not mandatory. You can take advantage of their free trial.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/028_e7738f47c8_should-i-pay-for-cloud-services.md'},\n",
       " {'id': '77a076baa3',\n",
       "  'question': 'Environment: The GCP and other cloud providers are unavailable in some countries. Is it possible to provide a guide to installing a home lab?',\n",
       "  'sort_order': 29,\n",
       "  'content': 'You can do most of the course without a cloud. Almost everything we use (excluding BigQuery) can be run locally. We won’t be able to provide guidelines for some things, but most of the materials are runnable without GCP.\\n\\nFor everything in the course, there’s a local alternative. You could even do the whole course locally. Note that Homework 3 requires BigQuery.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/029_77a076baa3_environment-the-gcp-and-other-cloud-providers-are.md'},\n",
       " {'id': '109e36c115',\n",
       "  'question': 'Environment: Is GCP Sandbox enough or we need the Free Trial?',\n",
       "  'sort_order': 30,\n",
       "  'content': 'Google Cloud Platform (GCP) provides two free trial options: the Free Trial with $300 credit and the Sandbox. Users can switch between these options by managing billing details.\\n\\nHowever, completing the course solely using the GCP Sandbox option is not feasible due to its limited features. The Sandbox lacks some services required for the course, such as VMs, GCS Buckets, and other paid services that are integral to the curriculum.\\n\\nThe course will eventually require utilizing the following:\\n\\n- **VMs and GCS Buckets**: These resources are not fully available in the Sandbox.\\n- **BigQuery**: A key component of GCP, and the Sandbox may not support all necessary functionalities.\\n\\nTherefore, it is recommended to use the GCP Free Trial with billing details to access all needed features and ensure a smooth learning experience.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/030_109e36c115_environment-is-gcp-sandbox-enough-or-we-need-the-f.md'},\n",
       " {'id': '21c45a3556',\n",
       "  'question': 'Environment: I want to use AWS. May I do that?',\n",
       "  'sort_order': 31,\n",
       "  'content': 'Yes, you can. Just remember to adapt all the information from the videos to AWS. Additionally, the final capstone will be evaluated based on these tasks:\\n\\n- Create a data pipeline\\n- Develop a visualization\\n\\nConsider that when seeking help, you might need to rely on fellow coursemates who use AWS, which could be fewer compared to those using GCP.\\n\\nAlso, see [\"Is it possible to use x tool instead of the one tool you use?\"](#4dec1f8407)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/031_21c45a3556_environment-i-want-to-use-aws-may-i-do-that.md'},\n",
       " {'id': '99bb0ceeb6',\n",
       "  'question': 'Besides the “Office Hour” which are the live zoom calls?',\n",
       "  'sort_order': 32,\n",
       "  'content': 'We will probably have some calls during the Capstone period to clear some questions, but it will be announced in advance if that happens.\\n\\nSee [Google Calendar](https://calendar.google.com/calendar/?cid=ZXIxcjA1M3ZlYjJpcXU0dTFmaG02MzVxMG9AZ3JvdXAuY2FsZW5kYXIuZ29vZ2xlLmNvbQ)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/032_99bb0ceeb6_besides-the-office-hour-which-are-the-live-zoom-ca.md'},\n",
       " {'id': '31e9251331',\n",
       "  'question': 'Can I use Airflow instead for my final project?',\n",
       "  'sort_order': 33,\n",
       "  'content': 'Yes, you can use any tool you want for your project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/033_31e9251331_can-i-use-airflow-instead-for-my-final-project.md'},\n",
       " {'id': '4dec1f8407',\n",
       "  'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?',\n",
       "  'sort_order': 34,\n",
       "  'content': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products, or Tableau instead of Metabase or Google Data Studio.\\n\\nThe course covers two alternative data stacks: one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\n\\nConsiderations:\\n- We can’t support you if you choose to use a different stack.\\n- You would need to explain the different choices of tools for the peer review of your capstone project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/034_4dec1f8407_is-it-possible-to-use-tool-x-instead-of-the-one-to.md'},\n",
       " {'id': '721f9e0c29',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'sort_order': 35,\n",
       "  'content': '- [Star the repository](https://github.com/DataTalksClub/data-engineering-zoomcamp).\\n- Share it with friends if you find it useful.\\n- Create a pull request (PR) if you can improve the text or structure of the repository.\\n- [Update this FAQ](https://github.com/DataTalksClub/faq/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/035_721f9e0c29_how-can-we-contribute-to-the-course.md'},\n",
       " {'id': 'b7542b8d36',\n",
       "  'question': 'Environment: Is the course [Windows/macOS/Linux/...] friendly?',\n",
       "  'sort_order': 36,\n",
       "  'content': 'Yes! Linux is ideal but technically it should not matter. Students in the 2024 cohort used all 3 OSes successfully.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/036_b7542b8d36_environment-is-the-course-windowsmacoslinux-friend.md'},\n",
       " {'id': '8b0214d089',\n",
       "  'question': 'Environment: Roadblock for Windows users in modules with *.sh (shell scripts)',\n",
       "  'sort_order': 37,\n",
       "  'content': 'Later modules (module-05 & RisingWave workshop) use shell scripts in *.sh files. Most Windows users not using WSL will encounter issues and may not be able to continue, even in Git Bash or MINGW64. It is recommended to set up a WSL environment from the start.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/037_8b0214d089_environment-roadblock-for-windows-users-in-modules.md'},\n",
       " {'id': '86251abcdf',\n",
       "  'question': 'Any books or additional resources you recommend?',\n",
       "  'sort_order': 38,\n",
       "  'content': 'Yes to both! Check out this document: [Awesome Data Engineering Resources](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/awesome-data-engineering.md)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/038_86251abcdf_any-books-or-additional-resources-you-recommend.md'},\n",
       " {'id': '9b0561dbf8',\n",
       "  'question': 'Project: What is Project Attempt #1 and Project Attempt #2 exactly?',\n",
       "  'sort_order': 39,\n",
       "  'content': 'You will have two attempts for a project.\\n\\n- If the first project deadline is over and you’re late, or you submit the project and fail the first attempt, you have another chance to submit the project with the second attempt.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/039_9b0561dbf8_project-what-is-project-attempt-1-and-project-atte.md'},\n",
       " {'id': 'a83f047f52',\n",
       "  'question': 'How to troubleshoot issues',\n",
       "  'sort_order': 40,\n",
       "  'content': '**First Steps:**\\n\\n- Attempt to solve the issue independently. Familiarize yourself with documentation as a crucial skill for problem-solving.\\n- Use shortcuts like `[ctrl+f]` to search within documents and browsers.\\n- Analyze the error message for descriptions, instructions, and possible solutions.\\n- Restart your application, server, or computer as needed.\\n  \\n**Search for Solutions:**\\n\\n- Use search engines like Google, ChatGPT, or Bing AI to research the issue. It is rare to encounter a unique problem.\\n- Form your search queries using: `<technology> <problem statement>`. E.g., `pgcli error column c.relhasoids does not exist.`\\n- Consult the technology’s official documentation for guidance.\\n\\n**Uninstallation and Reinstallation:**\\n\\n- Uninstall and then reinstall the application if needed, including a system restart.\\n- Note that reinstalling without prior uninstallation might not resolve the issue.\\n\\n**Seeking Help:**\\n\\n- Post questions on platforms like StackOverflow. Ensure your question adheres to guidelines: [how-to-ask](https://stackoverflow.com/help/how-to-ask).\\n- Consider asking experts or colleagues in the future.\\n\\n**Community Resources:**\\n\\n- Check Slack channels for pinned messages and use its search function.\\n- Refer to this FAQ using search `[ctrl+f]` or utilize the `@ZoomcampQABot` for assistance.\\n\\n**When Asking for Help:**\\n\\n- Provide detailed information: coding environment, OS, commands, videos followed, etc.\\n- Share errors received, with specifics, including line numbers and actions taken.\\n- Avoid screenshots; paste code or errors directly. Use ``` for code formatting.\\n- Maintain thread consistency; respond in the same thread instead of creating multiple ones.\\n\\n**Re-evaluation:**\\n\\n- If the issue recurs, create a new post detailing changes in the environment.\\n- Communicate additional troubleshooting steps in the same thread.\\n- Occasionally take a break to gain a fresh perspective on the problem.\\n\\n**Documentation Contribution:**\\n\\n- If your problem solution is not listed, consider adding it to the FAQ to assist others.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/040_a83f047f52_how-to-troubleshoot-issues.md'},\n",
       " {'id': '4eba13edb1',\n",
       "  'question': 'How to ask questions',\n",
       "  'sort_order': 41,\n",
       "  'content': 'When the troubleshooting guide does not help resolve your issue and you need another pair of eyes, include as much information as possible when asking a question:\\n\\n- What are you coding on? What operating system are you using?\\n- What command did you run, and which video or tutorial did you follow?\\n- What error did you get? Does it have a line number pointing to the problematic code, and have you checked it for typos?\\n- What have you tried that did not work? This is crucial because, without it, helpers might suggest steps mentioned in the error log first. Or just refer to this FAQ document.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/041_4eba13edb1_how-to-ask-questions.md'},\n",
       " {'id': 'dc06a38bc6',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'sort_order': 42,\n",
       "  'content': 'After you create a GitHub account, clone the course repo to your local machine using the process outlined in this video:\\n\\n[Git for Everybody: How to Clone a Repository from GitHub](https://www.youtube.com/watch?v=CKcqniGu3tA).\\n\\nHaving this local repository on your computer will make it easy to access the instructors’ code and make pull requests if you want to add your own notes or make changes to the course content.\\n\\nYou will probably also create your own repositories to host your notes and versions of files. Here is a great tutorial that shows you how to do this:\\n\\n[How to Create a Git Repository](https://www.atlassian.com/git/tutorials/setting-up-a-repository).\\n\\nRemember to ignore large databases, .csv, and .gz files, and other files that should not be saved to a repository. Use `.gitignore` for this:\\n\\n[.gitignore file](https://www.atlassian.com/git/tutorials/saving-changes/gitignore).\\n\\n**Important:**\\n\\n**NEVER store passwords or keys in a git repo** (even if the repo is set to private). Put files containing sensitive information (.env, secret.json, etc.) in your `.gitignore`.\\n\\nThis is also a great resource: [Dangit, Git!?!](https://dangitgit.com/)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/042_dc06a38bc6_how-do-i-use-git-github-for-this-course.md'},\n",
       " {'id': 'e5dc51eac9',\n",
       "  'question': 'VS Code: Tab using spaces',\n",
       "  'sort_order': 43,\n",
       "  'content': 'Error:\\n\\n```\\nMakefile:2: *** missing separator.  Stop.\\n```\\n\\nSolution:\\n\\nTabs in documents should be converted to Tab instead of spaces. [Follow this stack](https://stackoverflow.com/questions/36814642/visual-studio-code-convert-spaces-to-tabs).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/043_e5dc51eac9_vs-code-tab-using-spaces.md'},\n",
       " {'id': '5b54567e89',\n",
       "  'question': 'Opening an HTML file with a Windows browser from Linux running on WSL',\n",
       "  'sort_order': 44,\n",
       "  'content': \"If you’re running Linux on Windows Subsystem for Linux (WSL) 2, you can open HTML files from the guest (Linux) with any Internet Browser installed on the host (Windows). Just install [wslu](https://wslutiliti.es/wslu/install.html) and open the page using `wslview`:\\n\\n```bash\\nwslview index.html\\n```\\n\\nYou can customize which browser to use by setting the `BROWSER` environment variable first. For example:\\n\\n```bash\\nexport BROWSER='/mnt/c/Program Files/Firefox/firefox.exe'\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/044_5b54567e89_opening-an-html-file-with-a-windows-browser-from-l.md'},\n",
       " {'id': '1b03f3dadf',\n",
       "  'question': 'Set up Chrome Remote Desktop for Linux on Compute Engine',\n",
       "  'sort_order': 45,\n",
       "  'content': 'This [tutorial](https://cloud.google.com/architecture/chrome-desktop-remote-on-compute-engine?hl=en) shows you how to set up the Chrome Remote Desktop service on a Debian Linux virtual machine (VM) instance on Compute Engine. Chrome Remote Desktop allows you to remotely access applications with a graphical user interface.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/045_1b03f3dadf_set-up-chrome-remote-desktop-for-linux-on-compute.md'},\n",
       " {'id': '6314bc3029',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}],\n",
       "  'question': 'How do I get my certificate?',\n",
       "  'sort_order': 46,\n",
       "  'content': 'There\\'ll be an announcement in Telegram and the course channel for:\\n\\n- Checking that your full name is displayed correctly on the Certificate (see Editing course profile on the Course Management webpage).\\n- Notifying when the grading is completed.\\n\\nYou will find it in your course profile (you need to be\\nlogged it). \\n\\nFor 2025 the link to the course profile is this:\\n\\n`https://courses.datatalks.club/de-zoomcamp-2025/enrollment`\\n\\nFor other editions, change \"2025\" to your edition.\\n\\nAfter the second announcement, follow instructions in [certificates.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md) on how to generate the Certificate document yourself.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md'},\n",
       " {'id': '05727a95dd',\n",
       "  'question': 'Homework and Leaderboard: What is the system for points in the course management',\n",
       "  'sort_order': 18,\n",
       "  'content': \"After you submit your homework, it will be graded based on the number of questions in that particular assignment. You can see the number of points you have earned at the top of the homework page. Additionally, in the [leaderboard](https://courses.datatalks.club/de-zoomcamp-2025/leaderboard), you will find the sum of all points you've earned: points for Homeworks, FAQs, and Learning in Public.\\n\\nPoint System Overview:\\n\\n- **Homework:** Points vary by assignment based on the number of questions.\\n- **FAQ Contribution:** You get a maximum of 1 point for contributing to the FAQ in the respective week.\\n- **Learning in Public:** For each learning in public link, you earn one point. You can achieve a maximum of 7 points.\\n\\nCheck this [video](https://www.loom.com/share/710e3297487b409d94df0e8da1c984ce) for more details.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/05727a95dd_homework-and-leaderboard-wha.md'},\n",
       " {'id': '6affd2987c',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_1813f02b.png'}],\n",
       "  'question': 'Taxi Data: Yellow Taxi Trip Records downloading error',\n",
       "  'sort_order': 1,\n",
       "  'content': 'When attempting to download the 2021 data from the [TLC website](https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page), you may encounter the following error:\\n\\n```bash\\nERROR 403: Forbidden\\n```\\n\\n<{IMAGE:image_1}>\\n\\nWe have a backup, so use it instead: [nyc-tlc-data](https://github.com/DataTalksClub/nyc-tlc-data)\\n\\nSo the link should be [yellow_tripdata_2021-01.csv.gz](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz).\\n\\n**Note:** Make sure to [unzip the \"gz\" file](https://linuxize.com/post/how-to-unzip-gz-file/) (no, the \"unzip\" command won’t work for this).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/001_6affd2987c_taxi-data-yellow-taxi-trip-records-downloading-err.md'},\n",
       " {'id': 'd677df9ccb',\n",
       "  'question': 'Taxi Data: How to handle *.csv.gz taxi data files?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'In [this video](https://www.youtube.com/watch?v=B1WwATwf-vY&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb), the data file is stored as `output.csv`. If the file extension is `csv.gz` instead of `csv`, it won\\'t store correctly.\\n\\nTo handle this:\\n\\n1. Replace `csv_name = \"output.csv\"` with the file name extracted from the URL. For example, for the yellow taxi data, use:\\n   \\n   ```python\\n   url = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\\n   csv_name = url.split(\"/\")[-1]\\n   ```\\n\\n2. When you use `csv_name` with `pandas.read_csv`, it will work correctly because `pandas.read_csv` can directly read files with the `csv.gz` extension.\\n\\nExample:\\n\\n```python\\nimport pandas as pd\\n\\nurl = \"https://github.com/DataTalksClub/nyc-tlc-data/releases/download/yellow/yellow_tripdata_2021-01.csv.gz\"\\ncsv_name = url.split(\"/\")[-1]\\n\\ndata = pd.read_csv(csv_name)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/002_d677df9ccb_taxi-data-how-to-handle-csvgz-taxi-data-files.md'},\n",
       " {'id': '4ee5e16952',\n",
       "  'question': 'Taxi Data: Data Dictionary for NY Taxi data?',\n",
       "  'sort_order': 3,\n",
       "  'content': 'Yellow Trips: [Data Dictionary](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf)\\n\\nGreen Trips: [Data Dictionary - LPEP Trip Records May 1, 2018](https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/003_4ee5e16952_taxi-data-data-dictionary-for-ny-taxi-data.md'},\n",
       " {'id': '8585d2d7f4',\n",
       "  'question': 'Taxi Data: Unzip Parquet file',\n",
       "  'sort_order': 4,\n",
       "  'content': 'You can unzip the downloaded parquet file from the command line. The result is a CSV file which can be imported with pandas using `pd.read_csv()` as shown in the videos.\\n\\n```bash\\ngunzip green_tripdata_2019-09.csv.gz\\n```\\n\\n### Solution for Using Parquet Files Directly in Python Script `ingest_data.py`\\n\\n1. In the `def main(params)`, add this line:\\n   \\n   ```python\\n   parquet_name = \\'output.parquet\\'\\n   ```\\n\\n2. Edit the code which downloads the files:\\n\\n   ```python\\n   os.system(f\"wget {url} -O {parquet_name}\")\\n   ```\\n\\n3. Convert the downloaded `.parquet` file to CSV and rename it to `csv_name` to keep it relevant to the rest of the code:\\n\\n   ```python\\n   df = pd.read_parquet(parquet_name)\\n   df.to_csv(csv_name, index=False)\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/004_8585d2d7f4_taxi-data-unzip-parquet-file.md'},\n",
       " {'id': 'b3030c88e7',\n",
       "  'question': 'wget is not recognized as an internal or external command',\n",
       "  'sort_order': 5,\n",
       "  'content': 'If you encounter the error \"wget is not recognized as an internal or external command,\" wget needs to be installed.\\n\\nThis error may also cause messages like \"No such file or directory: \\'output.csv.gz\\'.\"\\n\\n### Installation Instructions:\\n\\n- **On Ubuntu:**\\n\\n  ```bash\\n  sudo apt-get install wget\\n  ```\\n\\n- **On macOS:**\\n\\n  Use [Homebrew](https://brew.sh/):\\n\\n  ```bash\\n  brew install wget\\n  ```\\n\\n- **On Windows:**\\n\\n  Use [Chocolatey](https://chocolatey.org/):\\n\\n  ```bash\\n  choco install wget\\n  ```\\n\\n  Alternatively, download a binary from [GnuWin32](https://gnuwin32.sourceforge.net/packages/wget.htm) and place it in a location that is in your PATH (e.g., `C:/tools/`).\\n\\n#### Alternative Windows Installation:\\n\\n1. Download the latest wget binary for Windows from [eternallybored](https://eternallybored.org/misc/wget/).\\n2. If you downloaded the zip, extract all files (use [7-zip](https://7-zip.org/) if the built-in utility gives an error).\\n3. Rename the file `wget64.exe` to `wget.exe` if necessary.\\n4. Move `wget.exe` to your `Git\\\\mingw64\\\\bin\\\\` directory.\\n\\n#### Python Alternative:\\n\\n- Use the Python wget library:\\n\\n  First, install using pip:\\n\\n  ```bash\\n  pip install wget\\n  ```\\n\\n- Use it with Python:\\n\\n  ```bash\\n  python -m wget\\n  ```\\n\\nYou can also paste the file URL into your web browser to download normally, then move the file to your working directory.\\n\\n### Additional Recommendation:\\n\\nConsider using the Python library [requests](https://pypi.org/project/requests) for loading gz files.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/005_b3030c88e7_wget-is-not-recognized-as-an-internal-or-external.md'},\n",
       " {'id': '542abbcb6a',\n",
       "  'question': 'wget - ERROR: cannot verify <website> certificate  (MacOS)',\n",
       "  'sort_order': 6,\n",
       "  'content': 'Firstly, make sure that you add `!` before `wget` if you’re running your command in a Jupyter Notebook or CLI. Then, you can check one of these two things (from CLI):\\n\\n- **Using the Python library wget installed with pip:**\\n\\n  ```bash\\n  python -m wget <url>\\n  ```\\n\\n- **Use the usual command and add `--no-check-certificate` at the end:**\\n\\n  ```bash\\n  !wget <website_url> --no-check-certificate\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/006_542abbcb6a_wget-error-cannot-verify-website-certificate-macos.md'},\n",
       " {'id': '1ba19ed6a0',\n",
       "  'question': 'Git Bash: Backslash as an escape character in Git Bash for Windows',\n",
       "  'sort_order': 7,\n",
       "  'content': 'For those who wish to use the backslash as an escape character in Git Bash for Windows, type the following in the terminal:\\n\\n```bash\\nbash.escapeChar=\\\\\\n```\\n\\n(Note: There is no need to include this in your `.bashrc` file.)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/007_1ba19ed6a0_git-bash-backslash-as-an-escape-character-in-git-b.md'},\n",
       " {'id': '994ef10e79',\n",
       "  'question': 'GitHub Codespaces: How to store secrets',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Instruction on how to store secrets that will be available in GitHub Codespaces. See [Managing your account-specific secrets for GitHub Codespaces - GitHub Docs](https://docs.github.com/en/codespaces/managing-your-codespaces/managing-your-account-specific-secrets-for-github-codespaces#about-secrets-for-github-codespaces).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/008_994ef10e79_github-codespaces-how-to-store-secrets.md'},\n",
       " {'id': '3d24f7796d',\n",
       "  'question': 'GitHub Codespaces: Running pgadmin in Docker',\n",
       "  'sort_order': 9,\n",
       "  'content': 'With the default instructions, running pgadmin in Docker may result in a blank screen after logging into the pgadmin console. To resolve this, add the following two environment variables to your pgadmin configuration to allow it to work with Codespaces’ reverse proxy:\\n\\n```plaintext\\nPGADMIN_CONFIG_PROXY_X_HOST_COUNT: 1\\nPGADMIN_CONFIG_PROXY_X_PREFIX_COUNT: 1\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/009_3d24f7796d_github-codespaces-running-pgadmin-in-docker.md'},\n",
       " {'id': '5e6c4090af',\n",
       "  'question': 'Docker: Cannot connect to Docker daemon at unix:///var/run/docker.sock. Is the Docker daemon running?',\n",
       "  'sort_order': 10,\n",
       "  'content': \"Make sure you're able to start the Docker daemon. Check the issue immediately as described below:\\n\\n- Ensure the Docker daemon is running.\\n- Update WSL in PowerShell with the following command:\\n\\n  ```bash\\n  wsl --update\\n  ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/010_5e6c4090af_docker-cannot-connect-to-docker-daemon-at-unixvarr.md'},\n",
       " {'id': '46dbe4810d',\n",
       "  'question': 'Docker - error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges',\n",
       "  'sort_order': 11,\n",
       "  'content': 'If you get this error:\\n\\n```\\ndocker: error during connect: In the default daemon configuration on Windows, the docker client must be run with elevated privileges to connect.: Post \"http://%2F%2F.%2Fpipe%2Fdocker_engine/v1.24/containers/create\": open //./pipe/docker_engine: The system cannot find the file specified.\\nSee \\'docker run --help\\'.\\n```\\n\\nTo resolve it on Windows, follow these guidelines based on your version:\\n\\n**Windows 10 Pro / 11 Pro Users**:\\n\\n* Ensure Hyper-V is enabled, as Docker can use it as a backend.\\n* Follow the [Enable Hyper-V Option on Windows 10 / 11](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/) tutorial.\\n\\n**Windows 10 Home / 11 Home Users**:\\n\\n* The \\'Home\\' version doesn\\'t support Hyper-V, so use WSL2 (Windows Subsystem for Linux).\\n* Refer to [install WSL on Windows 11](https://pureinfotech.com/install-wsl-windows-11/) for detailed instructions.\\n\\nIf you encounter the \"WslRegisterDistribution failed with error: 0x800701bc\" error:\\n\\n- Update the WSL2 Linux Kernel by following the guidelines at [GitHub: WSL Issue 5393](https://github.com/microsoft/WSL/issues/5393).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/011_46dbe4810d_docker-error-during-connect-in-the-default-daemon.md'},\n",
       " {'id': 'bceb4aa421',\n",
       "  'question': 'Docker: docker pull dbpage',\n",
       "  'sort_order': 12,\n",
       "  'content': \"Whenever a `docker pull` is performed (either manually or by `docker-compose up`), it attempts to fetch the given image name from a repository. If the repository is public, the fetch and download occur without any issues.\\n\\nFor instance:\\n\\n```bash\\ndocker pull postgres:13\\ndocker pull dpage/pgadmin4\\n```\\n\\n**Be Advised:** The Docker images we'll be using throughout the Data Engineering Zoomcamp are all public, unless otherwise specified. This means you are not required to perform a `docker login` to fetch them.\\n\\nIf you encounter the message:\\n\\n```\\ndocker login': denied: requested access to the resource is denied.\\n```\\n\\nThis is likely due to a typo in your image name. For instance:\\n\\n```bash\\n$ docker pull dbpage/pgadmin4\\n```\\n\\nThis command will throw an exception:\\n\\n```\\nError response from daemon: pull access denied for dbpage/pgadmin4, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\\n```\\n\\nThis occurs because the actual image name is `dpage/pgadmin4`, not `dbpage/pgadmin4`.\\n\\n**How to fix it:**\\n\\n```bash\\n$ docker pull dpage/pgadmin4\\n```\\n\\n**Extra Notes:** In some professional environments, the Docker image may be in a private repository that your DockerHub username has access to. In this case, you must:\\n\\n1. Execute:\\n   ```bash\\n   $ docker login\\n   ```\\n2. Enter your username and password.\\n3. Then perform the `docker pull` against that private repository.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/012_bceb4aa421_docker-docker-pull-dbpage.md'},\n",
       " {'id': '0beb2b5df7',\n",
       "  'question': 'Docker: \"permission denied\" error when creating a PostgreSQL Docker with a mounted volume on macOS M1',\n",
       "  'sort_order': 13,\n",
       "  'content': 'When attempting to run a Docker command similar to the one below:\\n\\n```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```\\n\\nYou encounter the error message:\\n\\n```\\ndocker: Error response from daemon: error while creating mount source path \\'/path/to/ny_taxi_postgres_data\\': chown /path/to/ny_taxi_postgres_data: permission denied.\\n```\\n\\nSolution\\n\\n1. **Stop Rancher Desktop:**  \\n   If you are using Rancher Desktop and face this issue, stop Rancher Desktop to resolve compatibility problems.\\n\\n2. **Install Docker Desktop:**  \\n   Install Docker Desktop, ensuring that it is properly configured and has the required permissions.\\n\\n3. **Retry Docker Command:**  \\n   Run the Docker command again after switching to Docker Desktop. This step resolves compatibility issues on some systems.\\n\\n**Note:** The issue occurred because Rancher Desktop was in use. Switching to Docker Desktop resolves compatibility problems and allows for the successful creation of PostgreSQL containers with mounted volumes for the New York Taxi Database on macOS M1.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/013_0beb2b5df7_docker-permission-denied-error-when-creating-a-pos.md'},\n",
       " {'id': '1ae137c022',\n",
       "  'question': 'Docker: can’t delete local folder that mounted to docker volume',\n",
       "  'sort_order': 14,\n",
       "  'content': 'When a PostgreSQL Docker container is created, it may create a folder on the local machine to mount to a volume inside the container. This folder is often owned by user 999 and has read and write protection, preventing deletion by conventional means such as dragging it to the trash.\\n\\nIf you encounter an access error or need to delete the folder, you can use the following command:\\n\\n```bash\\nsudo rm -r -f docker_test/\\n```\\n\\n- `rm` : Command to remove files or directories.\\n- `-r` : Recursively remove directories and their contents.\\n- `-f` : Forcefully remove files/directories without prompting.\\n- `docker_test/` : The folder to be deleted.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/014_1ae137c022_docker-cant-delete-local-folder-that-mounted-to-do.md'},\n",
       " {'id': '3549528659',\n",
       "  'question': \"Docker: Docker won't start or is stuck in settings (Windows 10 / 11)\",\n",
       "  'sort_order': 15,\n",
       "  'content': \"Ensure you are running the latest version of Docker for Windows. Download the updated version from [Docker's official site](https://docs.docker.com/desktop/install/windows-install/). If the upgrade option in the menu doesn't work, uninstall and reinstall with the latest version.\\n\\nIf Docker is stuck on starting, try switching the containers by right-clicking the [docker symbol](https://imgur.com/vsVUAzK) from the running programs, and switch the containers from Windows to Linux or vice versa.\\n\\nFor Windows 10 / 11 Pro Edition:\\n\\n- **Hyper-V Backend:** ensure Hyper-V is enabled by following this [tutorial](https://www.c-sharpcorner.com/article/install-and-configured-docker-desktop-in-windows-10/).\\n- **WSL2 Backend:** follow the steps detailed in this [tutorial](https://pureinfotech.com/install-wsl-windows-11/).\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/015_docker-docker-wont-start-or.md'},\n",
       " {'id': 'c7e1100613',\n",
       "  'question': 'Docker: Should I run docker commands from the windows file system or a file system of a Linux distribution in WSL?',\n",
       "  'sort_order': 16,\n",
       "  'content': \"If you're running a Home Edition, you can still make it work with WSL2 (Windows Subsystem for Linux) by following the [tutorial here](https://pureinfotech.com/install-wsl-windows-11/).\\n\\nIf even after making sure your WSL2 (or Hyper-V) is set up accordingly, Docker remains stuck, you can try the following options:\\n\\n- [Reset to Factory Defaults](https://imgur.com/CfESyNt)\\n- Perform a fresh install.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/016_c7e1100613_docker-should-i-run-docker-commands-from-the-windo.md'},\n",
       " {'id': 'ed8dcfbb5a',\n",
       "  'question': 'Docker: The input device is not a TTY (Docker run for Windows)',\n",
       "  'sort_order': 17,\n",
       "  'content': 'You may encounter this error:\\n\\n```bash\\n$ docker run -it ubuntu bash\\nthe input device is not a TTY. If you are using mintty, try prefixing the command with \\'winpty\\'\\n```\\n\\nSolution:\\n\\n- Use `winpty` before the Docker command:\\n  \\n  ```bash\\n  $ winpty docker run -it ubuntu bash\\n  ```\\n\\n- Alternatively, create an alias:\\n  \\n  ```bash\\n  echo \"alias docker=\\'winpty docker\\'\" >> ~/.bashrc\\n  ```\\n  \\n  or\\n  \\n  ```bash\\n  echo \"alias docker=\\'winpty docker\\'\" >> ~/.bash_profile\\n  ```\\n  \\nSource: [Stack Overflow](https://stackoverflow.com/a/49965690)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/017_ed8dcfbb5a_docker-the-input-device-is-not-a-tty-docker-run-fo.md'},\n",
       " {'id': '4c8959502e',\n",
       "  'question': 'Docker: Cannot pip install on Docker container (Windows)',\n",
       "  'sort_order': 18,\n",
       "  'content': \"You may encounter this error:\\n\\n```\\nRetrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7efe331cf790>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution')': /simple/pandas/\\n```\\n\\nPossible solution:\\n\\nRun the following command:\\n\\n```bash\\nwinpty docker run -it --dns=8.8.8.8 --entrypoint=bash python:3.9\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/018_4c8959502e_docker-cannot-pip-install-on-docker-container-wind.md'},\n",
       " {'id': '87e92edf02',\n",
       "  'question': 'Docker: ny_taxi_postgres_data is empty',\n",
       "  'sort_order': 19,\n",
       "  'content': 'Even after properly running the Docker script, the folder may appear empty in VS Code. For Windows, try the following steps:\\n\\n**Solution 1:**\\n\\nRun the Docker command with the absolute path quoted in the `-v` parameter:\\n\\n```bash\\nwinpty docker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v \"C:\\\\Users\\\\abhin\\\\dataengg\\\\DE_Project_git_connected\\\\DE_OLD\\\\week1_set_up\\\\docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data\" \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```\\n\\nThis should resolve the visibility issue in the VS Code `ny_taxi` folder.\\n\\n**Note:** Ensure the correct direction for the slashes: `/` versus `\\\\`.\\n\\n**Solution 2:**\\n\\nAnother possible solution for Windows is to finish the folder path with a forward slash `/`:\\n\\n```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v /\"$(pwd)\"/ny_taxi_postgres_data/:/var/lib/postgresql/data/ \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```\\n\\nThese steps should help resolve the issue of the `ny_taxi_postgres_data` folder appearing empty in your Docker setup.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/019_87e92edf02_docker-ny_taxi_postgres_data-is-empty.md'},\n",
       " {'id': 'b2eabcd7dc',\n",
       "  'question': 'Docker: Setting up Docker on Mac',\n",
       "  'sort_order': 20,\n",
       "  'content': 'For setting up Docker on macOS, you have two main options:\\n\\n1. **Download from Docker Website:**\\n   - Visit the official Docker website and download the Docker Desktop for Mac as a `.dmg` file. This method is generally reliable and avoids issues related to licensing changes.\\n\\n2. **Using Homebrew:**\\n   - Be aware that there can be conflicts when installing with Homebrew, especially between Docker Desktop and command-line tools. To avoid issues:\\n     \\n     - Install Docker Desktop first.\\n     - Then install the command line tools.\\n\\n   - Commands:\\n     \\n     ```bash\\n     brew install --cask docker\\n     ```\\n     \\n     ```bash\\n     brew install docker docker-compose\\n     ```\\n\\n   - For more detailed issues related to `brew install`, refer to this [Issue](https://github.com/Homebrew/brew/issues/16309). \\n\\nFor more details, you can check the article on [Setting up Docker in macOS](https://medium.com/@vivekslair/setting-up-docker-in-macos-ee36d37b3be2).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/020_b2eabcd7dc_docker-setting-up-docker-on-mac.md'},\n",
       " {'id': 'ef201a0b0b',\n",
       "  'question': 'Docker - Could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted',\n",
       "  'sort_order': 21,\n",
       "  'content': '```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"admin\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v \"/mnt/path/to/ny_taxi_postgres_data\":\"/var/lib/postgresql/data\" \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```\\n\\n**Error Message:**\\n\\n```plaintext\\nThe files belonging to this database system will be owned by user \"postgres\". \\nThe database cluster will be initialized with locale \"en_US.utf8\". \\nThe default database encoding has accordingly been set to \"UTF8\". \\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ...\\ninitdb: error: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted\\n```\\n\\n**Solution:**\\n\\n1. Create a local Docker volume and map it to the Postgres data directory `/var/lib/postgresql/data`.\\n   \\n   - The volume name `dtc_postgres_volume_local` must match in both commands below:\\n\\n    ```bash\\n    docker volume create --name dtc_postgres_volume_local -d local\\n    ```\\n\\n2. Run the Docker container using the created volume:\\n\\n    ```bash\\n    docker run -it \\\\\\n      -e POSTGRES_USER=\"root\" \\\\\\n      -e POSTGRES_PASSWORD=\"root\" \\\\\\n      -e POSTGRES_DB=\"ny_taxi\" \\\\\\n      -v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n      -p 5432:5432 \\\\\\n      postgres:13\\n    ```\\n\\n3. Verify the command works in Docker Desktop under Volumes. The `dtc_postgres_volume_local` should be listed, but the folder `ny_taxi_postgres_data` will be empty as an alternative configuration is used.\\n\\n**Alternate Error:**\\n\\n```plaintext\\ninitdb: error: directory \"/var/lib/postgresql/data\" exists but is not empty\\n```\\n\\nTo resolve this, either remove or empty the directory \"/var/lib/postgresql/data\", or run `initdb`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/021_ef201a0b0b_docker-could-not-change-permissions-of-directory-v.md'},\n",
       " {'id': '3e83372387',\n",
       "  'question': 'Docker: invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)',\n",
       "  'sort_order': 22,\n",
       "  'content': 'Mapping volumes on Windows can be tricky. If the approach shown in the course video doesn\\'t work for you, consider the following suggestions:\\n\\n- Move your data to a directory without spaces. For example, move from `C:/Users/Alexey Grigorev/git/...` to `C:/git/...`.\\n\\n- Replace the `-v` part with one of these options:\\n\\n  ```bash\\n  -v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n  -v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n  -v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n  -v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n  --volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\n  ```\\n\\n- Add `winpty` before the whole command:\\n\\n  ```bash\\n  winpty docker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:1\\n  ```\\n\\n- Try adding quotes:\\n\\n  ```bash\\n  -v \"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n  -v \"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n  -v “/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n  -v \"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\"\\n  -v \"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\":/var/lib/postgresql/data\\n  ```\\n\\n- Note: If Windows automatically creates a folder called `ny_taxi_postgres_data;C`, it suggests a problem with volume mapping. Try deleting both folders and replacing the `-v` part with other options. Using `//c/` instead of `/c/` might work, as it creates the correct folder `ny_taxi_postgres_data`.\\n\\n- A possible solution is using `\"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data` and pay attention to the placement of quotes.\\n\\n- If none of these work, use a volume name instead of the path:\\n\\n  ```bash\\n  -v ny_taxi_postgres_data:/var/lib/postgresql/data\\n  ```\\n\\n- For Mac, you can wrap `$(pwd)` with quotes:\\n\\n  ```bash\\n  docker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v \"$(pwd)\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n  ```\\n\\nSource: [StackOverflow](https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/022_3e83372387_docker-invalid-reference-format-repository-name-mu.md'},\n",
       " {'id': 'fb9fab513f',\n",
       "  'question': 'Docker: Error response from daemon: invalid mode: \\\\Program Files\\\\Git\\\\var\\\\lib\\\\postgresql\\\\data.',\n",
       "  'sort_order': 23,\n",
       "  'content': 'Change the mounting path. Replace it with one of the following:\\n\\n```bash\\n-v /e/zoomcamp/...:/var/lib/postgresql/data\\n```\\nOr\\n\\n```bash\\n-v /c:/.../ny_taxi_postgres_data:/var/lib/postgresql/data\\n```\\n\\n(Note: Include a leading slash in front of `c:`)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/023_fb9fab513f_docker-error-response-from-daemon-invalid-mode-pro.md'},\n",
       " {'id': 'c1eeebf4ce',\n",
       "  'question': 'Docker: Error response from daemon: error while creating buildmount source',\n",
       "  'sort_order': 24,\n",
       "  'content': 'You may get this error:\\n\\n```\\nerror while creating buildmount source path \\'/run/desktop/mnt/host/c/<your path>\\': mkdir /run/desktop/mnt/host/c: file exists\\n```\\n\\nWhen you encounter the error above while rerunning your Docker command, it indicates that you should not mount on the second run. Here’s the initial problematic command:\\n\\n```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v <your path>:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```\\n\\nTo resolve the issue, use the revised command without the volume mount:\\n\\n```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -p 5432:5432 \\\\\\n  postgres:13\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/024_c1eeebf4ce_docker-error-response-from-daemon-error-while-crea.md'},\n",
       " {'id': '4e92a486d1',\n",
       "  'question': \"Docker: build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\",\n",
       "  'sort_order': 25,\n",
       "  'content': 'This error appeared when running the command:\\n\\n```bash\\ndocker build -t taxi_ingest:v001 .\\n```\\n\\nThe issue often arises because the user ID of the directory `ny_taxi_postgres_data` was changed, causing permission errors when accessing it. To resolve this error, use a directory containing only the necessary files, `Dockerfile` and `ingest_data.py`.\\n\\nIf you need to change permissions, use the following command on Ubuntu:\\n\\n```bash\\nsudo chown -R $USER dir_path\\n```\\n\\nOn Windows, follow the instructions in this guide: [The Geek Page](https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/).\\n\\nFor more information, refer to this explanation on Stack Overflow: [Docker build error checking context](https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/025_4e92a486d1_docker-build-error-error-checking-context-cant-sta.md'},\n",
       " {'id': 'd87e3d2a14',\n",
       "  'question': 'Docker: ERRO[0000] error waiting for container: context canceled',\n",
       "  'sort_order': 26,\n",
       "  'content': 'You might have installed Docker via snap. Run the following command to verify:\\n\\n```bash\\nsudo snap status docker\\n```\\n\\nIf you receive the response:\\n\\n```\\nerror: unknown command \"status\", see \\'snap help\\'.\\n```\\n\\nThen uninstall Docker and install it via the [official website](https://docs.docker.com/engine/install/ubuntu/).\\n\\nError message: \"Bind for 0.0.0.0:5432 failed: port is already allocated.\"',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/026_d87e3d2a14_docker-erro0000-error-waiting-for-container-contex.md'},\n",
       " {'id': '7c4d422363',\n",
       "  'question': 'Docker: build error checking context: can’t stat ‘/home/fhrzn/Projects/…./ny_taxi_postgres_data’',\n",
       "  'sort_order': 27,\n",
       "  'content': 'This issue occurs due to insufficient authorization rights to the host folder, which may cause it to appear empty.\\n\\n**Solution:**\\n\\nAdd permission for everyone to the folder:\\n\\n```bash\\nsudo chmod -R 777 <path_to_folder>\\n```\\n\\nExample:\\n\\n```bash\\nsudo chmod -R 777 ny_taxi_postgres_data/\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/027_7c4d422363_docker-build-error-checking-context-cant-stat-home.md'},\n",
       " {'id': '82bb8e49ea',\n",
       "  'question': 'Docker: failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.',\n",
       "  'sort_order': 28,\n",
       "  'content': 'This issue occurs on Ubuntu/Linux systems when attempting to rebuild the Docker container.\\n\\n```bash\\n$ docker build -t taxi_ingest:v001 .\\n```\\n\\nA folder is created to host the Docker files. When the build command is executed again, a permission error may occur because there are no permissions on this new folder. To resolve this, grant permissions by running the command:\\n\\n```bash\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\n```\\n\\nIf issues persist, use:\\n\\n```bash\\n$ sudo chmod -R 777 ny_taxi_postgres_data\\n```\\n\\nNote: 755 grants write access only to the owner.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/028_82bb8e49ea_docker-failed-to-solve-with-frontend-dockerfilev0.md'},\n",
       " {'id': '79e44b9c07',\n",
       "  'question': 'Docker: Docker network name',\n",
       "  'sort_order': 29,\n",
       "  'content': 'Get the network name via:\\n\\n```bash\\ndocker network ls\\n```\\n\\nFor more details, refer to the [Docker network ls documentation](https://docs.docker.com/engine/reference/commandline/network_ls/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/029_79e44b9c07_docker-docker-network-name.md'},\n",
       " {'id': '381dfe5145',\n",
       "  'question': 'Docker: Error response from daemon: Conflict. The container name \"pg-database\" is already in use by container \"xxx\". You have to remove (or rename) that container to be able to reuse that name.',\n",
       "  'sort_order': 30,\n",
       "  'content': 'Sometimes, when you try to restart a Docker container configured with a network name, the error message appears.\\n\\nTo resolve this issue:\\n\\n1. If the container is in a running state, stop it using:\\n   \\n   ```bash\\n   docker stop <container_name>\\n   ```\\n   \\n2. Then remove the container:\\n   \\n   ```bash\\n   docker rm pg-database\\n   ```\\n\\nAlternatively, you can use `docker start` instead of `docker run` to restart the Docker container without removing it.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/030_381dfe5145_docker-error-response-from-daemon-conflict-the-con.md'},\n",
       " {'id': '1700cb2bd4',\n",
       "  'question': 'Docker: ingestion when using docker-compose could not translate host name',\n",
       "  'sort_order': 31,\n",
       "  'content': 'Typical error:\\n```python\\nn.exc.OperationalError: (psycopg2.OperationalError) could not translate host name \"pgdatabase\" to address: Name or service not known\\n```\\n\\n**Solution:**\\n\\n1. Run `docker-compose up -d` to start your containers.\\n2. Check which network is created by Docker, as it may differ from your expectations.\\n3. Use the actual network name in your ingestion script instead of \"pg-network\".\\n4. Confirm the correct database service name, replacing \"pgdatabase\" accordingly.\\n\\n**Example:**\\n- \"pg-network\" might become \"2docker_default\".',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/031_1700cb2bd4_docker-ingestion-when-using-docker-compose-could-n.md'},\n",
       " {'id': 'c6cd0fedc5',\n",
       "  'question': 'Docker: Cannot install docker on MacOS/Windows 11 VM running on top of Linux (due to Nested virtualization).',\n",
       "  'sort_order': 32,\n",
       "  'content': 'Before starting your VM, you need to enable nested virtualization. Run the following commands based on your CPU:\\n\\n- **For Intel CPU:**\\n  \\n  ```bash\\n  modprobe -r kvm_intel\\n  modprobe kvm_intel nested=1\\n  ```\\n\\n- **For AMD CPU:**\\n  \\n  ```bash\\n  modprobe -r kvm_amd\\n  modprobe kvm_amd nested=1\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/032_c6cd0fedc5_docker-cannot-install-docker-on-macoswindows-11-vm.md'},\n",
       " {'id': '0cc94bef66',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_ea5934b5.png'}],\n",
       "  'question': 'Docker: Connecting from VS Code',\n",
       "  'sort_order': 33,\n",
       "  'content': 'It’s very easy to manage your Docker container, images, network, and compose projects from VS Code.\\n\\n- Install the [official extension](https://marketplace.visualstudio.com/items?itemName=ms-azuretools.vscode-docker) and launch it from the left side icon.\\n\\n  <{IMAGE:image_1}>\\n\\n- It will work even if your Docker runs on WSL2, as VS Code can easily connect with your Linux.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/033_0cc94bef66_docker-connecting-from-vs-code.md'},\n",
       " {'id': '9e05e54958',\n",
       "  'question': 'Docker: How to stop a container?',\n",
       "  'sort_order': 34,\n",
       "  'content': 'Use the following command:\\n\\n```bash\\ndocker stop <container_id>\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/034_9e05e54958_docker-how-to-stop-a-container.md'},\n",
       " {'id': '0812df9bf6',\n",
       "  'question': 'Docker: PostgreSQL Database directory appears to contain a database. Database system is shut down',\n",
       "  'sort_order': 35,\n",
       "  'content': 'When you see this in logs, your container with PostgreSQL is not accepting any requests. Attempting to connect may result in the error:\\n\\n```\\nconnection failed: server closed the connection unexpectedly\\n\\nThis probably means the server terminated abnormally before or while processing the request.\\n```\\n\\nTo resolve this issue:\\n\\n1. **Delete Data Directory**: Delete the directory with data (the one you map to the container using the `-v` flag) and restart the container.\\n\\n2. **Preserve Critical Data**: If your data is critical, you may be able to reset the write-ahead log from within the Docker container. For more details, see [here](https://github.com/alexg9010/2025_data_engineering_zoomcamp/blob/master/01_docker/README.md#fix-broken-postgress-docker-container).\\n\\n   ```bash\\n   docker run -it \\\\\\n   -e POSTGRES_USER=\"root\" \\\\\\n   -e POSTGRES_PASSWORD=\"root\" \\\\\\n   -e POSTGRES_DB=\"ny_taxi\" \\\\\\n   -v $(pwd)/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n   -p 5432:5432 \\\\\\n   --network pg-network \\\\\\n   postgres:13 \\\\\\n   /bin/bash -c \\'gosu postgres pg_resetwal /var/lib/postgresql/data\\'\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/035_0812df9bf6_docker-postgresql-database-directory-appears-to-co.md'},\n",
       " {'id': '1b727dde32',\n",
       "  'question': 'Docker: Docker not installable on Ubuntu',\n",
       "  'sort_order': 36,\n",
       "  'content': 'On some versions of Ubuntu, the `snap` command can be used to install Docker.\\n\\n```bash\\nsudo snap install docker\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/036_1b727dde32_docker-docker-not-installable-on-ubuntu.md'},\n",
       " {'id': '3f1c2f93bd',\n",
       "  'question': 'Docker-Compose: mounting error',\n",
       "  'sort_order': 37,\n",
       "  'content': '```\\nerror: could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted\\n```\\n\\nIf you have used the previous answer and created a local Docker volume, then you need to inform the compose file about the named volume:\\n\\n```yaml\\ndtc_postgres_volume_local:  # Define the named volume here\\n```\\n\\n- Services mentioned in the compose file automatically become part of the same network.\\n\\n### Steps:\\n\\n1. Use the command:\\n   ```bash\\n   docker volume inspect dtc_postgres_volume_local\\n   ```\\n   to see the location by checking the value of `Mountpoint`.\\n\\n2. In some cases, after running `docker compose up`, the mounting directory created is named `docker_sql_dtc_postgres_volume_local` instead of the existing `dtc_postgres_volume_local`.\\n\\n3. Rename the existing `dtc_postgres_volume_local` to `docker_sql_dtc_postgres_volume_local`:\\n   - Be careful when performing this operation.\\n\\n4. Remove the newly created one.\\n\\n5. Run `docker compose up` again and check if the table is there.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/037_3f1c2f93bd_docker-compose-mounting-error.md'},\n",
       " {'id': '8516ca8849',\n",
       "  'question': 'Docker-Compose: Error translating host name to address',\n",
       "  'sort_order': 38,\n",
       "  'content': 'Couldn’t translate host name to address',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/038_8516ca8849_docker-compose-error-translating-host-name-to-addr.md'},\n",
       " {'id': 'edcc24a810',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_dc508dc3.png'}],\n",
       "  'question': 'Docker-Compose: Data retention (could not translate host name \"pg-database\" to address: Name or service not known)',\n",
       "  'sort_order': 39,\n",
       "  'content': '<{IMAGE:image_1}>\\n\\nMake sure the PostgreSQL database is running. Use the command to start containers in detached mode:\\n\\n```bash\\ndocker-compose up -d\\n```\\n\\nExample output:\\n\\n```plaintext\\n% docker compose up -d\\n\\n[+] Running 2/2\\n⠿ Container pg-admin     Started\\n⠿ Container pg-database  Started\\n```\\n\\nTo view the containers use:\\n\\n```bash\\ndocker ps\\n```\\n\\nExample output:\\n\\n```\\n% docker ps\\n\\nCONTAINER ID   IMAGE            COMMAND                  CREATED          STATUS          PORTS                           NAMES\\nfaf05090972e   postgres:13      \"docker-entrypoint.s…\"   39 seconds ago   Up 37 seconds   0.0.0.0:5432->5432/tcp          pg-database\\n6344dcecd58f   dpage/pgadmin4   \"/entrypoint.sh\"         39 seconds ago   Up 37 seconds   443/tcp, 0.0.0.0:8080->80/tcp   pg-adminhw\\n```\\n\\nTo view logs for a container:\\n\\n```bash\\ndocker logs <containerid>\\n```\\n\\nExample logs for PostgreSQL:\\n\\n```\\n% docker logs faf05090972e\\n\\nPostgreSQL Database directory appears to contain a database; Skipping initialization\\n2022-01-25 05:58:45.948 UTC [1] LOG:  starting PostgreSQL 13.5 (Debian 13.5-1.pgdg110+1) on aarch64-unknown-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv4 address \"0.0.0.0\", port 5432\\n2022-01-25 05:58:45.948 UTC [1] LOG:  listening on IPv6 address \"::\", port 5432\\n2022-01-25 05:58:45.954 UTC [1] LOG:  listening on Unix socket \"/var/run/postgresql/.s.PGSQL.5432\"\\n2022-01-25 05:58:45.984 UTC [28] LOG:  database system was interrupted; last known up at 2022-01-24 17:48:35 UTC\\n2022-01-25 05:58:48.581 UTC [28] LOG:  database system was not properly shut down; automatic recovery in progress\\n2022-01-25 05:58:48.602 UTC [28] LOG:  redo starts at 0/872A5910\\n2022-01-25 05:59:33.726 UTC [28] LOG:  invalid record length at 0/98A3C160: wanted 24, got 0\\n2022-01-25 05:59:33.726 UTC [28] LOG:  redo done at 0/98A3C128\\n2022-01-25 05:59:48.051 UTC [1] LOG:  database system is ready to accept connections\\n```\\n\\nIf `docker ps` doesn’t show `pg-database` running, use:\\n\\n```bash\\ndocker ps -a\\n```\\n\\n- This will show all containers, either running or stopped.\\n- Get the container ID for `pg-database-1` and run the appropriate command.\\n\\nIf you lose database data after executing `docker-compose up` and cannot successfully execute your ingestion script due to the following error:\\n\\n```plaintext\\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not translate host name /data_pgadmin:/var/lib/pgadmin\"pg-database\" to address: Name or service not known\\n```\\n\\n- Docker Compose may be creating its own default network since it is no longer specified in the command or file.\\n- Check logs after executing `docker-compose up` to find the network name and change the network name argument in your ingestion script.\\n\\nIf problems persist with `pgcli`, consider using HeidiSQL.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/039_edcc24a810_docker-compose-data-retention-could-not-translate.md'},\n",
       " {'id': '432d3f7e09',\n",
       "  'question': 'Docker-Compose: Hostname does not resolve',\n",
       "  'sort_order': 40,\n",
       "  'content': 'When encountering the error:\\n\\n```\\nError response from daemon: network 66ae65944d643fdebbc89bd0329f1409dec2c9e12248052f5f4c4be7d1bdc6a3 not found\\n```\\n\\nTry the following steps:\\n\\n1. Run `docker ps -a` to see all stopped and running containers.\\n2. Remove all containers to clean up the environment.\\n3. Execute `docker-compose up -d` again.\\n\\nIf facing issues connecting to the server at `localhost:8080` with the error:\\n\\n```\\nUnable to connect to server: could not translate host name \\'pg-database\\' to address: Name does not resolve\\n```\\n\\nConsider these solutions:\\n\\n- Use a new hostname without dashes, e.g., `pgdatabase`.\\n- Make sure to specify the Docker network and use the same network in both containers in your `docker-compose.yml` file.\\n\\nExample `docker-compose.yml`:\\n\\n```yaml\\nservices:\\n\\n  pgdatabase:\\n    image: postgres:13\\n    environment:\\n      - POSTGRES_USER=root\\n      - POSTGRES_PASSWORD=root\\n      - POSTGRES_DB=ny_taxi\\n    volumes:\\n      - \"./ny_taxi_postgres_data:/var/lib/postgresql/data:rw\"\\n    ports:\\n      - \"5431:5432\"\\n    networks:\\n      - pg-network\\n\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n      - PGADMIN_DEFAULT_PASSWORD=root\\n    ports:\\n      - \"8080:80\"\\n    networks:\\n      - pg-network\\n\\nnetworks:\\n  pg-network:\\n    name: pg-network\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/040_432d3f7e09_docker-compose-hostname-does-not-resolve.md'},\n",
       " {'id': 'a950fda271',\n",
       "  'question': 'Docker-Compose: PgAdmin – no database in PgAdmin',\n",
       "  'sort_order': 41,\n",
       "  'content': \"When you log into PgAdmin and see an empty database, the following solution can help:\\n\\nRun:\\n   \\n```bash\\ndocker-compose up\\n```\\n\\nAnd at the same time run:\\n\\n```bash\\ndocker build -t taxi_ingest:v001 .\\n\\n# NETWORK NAME IS THE SAME AS THAT CREATED BY DOCKER COMPOSE\\ndocker run -it \\\\\\n  --network=pg-network \\\\\\n  taxi_ingest:v001 \\\\\\n  --user=postgres \\\\\\n  --password=postgres \\\\\\n  --host=db \\\\\\n  --port=5432 \\\\\\n  --db=ny_taxi \\\\\\n  --table_name=green_tripdata \\\\\\n  --url=${URL}\\n```\\n\\nIt's important to use the same `--network` as stated in the `docker-compose.yaml` file.\\n\\nThe `docker-compose.yaml` file might not specify a network, as shown below:\\n\\n```yaml\\nservices:\\n  db:\\n    container_name: postgres\\n    image: postgres:17-alpine\\n    environment:\\n      ...\\n    ports:\\n      - '5433:5432'\\n    volumes:\\n      - ...\\n  pgadmin:\\n    container_name: pgadmin\\n    image: dpage/pgadmin4:latest\\n    environment:\\n      ...\\n    ports:\\n      - '8080:80'\\n    volumes:\\n      - ...\\n\\nvolumes:\\n  vol-pgdata:\\n    name: vol-pgdata\\n  vol-pgadmin_data:\\n    name: vol-pgadmin_data\\n```\\n\\nIf the network name is not specified, it is generated automatically: The name of the directory containing the `docker-compose.yaml` file in lowercase + `_default`.\\n\\nYou can find the network’s name when running `docker-compose up`:\\n\\n```\\npg-database Pulling pg-database Pulled \\nNetwork week_1_default  Creating\\nNetwork week_1_default  Created\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/041_a950fda271_docker-compose-pgadmin-no-database-in-pgadmin.md'},\n",
       " {'id': 'ac52bea382',\n",
       "  'question': 'Docker-Compose: Persist PGAdmin docker contents on GCP',\n",
       "  'sort_order': 42,\n",
       "  'content': 'One common issue when running Docker Compose on GCP is that PostgreSQL might not persist its data to the specified path. For example:\\n\\n```yaml\\nservices:\\n  ...\\n  pgadmin:\\n    ...\\n    volumes:\\n      - \"./pgadmin:/var/lib/pgadmin:wr\"\\n```\\n\\nThis setup might not work. To resolve this, use Docker Volume to make the data persist:\\n\\n```yaml\\nservices:\\n  ...\\n  pgadmin:\\n    ...\\n    volumes:\\n      - pgadmin:/var/lib/pgadmin\\n\\nvolumes:\\n  pgadmin:\\n```\\n\\nThis configuration change ensures the persistence of the PGAdmin data on GCP.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/042_ac52bea382_docker-compose-persist-pgadmin-docker-contents-on.md'},\n",
       " {'id': '5baf3e8a5e',\n",
       "  'question': 'Docker: Docker engine stopped_failed to fetch extensions',\n",
       "  'sort_order': 43,\n",
       "  'content': 'The Docker engine may crash continuously and fail to work after restart. You might see error messages like \"docker engine stopped\" and \"failed to fetch extensions\" repeatedly on the screen.\\n\\n**Solution:**\\n\\n- Check if you have the latest version of Docker installed. Update Docker if necessary.\\n- If the problem persists, consider reinstalling Docker.\\n  - Note: You will need to fetch images again, but there should be no other issues.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/043_5baf3e8a5e_docker-docker-engine-stopped_failed-to-fetch-exten.md'},\n",
       " {'id': '4146155608',\n",
       "  'question': 'Docker-Compose: Persist PGAdmin configuration',\n",
       "  'sort_order': 44,\n",
       "  'content': 'To persist pgAdmin configuration, such as the server name, modify your `docker-compose.yml` by adding a \"volumes\" section:\\n\\n```yaml\\nservices:\\n\\n  pgdatabase:\\n    [...]\\n\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n      - PGADMIN_DEFAULT_PASSWORD=root\\n    volumes:\\n      - \"./pgAdmin_data:/var/lib/pgadmin/sessions:rw\"\\n    ports:\\n      - \"8080:80\"\\n```\\n\\nIn the example above, \"pgAdmin_data\" is a folder on the host machine, and \"/var/lib/pgadmin/sessions\" is the session settings folder in the pgAdmin container.\\n\\nBefore running `docker-compose up` on the YAML file, provide the pgAdmin container with access permissions to the \"pgAdmin_data\" folder. The container runs with a username \"5050\" and user group \"5050\". Use the following command to set permissions:\\n\\n```bash\\nsudo chown -R 5050:5050 pgAdmin_data\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/044_docker-compose-persist-pgadm.md'},\n",
       " {'id': '63e87e7442',\n",
       "  'question': 'Docker-Compose - dial unix /var/run/docker.sock: connect: permission denied',\n",
       "  'sort_order': 45,\n",
       "  'content': 'This happens if you did not create the docker group and add your user. Follow these steps from the link: [guides/docker-without-sudo.md at main · sindresorhus/guides · GitHub](https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md)\\n\\n1. Press `Ctrl+D` to log out and log back in again.\\n\\nIf you are tired of having to set up your database connection each time you start the containers, create a volume for pgAdmin:\\n\\nIn your `docker-compose.yaml` file, add the following under your pgAdmin service:\\n\\n```yaml\\nservices:\\n  pgadmin:\\n    volumes:\\n      - type: volume\\n        source: pgadmin_data\\n        target: /var/lib/pgadmin\\n```\\n\\nAlso, add the following to the end of the file:\\n\\n```yaml\\nvolumes:\\n  pgadmin_data:\\n```\\n\\nThis configuration will maintain the state so that pgAdmin remembers your previous connections.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/045_63e87e7442_docker-compose-dial-unix-varrundockersock-connect.md'},\n",
       " {'id': 'e43abaa421',\n",
       "  'question': 'Docker: docker-compose still not available after changing .bashrc',\n",
       "  'sort_order': 46,\n",
       "  'content': \"This issue can occur after installing Docker Compose in a Google Cloud VM, as demonstrated in video 1.4.1. \\n\\nIf the downloaded Docker Compose file from GitHub is named `docker-compose-linux-x86_64`, you may need to rename it for convenience. Here's how to resolve the issue:\\n\\n1. Rename `docker-compose-linux-x86_64` to `docker-compose` using the following command:\\n   \\n   ```bash\\n   mv docker-compose-linux-x86_64 docker-compose\\n   ```\\n\\nBy doing this, you can use the `docker-compose` command directly.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/046_e43abaa421_docker-docker-compose-still-not-available-after-ch.md'},\n",
       " {'id': '4ee3b7231a',\n",
       "  'question': 'Docker-Compose: Error getting credentials after running docker-compose up -d',\n",
       "  'sort_order': 47,\n",
       "  'content': 'Installing pass via `sudo apt install pass` helped to solve the issue. More about this can be found here: [https://github.com/moby/buildkit/issues/1078](https://github.com/moby/buildkit/issues/1078)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/047_4ee3b7231a_docker-compose-error-getting-credentials-after-run.md'},\n",
       " {'id': '73876c8348',\n",
       "  'images': None,\n",
       "  'question': 'Docker-Compose: Errors pertaining to docker-compose.yml and pgadmin setup',\n",
       "  'sort_order': 48,\n",
       "  'content': \"For those experiencing problems with Docker Compose, getting data in PostgreSQL, and similar issues, follow these steps:\\n\\n- **Create a new volume** on Docker, either using the command line or Docker Desktop app.\\n- **Modify your `docker-compose.yml` file** as needed to fix any setup issues.\\n- **Set `low_memory=False`** when importing the CSV file using pandas:\\n  \\n```python\\ndf = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False)\\n```\\n\\n- Use the specified function in your `upload-data.ipynb` for better tracking of the ingestion process.\\n\\n```python\\nfrom time import time\\n\\ncounter = 0\\ntime_counter = 0\\n\\nwhile True:\\n    t_start = time()\\n\\n    df = next(df_iter)\\n\\n    df.tpep_pickup_datetime = pd.to_datetime(df.tpep_pickup_datetime)\\n    df.tpep_dropoff_datetime = pd.to_datetime(df.tpep_dropoff_datetime)\\n\\n    df.to_sql(name='yellow_taxi_data', con=engine, if_exists='append')\\n\\n    t_end = time()\\n\\n    t_elapsed = t_end - t_start\\n\\n    print('Chunk Insertion Done! Time taken: %.2f seconds' %(t_elapsed))\\n\\n    counter += 1\\n    time_counter += t_elapsed\\n\\n    if counter == 14:\\n        print('All Chunks Inserted! Total Time Taken: %.2f seconds' %(time_counter))\\n        break\\n```\\n\\n### Order of Execution:\\n\\n1. Open the terminal in the `2_docker_sql` folder and run: `docker compose up`\\n2. Ensure no other containers are running except the ones you just executed (pgAdmin and pgdatabase).\\n3. Open Jupyter Notebook and begin the data ingestion.\\n4. Open pgAdmin and set up a server. Make sure you use the same configurations as your `docker-compose.yml` file, such as the same name (`pgdatabase`), port, and database name (`ny_taxi`).\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/048_73876c8348_docker-compose-errors-pertaining-to-docker-compose.md'},\n",
       " {'id': '9ef838bb46',\n",
       "  'question': 'Docker: Compose up -d error getting credentials - err: exec: \"docker-credential-desktop\": executable file not found in %PATH%, out: ``',\n",
       "  'sort_order': 49,\n",
       "  'content': 'To resolve this error, follow these steps:\\n\\n1. Locate the `config.json` file for Docker, typically found in your home directory at `Users/username/.docker`.\\n2. Modify the `credsStore` setting to `credStore`.\\n3. Save the file and re-run your Docker Compose command.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/049_9ef838bb46_docker-compose-up-d-error-getting-credentials-err.md'},\n",
       " {'id': '4ccef7c92d',\n",
       "  'question': 'Docker-Compose: Which docker-compose binary to use for WSL?',\n",
       "  'sort_order': 50,\n",
       "  'content': 'To determine which `docker-compose` binary to download from [Docker Compose releases](https://github.com/docker/compose/releases), you can check your system with the following commands:\\n\\n- To check the system type:\\n\\n  ```bash\\n  uname -s  # This will most likely return \\'Linux\\'\\n  ```\\n\\n- To check the system architecture:\\n\\n  ```bash\\n  uname -m  # This will return your system\\'s \\'flavor\\'\\n  ```\\n\\nAlternatively, you can use the following command to download `docker-compose` directly:\\n\\n```bash\\nsudo curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/050_4ccef7c92d_docker-compose-which-docker-compose-binary-to-use.md'},\n",
       " {'id': '5f056e236c',\n",
       "  'question': 'Docker-Compose - Error undefined volume in Windows/WSL',\n",
       "  'sort_order': 51,\n",
       "  'content': 'If you wrote the `docker-compose.yaml` file exactly like the video, you might run into an error:\\n\\n```\\nservice \"pgdatabase\" refers to undefined volume dtc_postgres_volume_local: invalid compose project\\n```\\n\\nTo resolve this, include the volume definition in your `docker-compose.yaml` file by adding:\\n\\n```yaml\\ndt_postgres_volume_local:\\n```\\n\\nThis should be added under the `volumes` section. Make sure your file looks similar to this:\\n\\n```yaml\\nvolumes:\\n  dtc_postgres_volume_local:\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/051_5f056e236c_docker-compose-error-undefined-volume-in-windowsws.md'},\n",
       " {'id': 'e532c9677e',\n",
       "  'question': 'Docker-Compose: cannot execute binary file: Exec format error',\n",
       "  'sort_order': 52,\n",
       "  'content': 'This error indicates that the docker-compose executable cannot be opened in the current OS. Ensure that the file you download from GitHub matches your system environment.\\n\\nAs of 2025/1/17, docker-compose ([v2.32.4](https://github.com/docker/compose/releases/tag/v2.32.4)) [docker-compose-linux-aarch64](https://github.com/docker/compose/releases/download/v2.32.4/docker-compose-linux-aarch64) does not work. Try v2.32.3 [docker-compose-linux-x86_64](https://github.com/docker/compose/releases/download/v2.32.3/docker-compose-linux-x86_64).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/052_e532c9677e_docker-compose-cannot-execute-binary-file-exec-for.md'},\n",
       " {'id': 'd3c860aa46',\n",
       "  'question': 'Docker: Postgres container fails to launch with exit code (1) when attempting to compose',\n",
       "  'sort_order': 53,\n",
       "  'content': 'This issue arises because the Postgres database is not initialized before executing `docker-compose up -d`. While there are other potential solutions [discussed in this thread](https://forums.docker.com/t/one-of-the-postgres-containers-stops-as-soon-as-it-starts/74714/3), you can resolve it by initializing the database first. Then, the Docker Compose will work as expected.\\n\\n```bash\\ndocker run -it \\\\\\n  -e POSTGRES_USER=\"root\" \\\\\\n  -e POSTGRES_PASSWORD=\"root\" \\\\\\n  -e POSTGRES_DB=\"ny_taxi\" \\\\\\n  -v $(pwd)/ny_taxi_data:/var/lib/postgresql/data \\\\\\n  -p 5432:5432 \\\\\\n  --network=pg-network \\\\\\n  --name=pg_database \\\\\\n  postgres:13\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/053_d3c860aa46_docker-postgres-container-fails-to-launch-with-exi.md'},\n",
       " {'id': '5bd34587a0',\n",
       "  'question': 'WSL: Docker directory permissions error',\n",
       "  'sort_order': 54,\n",
       "  'content': '```\\ninitdb: error: could not change permissions of directory\\n```\\n\\nWSL and Windows do not manage permissions in the same way, causing conflict if using the Windows file system rather than the WSL file system.\\n\\nSolution:  **Use Docker volumes.**\\n\\nVolume is used for storage of persistent data and not for transferring files. A local volume is unnecessary.\\n\\nThis resolves permission issues and allows for better management of volumes.\\n\\n**Note:** The `user:` is not necessary if using Docker volumes but is required if using a local drive.\\n\\n\\n```yaml\\nservices:\\n  postgres:\\n    image: postgres:15-alpine\\n    container_name: postgres\\n    user: \"0:0\"\\n    environment:\\n      - POSTGRES_USER=postgres\\n      - POSTGRES_PASSWORD=postgres\\n      - POSTGRES_DB=ny_taxi\\n    volumes:\\n      - \"pg-data:/var/lib/postgresql/data\"\\n    ports:\\n      - \"5432:5432\"\\n    networks:\\n      - pg-network\\n\\n  pgadmin:\\n    image: dpage/pgadmin4\\n    container_name: pgadmin\\n    user: \"${UID}:${GID}\"\\n    environment:\\n      - PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n      - PGADMIN_DEFAULT_PASSWORD=pgadmin\\n    volumes:\\n      - \"pg-admin:/var/lib/pgadmin\"\\n    ports:\\n      - \"8080:80\"\\n    networks:\\n      - pg-network\\n\\nnetworks:\\n  pg-network:\\n    name: pg-network\\n\\nvolumes:\\n  pg-data:\\n  pg-admin:\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/054_5bd34587a0_wsl-docker-directory-permissions-error.md'},\n",
       " {'id': 'ec091c69a0',\n",
       "  'question': 'WSL: Insufficient system resources exist to complete the requested service.',\n",
       "  'sort_order': 55,\n",
       "  'content': '**Cause:**\\n\\nThis error occurs because some applications are not updated. Specifically, check for any pending updates for Windows Terminal, WSL, and Windows Security updates.\\n\\n**Solution:**\\n\\nTo update Windows Terminal:\\n\\n1. Open the Microsoft Store.\\n2. Go to your library of installed apps.\\n3. Search for Windows Terminal.\\n4. Update the app.\\n5. Restart your system to apply the changes.\\n\\nFor updating Windows Security updates:\\n\\n1. Go to Windows Updates settings.\\n2. Check for any pending updates, especially security updates.\\n3. Restart your system once the updates are downloaded and installed successfully.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/055_ec091c69a0_wsl-insufficient-system-resources-exist-to-complet.md'},\n",
       " {'id': '62f6c8dd8d',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_bc654841.png'}],\n",
       "  'question': 'WSL: WSL integration with distro Ubuntu unexpectedly stopped with exit code 1.',\n",
       "  'sort_order': 56,\n",
       "  'content': '<{IMAGE:image_1}>\\n\\nUpon restarting, the same issue appears and occurs unexpectedly on Windows.\\n\\n**Solutions:**\\n\\n1. **Fixing DNS Issue**\\n   \\n   This solution is credited to [reddit](https://www.reddit.com/r/docker/comments/p98xq6/docker_failed_to_start_exit_code_1/) and has worked for some users.\\n\\n   ```bash\\n   reg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"4\" /f\\n   ```\\n\\n   Restart your computer and then re-enable it with the following command:\\n\\n   ```bash\\n   reg add \"HKLM\\\\System\\\\CurrentControlSet\\\\Services\\\\Dnscache\" /v \"Start\" /t REG_DWORD /d \"2\" /f\\n   ```\\n\\n   Restart your OS again. It should work.\\n\\n2. **Switch to Linux Containers**\\n\\n   - Right-click on the running Docker icon (next to the clock).\\n   - Choose \"Switch to Linux containers.\"\\n\\n```bash\\nbash: conda: command not found\\n```\\n\\n```bash\\nDatabase is uninitialized and superuser password is not specified.\\n```\\n\\n```bash\\nDatabase is uninitialized and superuser password is not specified.\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/056_62f6c8dd8d_wsl-wsl-integration-with-distro-ubuntu-unexpectedl.md'},\n",
       " {'id': '73c8de600c',\n",
       "  'question': 'WSL: Permissions too open at Windows',\n",
       "  'sort_order': 57,\n",
       "  'content': 'Issue when trying to run the GPC VM through SSH via WSL2, likely because WSL2 isn’t looking for .ssh keys in the correct folder. The command attempted:\\n\\n```bash\\nssh -i gpc [username]@[my external IP]\\n```\\n\\n### Solutions\\n\\n1. **Use `sudo` Command**\\n   \\n   Try using `sudo` before executing the command:\\n   \\n   ```bash\\n   sudo ssh -i gpc [username]@[my external IP]\\n   ```\\n\\n2. **Change Permissions**\\n   \\n   Navigate to your folder and change the permissions for the private key SSH file:\\n   \\n   ```bash\\n   chmod 600 gpc\\n   ```\\n\\n3. **Create a `.ssh` Folder in WSL2**\\n   \\n   - Navigate to your home directory:\\n     \\n     ```bash\\n     cd ~\\n     ```\\n   \\n   - Create a `.ssh` folder:\\n     \\n     ```bash\\n     mkdir .ssh\\n     ```\\n   \\n   - Copy the content from the Windows `.ssh` folder to the newly created `.ssh` folder:\\n     \\n     ```bash\\n     cp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\n     ```\\n   \\n   - Adjust the permissions of the files and folders in the `.ssh` directory if necessary.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/057_73c8de600c_wsl-permissions-too-open-at-windows.md'},\n",
       " {'id': '16d0d756c3',\n",
       "  'question': 'WSL: Could not resolve host name',\n",
       "  'sort_order': 58,\n",
       "  'content': 'WSL2 may not be referencing the correct `.ssh/config` path from Windows. You can create a config file in the home directory of WSL2 by following these steps:\\n\\n1. Navigate to your home directory:\\n   \\n   ```bash\\n   cd ~\\n   ```\\n\\n2. Create the `.ssh` directory:\\n   \\n   ```bash\\n   mkdir .ssh\\n   ```\\n\\n3. Create a `config` file in the `.ssh` folder with the following content:\\n\\n   ```\\n   HostName [GPC VM external IP]\\n   User [username]\\n   IdentityFile ~/.ssh/[private key]\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/058_16d0d756c3_wsl-could-not-resolve-host-name.md'},\n",
       " {'id': 'b9cd2aaccb',\n",
       "  'question': 'PGCLI - connection failed: :1), port 5432 failed: could not receive data from server: Connection refused could not send SSL negotiation packet: Connection refused',\n",
       "  'sort_order': 59,\n",
       "  'content': 'To resolve the connection failure with PGCLI, use the following command to connect via socket:\\n\\n```bash\\npgcli -h 127.0.0.1 -p 5432 -u root -d ny_taxi\\n```\\n\\nEnsure the database server is running and properly configured to accept connections.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/059_b9cd2aaccb_pgcli-connection-failed-1-port-5432-failed-could-n.md'},\n",
       " {'id': '8ddf54cbd0',\n",
       "  'question': 'PGCLI: Should we run pgcli inside another docker container?',\n",
       "  'sort_order': 60,\n",
       "  'content': 'In this section of the course, the 5432 port of PostgreSQL is mapped to your computer’s 5432 port. This means you can access the PostgreSQL database via pgcli directly from your computer.\\n\\nSo, no, you don’t need to run it inside another container. Your local system will suffice.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/060_8ddf54cbd0_pgcli-should-we-run-pgcli-inside-another-docker-co.md'},\n",
       " {'id': 'e644e2b7b6',\n",
       "  'question': 'PGCLI - FATAL: password authentication failed for user \"root\" (You already have Postgres)',\n",
       "  'sort_order': 61,\n",
       "  'content': 'For a more visual and detailed explanation, feel free to check the video [1.4.2 - Port Mapping and Networks in Docker](https://www.youtube.com/watch?v=tOr4hTsHOzU&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=16&ab_channel=DataTalksClub%E2%AC%9B).\\n\\nIf you want to debug the issue on MacOS, you can try the following steps:\\n\\n- **Check if something is blocking your port:**\\n  \\n  Use the `lsof` command to find out which application is using a specific port on your local machine:\\n  \\n  ```bash\\n  lsof -i :5432\\n  ```\\n\\n- **List running PostgreSQL services:**\\n\\n  Use `launchctl` to list running postgres services on your local machine.\\n\\n- **Unload the running service:**\\n\\n  Unload the launch agent for the PostgreSQL service, which will stop the service and free up the port:\\n  \\n  ```bash\\n  launchctl unload -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist\\n  ```\\n\\n- **Restart the service:**\\n\\n  ```bash\\n  launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.postgresql.plist\\n  ```\\n\\n- **Change the port:**\\n\\n  Changing the port from `5432:5432` to `5431:5432` can help avoid this error.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/061_e644e2b7b6_pgcli-fatal-password-authentication-failed-for-use.md'},\n",
       " {'id': '05e6bff42c',\n",
       "  'question': \"PGCLI - PermissionError: [Errno 13] Permission denied: '/some/path/.config/pgcli'\",\n",
       "  'sort_order': 62,\n",
       "  'content': 'I encountered this error:\\n\\n```bash\\npgcli -h localhost -p 5432 -U root -d ny_taxi\\n\\nTraceback (most recent call last):\\n  File \"/opt/anaconda3/bin/pgcli\", line 8, in <module>\\n    sys.exit(cli())\\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1128, in __call__\\n    return self.main(*args, **kwargs)\\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1053, in main\\n    rv = self.invoke(ctx)\\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 1395, in invoke\\n    return ctx.invoke(self.callback, **ctx.params)\\n  File \"/opt/anaconda3/lib/python3.9/site-packages/click/core.py\", line 754, in invoke\\n    return __callback(*args, **kwargs)\\n  File \"/opt/anaconda3/lib/python3.9/site-packages/pgcli/main.py\", line 880, in cli\\n    os.makedirs(config_dir)\\n  File \"/opt/anaconda3/lib/python3.9/os.py\", line 225, in makedirspython\\n    mkdir(name, mode)\\nPermissionError: [Errno 13] Permission denied: \\'/Users/vray/.config/pgcli\\'\\n```\\n\\n### Solution 1:\\n\\nThis error indicates that your user doesn’t have the necessary permissions to access or modify the directory or file (`/some/path/.config/pgcli`). This can occur in Docker environments when privileges are assigned to root instead of the current user.\\n\\nTo resolve this:\\n\\n1. Check the file permissions:\\n\\n   ```bash\\n   ls -l /some/path/.config/pgcli\\n   ```\\n\\n2. Change the ownership/permissions so that your user has the necessary permissions:\\n\\n   ```bash\\n   sudo chown -R user_name /Users/user_name/.config\\n   ```\\n\\n   - `sudo` stands for Super User DO.\\n   - `chown` means change owner.\\n   - `-R` applies recursively.\\n   - `user_name` is your PC username (e.g., vray).\\n\\n### Solution 2:\\n\\nMake sure you install pgcli without using `sudo`. The recommended approach is to use conda/anaconda to avoid affecting your system Python.\\n\\nIf `conda install` gets stuck at \"Solving environment,\" try these alternatives:\\n\\n[https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda](https://stackoverflow.com/questions/63734508/stuck-at-solving-environment-on-anaconda)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/062_05e6bff42c_pgcli-permissionerror-errno-13-permission-denied-s.md'},\n",
       " {'id': '0d73dde53e',\n",
       "  'question': 'PGCLI - no pq wrapper available.',\n",
       "  'sort_order': 63,\n",
       "  'content': \"**Error:**\\n\\n```\\nImportError: no pq wrapper available.\\n```\\n\\n### Problem Details:\\n\\n- Could not import `\\\\dt`\\n- `opg 'c' implementation: No module named 'psycopg_c'`\\n- `couldn't import psycopg 'binary' implementation: No module named 'psycopg_binary'`\\n- `couldn't import psycopg 'python' implementation: libpq library not found`\\n\\n### Solution:\\n\\n1. **Check Python Version:**\\n   \\n   Ensure your Python version is at least 3.9. The `'psycopg2-binary'` might fail to install on older versions like 3.7.3.\\n   \\n   ```bash\\n   $ python -V\\n   ```\\n\\n2. **Environment Setup:**\\n\\n   - If your Python version is not 3.9, create a new environment:\\n     \\n     ```bash\\n     $ conda create --name de-zoomcamp python=3.9\\n     $ conda activate de-zoomcamp\\n     ```\\n\\n3. **Install Required Libraries:**\\n\\n   - Install Postgres libraries:\\n     \\n     ```bash\\n     $ pip install psycopg2-binary\\n     $ pip install psycopg_binary\\n     ```\\n\\n4. **Upgrade pgcli:**\\n\\n   - If the above steps do not work, try upgrading `pgcli`:\\n     \\n     ```bash\\n     $ pip install --upgrade pgcli\\n     ```\\n\\n5. **Install pgcli via Conda:**\\n\\n   - Make sure to also install `pgcli` using conda:\\n     \\n     ```bash\\n     $ conda install -c conda-forge pgcli\\n     ```\\n\\nIf you follow these steps, you should be able to resolve the issue.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/063_0d73dde53e_pgcli-no-pq-wrapper-available.md'},\n",
       " {'id': '6704978d67',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_93f08019.png'}],\n",
       "  'question': 'PGCLI - stuck on password prompt',\n",
       "  'sort_order': 64,\n",
       "  'content': 'If your Bash prompt is stuck on the password command for postgres:\\n\\n<{IMAGE:image_1}>\\n\\nUse `winpty`:\\n\\n```bash\\nwinpty pgcli -h localhost -p 5432 -u root -d ny_taxi\\n```\\n\\nAlternatively, try using Windows Terminal or the terminal in VS Code.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/064_6704978d67_pgcli-stuck-on-password-prompt.md'},\n",
       " {'id': '12dffd5b1a',\n",
       "  'images': None,\n",
       "  'question': 'PGCLI -connection failed: FATAL: password authentication failed for user \"root\"',\n",
       "  'sort_order': 65,\n",
       "  'content': 'The error above was faced continually despite inputting the correct password.\\n\\n\\n1. **Stop the PostgreSQL service on Windows**\\n\\n2. **Using WSL:** Completely uninstall PostgreSQL 12 from Windows and install `postgresql-client` on WSL:  \\n\\n```bash\\nsudo apt install postgresql-client-common postgresql-client libpq-dev\\n```\\n\\n3. **Change the port of the Docker container**\\n\\n4. **Keep the Database Connection:**\\n   \\nIf you encounter the error:\\n   \\n```\\nPGCLI -connection failed: FATAL: password authentication failed for user \"root\"\\n```\\n   \\nIt might be because the connection to the `Postgres:13` image was closed. Ensure you keep the database connected in order to continue with the tutorial steps, using the following command:\\n\\n```bash\\ndocker run -it \\\\\\n   -e POSTGRES_USER=root \\\\\\n   -e POSTGRES_PASSWORD=root \\\\\\n   -e POSTGRES_DB=ny_taxi \\\\\\n   -v d:/git/data-engineering-zoomcamp/week_1/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n   -p 5432:5432 \\\\\\n   postgres:13\\n```\\n\\nYou should see this \\n\\n```\\n2024-01-26 20:14:43.124 UTC [1] LOG:  database system is ready to accept connections\\n```\\n\\n5. **Change the Port for Docker PostgreSQL:**\\n\\n   After running the command `pgcli -h localhost -p 5432 -u root -d ny_taxi`, if prompted for a password, the error may persist due to local Postgres installation. To resolve this port conflict between host and container:\\n\\n   - Configure your Docker PostgreSQL container to use a different port. Map it to a different port on your host machine:\\n   \\n```bash\\ndocker run -it \\\\\\n   -e POSTGRES_USER=\"root\" \\\\\\n   -e POSTGRES_PASSWORD=\"root\" \\\\\\n   -e POSTGRES_DB=\"ny_taxi\" \\\\\\n   -v c:/workspace/de-zoomcamp/1_intro_to_data_engineering/docker_sql/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n   -p 5433:5432 \\\\\\n   postgres:13\\n```\\n\\n- `5433` refers to the port on the host machine.\\n- `5432` refers to the port inside the Docker Postgres container.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/065_12dffd5b1a_pgcli-connection-failed-fatal-password-authenticat.md'},\n",
       " {'id': 'aa8c30b777',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_b33cbd22.png'}],\n",
       "  'question': 'PGCLI - pgcli: command not found',\n",
       "  'sort_order': 66,\n",
       "  'content': \"### Problem\\n\\nIf you have already installed `pgcli` but Bash or the Windows Terminal doesn't recognize the command:\\n\\n- On Git Bash: \\n  ```bash\\n  bash: pgcli: command not found\\n  ```\\n- On Windows Terminal: \\n  ```\\n  pgcli: The term 'pgcli' is not recognized…\\n  ```\\n\\n### Solution\\n\\nTry adding the Python path to the Windows PATH variable:\\n\\n1. Use the command to get the location:\\n   ```bash\\n   pip list -v\\n   ```\\n2. Copy the path, which looks like:\\n   ```\\n   C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\site-packages\\n   ```\\n3. Replace `site-packages` with `Scripts`:\\n   ```\\n   C:\\\\Users\\\\...\\\\AppData\\\\Roaming\\\\Python\\\\Python39\\\\Scripts\\n   ```\\n\\nIt might be that Python is installed elsewhere. For example, it could be under:\\n\\n- `c:\\\\python310\\\\lib\\\\site-packages`\\n\\nIn that case, you should add:\\n\\n- `c:\\\\python310\\\\lib\\\\Scripts` to PATH.\\n\\n### Instructions\\n\\n- Add the determined path to `Path` (or `PATH`) in System Variables.\\n\\n<{IMAGE:image_1}>\\n\\n### Reference\\n\\n[Stack Overflow Reference](https://stackoverflow.com/a/68233660)\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/066_aa8c30b777_pgcli-pgcli-command-not-found.md'},\n",
       " {'id': 'c6a6a991d8',\n",
       "  'question': 'PGCLI - running in a Docker container',\n",
       "  'sort_order': 67,\n",
       "  'content': \"If running pgcli locally causes issues or you do not want to install it on your machine, you can use it within a Docker container instead.\\n\\nBelow is the usage with values used in the course videos for:\\n\\n- Network name (Docker network)\\n- Postgres-related variables for pgcli\\n- Hostname\\n- Username\\n- Port\\n- Database name\\n\\n```bash\\ndocker run -it --rm --network pg-network ai2ys/dockerized-pgcli:4.0.1\\n```\\n\\nThen execute the following pgcli command:\\n\\n```sql\\npgcli -h pg-database -U root -p 5432 -d ny_taxi\\n```\\n\\nYou'll be prompted for the password for the user `root`.\\n\\nExample Output:\\n\\n```\\nServer: PostgreSQL 16.1 (Debian 16.1-1.pgdg120+1)\\nVersion: 4.0.1\\nHome: [pgcli.com](http://pgcli.com)\\n```\\n\\nTo list tables:\\n\\n```sql\\nroot@pg-database:ny_taxi> \\\\dt\\n\\n+--------+------------------+-------+-------+\\n| Schema | Name             | Type  | Owner |\\n|--------+------------------+-------+-------|\\n| public | yellow_taxi_data | table | root  |\\n+--------+------------------+-------+-------+\\n\\nSELECT 1\\nTime: 0.009s\\nroot@pg-database:ny_taxi>\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/067_c6a6a991d8_pgcli-running-in-a-docker-container.md'},\n",
       " {'id': '45ffd3e213',\n",
       "  'question': 'RRPGCLI: Case sensitive use of “Quotations” around columns with capital letters',\n",
       "  'sort_order': 68,\n",
       "  'content': '`PULocationID` will not be recognized, but `\"PULocationID\"` will be. This is because unquoted identifiers are case insensitive. [See docs](https://www.postgresql.org/docs/current/sql-syntax-lexical.html#SQL-SYNTAX-IDENTIFIERS).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/068_45ffd3e213_rrpgcli-case-sensitive-use-of-quotations-around-co.md'},\n",
       " {'id': '0a6a3ccf35',\n",
       "  'question': 'PGCLI - error column c.relhasoids does not exist',\n",
       "  'sort_order': 69,\n",
       "  'content': 'When using the command `\\\\d <database name>` you get the error `column c.relhasoids does not exist`.\\n\\nResolution:\\n\\n1. Uninstall pgcli.\\n2. Reinstall pgcli.\\n3. Restart your PC.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/069_0a6a3ccf35_pgcli-error-column-crelhasoids-does-not-exist.md'},\n",
       " {'id': 'f291e8d311',\n",
       "  'question': 'Postgres: bind: address already in use',\n",
       "  'sort_order': 70,\n",
       "  'content': '### Issue\\n\\nWhen attempting to start the Docker Postgres container, you may encounter the error message:\\n\\n```\\nError - postgres port is already in use.\\n```\\n\\n### Solutions\\n\\n#### Option 1: Identify and Stop the Service\\n\\n1. Determine which service is using the port by running:\\n   \\n   ```bash\\n   sudo lsof -i :5432\\n   ```\\n   \\n2. Stop the service that is using the port:\\n   \\n   ```bash\\n   sudo service postgresql stop\\n   ```\\n\\n#### Option 2: Map to a Different Port\\n\\nFor a more long-term solution, consider mapping to a different port:\\n\\n- Map local port 5433 to container port 5432 in your Docker configuration (`Dockerfile` or `docker-compose.yml`).\\n- If using a VM, ensure that port 5433 is forwarded in the host machine configuration.\\n\\nThis approach prevents conflicts and allows the Docker Postgres container to run without interruption.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/070_f291e8d311_postgres-bind-address-already-in-use.md'},\n",
       " {'id': '7f7aa5f5e6',\n",
       "  'question': 'PGCLI - After installing PGCLI and checking with `pgcli --help` we get the error: `ImportError: no pq wrapper available`',\n",
       "  'sort_order': 71,\n",
       "  'content': \"The error persists because the psycopg library cannot find the required libpq library. Ensure the required PostgreSQL client library is installed:\\n\\n```bash\\nsudo apt install libpq-dev\\n```\\n\\nRebuild psycopg:\\n\\n1. Uninstall the existing packages:\\n   \\n   ```bash\\n   pip uninstall psycopg psycopg_binary psycopg_c -y\\n   ```\\n\\n2. Reinstall psycopg:\\n   \\n   ```bash\\n   pip install psycopg --no-binary psycopg\\n   ```\\n\\nThe issue should be resolved by now. However, if you still encounter the error:\\n\\n`ModuleNotFoundError: No module named 'psycopg2'`\\n\\nThen run the following:\\n\\n```bash\\npip install psycopg2-binary\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/071_7f7aa5f5e6_pgcli-after-installing-pgcli-and-checking-with-pgc.md'},\n",
       " {'id': '409296db3e',\n",
       "  'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL: password authentication failed for user \"root\"',\n",
       "  'sort_order': 72,\n",
       "  'content': \"This error occurs when uploading data via a connection in Jupyter Notebook:\\n\\n```python\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\n```\\n\\n### Possible Solutions:\\n\\n1. **Port Conflict:**\\n   - Port 5432 might be occupied by another Postgres installation on your local machine. This can lead to your connection not reaching Docker.\\n   - Try using a different port, such as 5431, or verify the port mapping.\\n   - Alternatively, remove any old or unnecessary Postgres installations if they're not in use.\\n\\n2. **Windows Service Check:**\\n   - Check for any running services on Windows that might be using Postgres.\\n   - Stopping such services might resolve the issue.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/072_409296db3e_postgres-operationalerror-psycopg2operationalerror.md'},\n",
       " {'id': '60c212c0e8',\n",
       "  'question': 'Postgres: connection failed: connection to server at \"127.0.0.1\", port 5432 failed: FATAL: password authentication failed for user \"root\"',\n",
       "  'sort_order': 73,\n",
       "  'content': 'To resolve the issue of a failed connection to PostgreSQL due to password authentication, consider the following steps:\\n\\n- **Check Port Usage**: Ensure that port 5432 is properly forwarded. If it is being used by another process, follow these steps to kill it:\\n  \\n  ```bash\\n  sudo lsof -i :5432\\n  sudo kill -9 PID\\n  ```\\n\\n- **For Windows Users**: If PostgreSQL is running locally and pgAdmin4 is using the 5432 port, follow these instructions:\\n  \\n  1. Press **Win + R** to open the Run dialog.\\n  2. Type `services.msc` and press Enter.\\n  3. In the Services window, scroll down to find a service named like `PostgreSQL`, `postgresql-x64-13`, or similar, depending on your PostgreSQL version.\\n  4. Right-click the PostgreSQL service and select **Stop**.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/073_60c212c0e8_postgres-connection-failed-connection-to-server-at.md'},\n",
       " {'id': '3459487271',\n",
       "  'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  role \"root\" does not exist',\n",
       "  'sort_order': 74,\n",
       "  'content': \"This error can occur in the following scenarios:\\n\\n- **Using `pgcli`**:\\n  ```bash\\n  pgcli -h localhost -p 5432 -U root -d ny_taxi\\n  ```\\n- **Uploading data via a connection in a Jupyter notebook**:\\n  ```python\\n  engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\n  ```\\n\\n### Solutions:\\n\\n1. **Port Change**:\\n   - Change the port from 5432 to another port (e.g., 5431).\\n   - Example: Change `5432:5432` to `5431:5432`.\\n\\n2. **User Change**:\\n   - Change `POSTGRES_USER=root` to `PGUSER=postgres`.\\n\\n3. **Docker Solution**:\\n   - Run `docker compose down`.\\n   - Remove the folder containing the Postgres volume.\\n   - Run `docker compose up` again.\\n\\n### Additional Resources:\\nFor more details, refer to [this Stack Overflow discussion](https://stackoverflow.com/questions/60193781/postgres-with-docker-compose-gives-fatal-role-root-does-not-exist-error).\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/074_postgres-operationalerror.md'},\n",
       " {'id': '6f27b71a1f',\n",
       "  'question': 'Postgres - OperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist',\n",
       "  'sort_order': 75,\n",
       "  'content': '```\\nOperationalError: (psycopg2.OperationalError) connection to server at \"localhost\" (::1), port 5432 failed: FATAL:  database \"ny_taxi\" does not exist\\n```\\n\\nMake sure PostgreSQL is running. You can check that by running:\\n\\n```bash\\ndocker ps\\n```\\n\\n**Solution:**\\n\\n- If you have PostgreSQL software installed on your computer previously, consider building your instance on a different port, such as 8080, instead of 5432.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/075_6f27b71a1f_postgres-operationalerror-psycopg2operationalerror.md'},\n",
       " {'id': '52858dfd98',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_b7e005cb.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_c56a8539.png'}],\n",
       "  'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "  'sort_order': 76,\n",
       "  'content': \"Issue:\\n\\n```\\nModuleNotFoundError: No module named 'psycopg2'\\n```\\n\\n<IMAGE:image_1>\\n\\n<IMAGE:image_2>\\n\\nSolution:\\n\\n1. Install psycopg2-binary:\\n   \\n   ```bash\\n   pip install psycopg2-binary\\n   ```\\n\\n2. If psycopg2-binary is already installed, update it:\\n   \\n   ```bash\\n   pip install psycopg2-binary --upgrade\\n   ```\\n\\n3. Other methods if the above fails:\\n\\n   - If the error persists, update conda:\\n     \\n     ```bash\\n     conda update -n base -c defaults conda\\n     ```\\n\\n   - Alternatively, update pip:\\n     \\n     ```bash\\n     pip install --upgrade pip\\n     ```\\n\\n   - Reinstall psycopg:\\n     \\n     - Uninstall the psycopg package.\\n     - Update conda or pip.\\n     - Reinstall psycopg using pip.\\n\\n   - If an error shows about `pg_config` not being found, install PostgreSQL:\\n     \\n     - On Mac, use:\\n       \\n       ```bash\\n       brew install postgresql\\n       ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/076_52858dfd98_postgres-modulenotfounderror-no-module-named-psyco.md'},\n",
       " {'id': '4277a9cb86',\n",
       "  'question': 'Postgres: \"Column does not exist\" but it actually does (Pyscopg2 error in MacBook Pro M2)',\n",
       "  'sort_order': 77,\n",
       "  'content': 'In join queries, if you mention the column name directly or enclose it in single quotes, you\\'ll encounter an error saying \"column does not exist\".\\n\\n**Solution:** Enclose the column names in double quotes, and it will work correctly.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/077_4277a9cb86_postgres-column-does-not-exist-but-it-actually-doe.md'},\n",
       " {'id': '117164c439',\n",
       "  'question': 'pgAdmin: Create server dialog does not appear',\n",
       "  'sort_order': 78,\n",
       "  'content': 'pgAdmin has a new version. The create server dialog may not appear. Try using `Register` -> `Server` instead.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/078_117164c439_pgadmin-create-server-dialog-does-not-appear.md'},\n",
       " {'id': '0cfd7f7ed6',\n",
       "  'question': 'pgAdmin - Blank/white screen after login (browser)',\n",
       "  'sort_order': 79,\n",
       "  'content': 'Using GitHub Codespaces in the browser resulted in a blank screen after logging into pgAdmin (running in a Docker container). The terminal of the pgAdmin container was showing the following error message:\\n\\n```\\nCSRFError: 400 Bad Request: The referrer does not match the host.\\n```\\n\\n### Solution #1:\\n\\nAs recommended in the following issue: [GitHub Issue #5432](https://github.com/pgadmin-org/pgadmin4/issues/5432), setting the following environment variable solved it:\\n\\n```bash\\ndocker run --rm -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"root\" \\\\\\n  -e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n  -p \"8080:80\" \\\\\\n  --name pgadmin \\\\\\n  --network=pg-network \\\\\\n  dpage/pgadmin4:8.2\\n```\\n\\n### Solution #2:\\n\\nUsing the locally installed VSCode to display GitHub Codespaces. When using GitHub Codespaces in the locally installed VSCode (opening a Codespace or creating/starting one), this issue did not occur.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/079_0cfd7f7ed6_pgadmin-blankwhite-screen-after-login-browser.md'},\n",
       " {'id': '2efd03d7f8',\n",
       "  'question': 'pgAdmin - Can not access/open the PgAdmin address via browser',\n",
       "  'sort_order': 80,\n",
       "  'content': 'I am using a Mac Pro device and connect to the GCP Compute Engine via Remote SSH - VSCode. But when trying to run the PgAdmin container via `docker run` or `docker compose`, I couldn\\'t access the PgAdmin address via my browser. After modifications, I was able to access it.\\n\\n### Solution #1:\\n\\nModify the `docker run` command:\\n\\n```bash\\ndocker run --rm -it \\\\\\n  -e PGADMIN_DEFAULT_EMAIL=\"admin@admin.com\" \\\\\\n  -e PGADMIN_DEFAULT_PASSWORD=\"pgadmin\" \\\\\\n  -e PGADMIN_CONFIG_WTF_CSRF_ENABLED=\"False\" \\\\\\n  -e PGADMIN_LISTEN_ADDRESS=0.0.0.0 \\\\\\n  -e PGADMIN_LISTEN_PORT=5050 \\\\\\n  -p 5050:5050 \\\\\\n  --network=de-zoomcamp-network \\\\\\n  --name pgadmin-container \\\\\\n  --link postgres-container \\\\\\n  -t dpage/pgadmin4\\n```\\n\\n### Solution #2:\\n\\nModify the `docker-compose.yaml` configuration and use the `docker compose up` command:\\n\\n```yaml\\npgadmin:\\n  image: dpage/pgadmin4\\n  container_name: pgadmin-container\\n  environment:\\n    - PGADMIN_DEFAULT_EMAIL=admin@admin.com\\n    - PGADMIN_DEFAULT_PASSWORD=pgadmin\\n    - PGADMIN_CONFIG_WTF_CSRF_ENABLED=False\\n    - PGADMIN_LISTEN_ADDRESS=0.0.0.0\\n    - PGADMIN_LISTEN_PORT=5050\\n  volumes:\\n    - \"./pgadmin_data:/var/lib/pgadmin/data\"\\n  ports:\\n    - \"5050:5050\"\\n  networks:\\n    - de-zoomcamp-network\\n  depends_on:\\n    - postgres-container\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/080_2efd03d7f8_pgadmin-can-not-accessopen-the-pgadmin-address-via.md'},\n",
       " {'id': 'ca3b8ac8db',\n",
       "  'question': 'pgAdmin: How to Persist pgAdmin Configurations',\n",
       "  'sort_order': 81,\n",
       "  'content': \"To keep pgAdmin settings after restarting the container, follow these steps:\\n\\n1. Create the directory for pgAdmin data:\\n   \\n   ```bash\\n   mkdir -p /path/to/pgadmin-data\\n   ```\\n\\n2. Assign ownership to pgAdmin's user (ID 5050):\\n   \\n   ```bash\\n   sudo chown -R 5050:5050 /path/to/pgadmin-data\\n   ```\\n\\n3. Set the appropriate permissions:\\n   \\n   ```bash\\n   sudo chmod -R 755 /path/to/pgadmin-data\\n   ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/081_ca3b8ac8db_pgadmin-how-to-persist-pgadmin-configurations.md'},\n",
       " {'id': '3124f13c7c',\n",
       "  'question': 'pgAdmin - Unable to connect to server: [Errno -3] Try again',\n",
       "  'sort_order': 82,\n",
       "  'content': 'This error occurs when connecting pgAdmin with Docker Postgres. In the tutorial, the pgAdmin server creation under **Connection > Host name/address** uses `pg-database` and results in the above-mentioned error when saved.\\n\\n### Solution 1:\\n\\n- Verify that both containers are connected to `pg-network`:\\n  \\n  ```bash\\n  docker network inspect pg-network\\n  ```\\n\\n- If the Docker Postgres container is not connected, connect it to `pg-network`:\\n\\n  ```bash\\n  docker network connect pg-network postgresContainer_name\\n  ```\\n  \\n- Retry the connection. If the error persists, instead of using `pg-database` under **Connection > Host name/address**, try using the IP Address:\\n\\n  - Use the IP address of the `postgresContainer_name` container (e.g., `172.19.0.3`) in the pgAdmin configuration instead of the container name or `pg-database`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/082_3124f13c7c_pgadmin-unable-to-connect-to-server-errno-3-try-ag.md'},\n",
       " {'id': '646e2067e8',\n",
       "  'question': \"Python - ModuleNotFoundError: No module named 'pysqlite2'\",\n",
       "  'sort_order': 83,\n",
       "  'content': '```\\nImportError: DLL load failed while importing _sqlite3: The specified module could not be found. \\nModuleNotFoundError: No module named \\'pysqlite2\\'\\n```\\n\\nThe issue may arise due to the absence of `sqlite3.dll` in the path `\".\\\\Anaconda\\\\Dlls\\\\\"`.\\n\\nTo resolve the issue:\\n\\n1. Copy the `sqlite3.dll` file from `\\\\Anaconda3\\\\Library\\\\bin`.\\n2. Paste the file into the `\".\\\\Anaconda\\\\Dlls\\\\\"` directory.\\n\\nThis solution applies if you are using Anaconda.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/083_646e2067e8_python-modulenotfounderror-no-module-named-pysqlit.md'},\n",
       " {'id': '0d6dc0d041',\n",
       "  'question': 'Python: Ingestion with Jupyter notebook - missing 100000 records',\n",
       "  'sort_order': 84,\n",
       "  'content': 'If you follow the video [1.2.2 - Ingesting NY Taxi Data to Postgres](https://www.youtube.com/watch?v=2JM-ziJt0WI&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=5) and execute the same steps, you will ingest all the data (~1.3 million rows) into the table `yellow_taxi_data`. However, running the whole script in the Jupyter notebook for a second time from top to bottom will result in missing the first chunk of 100,000 records. This occurs because a call to the iterator appears before the while loop, leading to the second chunk being ingested first.\\n\\n### Solution:\\n\\n- Remove the cell `df=next(df_iter)` located higher up in the notebook than the while loop.\\n- Ensure the first `w(df_iter)` call is within the while loop.\\n\\n📔 **Note:** The notebook is used to test the code and is not intended to be run top to bottom. The logic is organized in a later step when inserted into a `.py` file for the pipeline.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/084_0d6dc0d041_python-ingestion-with-jupyter-notebook-missing-100.md'},\n",
       " {'id': '5b4c133384',\n",
       "  'question': 'IPython - Pandas parsing dates with \"read_csv\"',\n",
       "  'sort_order': 85,\n",
       "  'content': 'Pandas can interpret \"string\" column values as \"datetime\" directly when reading the CSV file using `pd.read_csv` with the `parse_dates` parameter. This can include a list of column names or column indices, eliminating the need for conversion afterward.\\n\\n[Reference: pandas.read_csv documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html)\\n\\n**Example from Week 1:**\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.read_csv(\\n    \\'yellow_tripdata_2021-01.csv\\',\\n    nrows=100,\\n    parse_dates=[\\'tpep_pickup_datetime\\', \\'tpep_dropoff_datetime\\']\\n)\\n\\ndf.info()\\n```\\n\\n**Output:**\\n\\n```\\n<class \\'pandas.core.frame.DataFrame\\'>\\nRangeIndex: 100 entries, 0 to 99\\nData columns (total 18 columns):\\n #   Column                 Non-Null Count  Dtype          \\n---  ------                 --------------  -----          \\n 0   VendorID               100 non-null    int64          \\n 1   tpep_pickup_datetime   100 non-null    datetime64[ns] \\n 2   tpep_dropoff_datetime  100 non-null    datetime64[ns] \\n 3   passenger_count        100 non-null    int64          \\n 4   trip_distance          100 non-null    float64        \\n 5   RatecodeID             100 non-null    int64          \\n 6   store_and_fwd_flag     100 non-null    object         \\n 7   PULocationID           100 non-null    int64          \\n 8   DOLocationID           100 non-null    int64          \\n 9   payment_type           100 non-null    int64          \\n 10  fare_amount            100 non-null    float64        \\n 11  extra                  100 non-null    float64        \\n 12  mta_tax                100 non-null    float64        \\n 13  tip_amount             100 non-null    float64        \\n 14  tolls_amount           100 non-null    float64        \\n 15  improvement_surcharge  100 non-null    float64        \\n 16  total_amount           100 non-null    float64        \\n 17  congestion_surcharge   100 non-null    float64        \\ndtypes: datetime64[ns](2), float64(9), int64(6), object(1)\\nmemory usage: 14.2+ KB\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/085_5b4c133384_ipython-pandas-parsing-dates-with-read_csv.md'},\n",
       " {'id': 'e80e8216d5',\n",
       "  'question': \"Python: Python can't ingest data from the GitHub link provided using curl\",\n",
       "  'sort_order': 86,\n",
       "  'content': '```python\\nos.system(f\"curl -LO {url} -o {csv_name}\")\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/086_e80e8216d5_python-python-cant-ingest-data-from-the-github-lin.md'},\n",
       " {'id': 'e0c1900c47',\n",
       "  'question': 'Python: Pandas can read *.csv.gzip',\n",
       "  'sort_order': 87,\n",
       "  'content': 'When a CSV file is compressed using Gzip, it is saved with a \".csv.gz\" file extension. This file type is also known as a Gzip compressed CSV file. To read a Gzip compressed CSV file using Pandas, you can use the `read_csv()` function.\\n\\nHere is an example of how to read a Gzip compressed CSV file using Pandas:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.read_csv(\\'file.csv.gz\\',\\n                 compression=\\'gzip\\',\\n                 low_memory=False)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/087_e0c1900c47_python-pandas-can-read-csvgzip.md'},\n",
       " {'id': 'fbfa475350',\n",
       "  'question': 'Python: How to iterate through and ingest parquet file',\n",
       "  'sort_order': 88,\n",
       "  'content': 'Contrary to pandas’ `read_csv` method, there’s no simple way to iterate through and set chunksize for parquet files. We can use PyArrow (Apache Arrow Python bindings) to resolve that.\\n\\n```python\\nimport pyarrow.parquet as pq\\nfrom sqlalchemy import create_engine\\nimport time\\n\\noutput_name = \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2021-01.parquet\"\\n\\nparquet_file = pq.ParquetFile(output_name)\\nparquet_size = parquet_file.metadata.num_rows\\n\\nengine = create_engine(f\\'postgresql://{user}:{password}@{host}:{port}/{db}\\')\\n\\ntable_name = \"yellow_taxi_schema\"\\n\\n# Clear table if exists\\npq.read_table(output_name).to_pandas().head(n=0).to_sql(name=table_name, con=engine, if_exists=\\'replace\\')\\n\\n# Default (and max) batch size\\nindex = 65536\\n\\nfor i in parquet_file.iter_batches(use_threads=True):\\n    t_start = time.time()\\n    print(f\\'Ingesting {index} out of {parquet_size} rows ({index / parquet_size:.0%})\\')\\n    i.to_pandas().to_sql(name=table_name, con=engine, if_exists=\\'append\\')\\n    index += 65536\\n    t_end = time.time()\\n    print(f\\'\\\\t- it took %.1f seconds\\' % (t_end - t_start))\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/088_fbfa475350_python-how-to-iterate-through-and-ingest-parquet-f.md'},\n",
       " {'id': '26ff32cb35',\n",
       "  'question': \"Python - SQLAlchemy - ImportError: cannot import name 'TypeAliasType' from 'typing_extensions'.\",\n",
       "  'sort_order': 89,\n",
       "  'content': 'The following error occurs during the execution of a Jupyter notebook cell:\\n\\n```python\\nfrom sqlalchemy import create_engine\\n```\\n\\nSolution:\\n\\nThe issue can be resolved by ensuring the version of the Python module `typing_extensions` is 4.6.0 or later. You can update it using either Conda or pip:\\n\\n- **Using Conda:**\\n  ```bash\\n  conda update typing_extensions\\n  ```\\n\\n- **Using pip:**\\n  ```bash\\n  pip install --upgrade typing_extensions\\n  ```\\n\\nFor more details, you can refer to the [changelog for typing_extensions 4.6.0](https://github.com/python/typing_extensions/blob/main/CHANGELOG.md#release-460-may-22-2023).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/089_26ff32cb35_python-sqlalchemy-importerror-cannot-import-name-t.md'},\n",
       " {'id': '5d8ffd5be5',\n",
       "  'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "  'sort_order': 90,\n",
       "  'content': 'When using `create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')`, you may encounter the error:\\n\\n```\\nTypeError: \\'module\\' object is not callable\\n```\\nUse the correct connection string syntax:\\n\\n\\n```python\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/090_5d8ffd5be5_python-sqlalchemy-typeerror-module-object-is-not-c.md'},\n",
       " {'id': 'e6e9a25246',\n",
       "  'question': \"Python: SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "  'sort_order': 91,\n",
       "  'content': \"Error raised during the Jupyter Notebook cell execution:\\n\\n```python\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi')\\n```\\n\\nSolution:\\n\\nInstall the Python module `psycopg2`. It can be installed using Conda or pip:\\n\\n- Using Conda:\\n  ```bash\\n  conda install psycopg2\\n  ```\\n- Using pip:\\n  ```bash\\n  pip install psycopg2\\n  ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/091_e6e9a25246_python-sqlalchemy-modulenotfounderror-no-module-na.md'},\n",
       " {'id': '1f21373dff',\n",
       "  'question': \"Python - SQLAlchemy: NoSuchModuleError: Can't load plugin: sqlalchemy.dialects:postgresql.psycopg\",\n",
       "  'sort_order': 92,\n",
       "  'content': 'Error raised during the Jupyter notebook’s cell execution:\\n\\n```python\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\n\\nengine = create_engine(conn_string)\\n```\\n\\nSolution: \\n\\nWe had a scenario of a virtual environment (created by PyCharm) being run on top of another virtual environment (on conda). The solution was:\\n\\n1. Remove the `.venv`.\\n2. Create a new virtual environment with conda:\\n   \\n   ```bash\\n   conda create -n pyingest python=3.12\\n   ```\\n\\n3. Install the required dependencies:\\n   \\n   ```bash\\n   pip install pandas sqlalchemy psycopg2-binary jupyterlab\\n   ```\\n\\n4. Re-execute the code.\\n\\nFor `psycopg2`, the connection string should be:\\n\\n```python\\npostgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\\n```\\n\\nReference - Kayla Tinker 1/14/25',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/092_1f21373dff_python-sqlalchemy-nosuchmoduleerror-cant-load-plug.md'},\n",
       " {'id': '5f8cf90bb4',\n",
       "  'question': 'Python - SQLAlchemy - read_sql_query() throws \"\\'OptionEngine\\' object has no attribute \\'execute\\'\"',\n",
       "  'sort_order': 93,\n",
       "  'content': 'First, check the versions of SQLAlchemy and Pandas to ensure they are both up-to-date. You can upgrade them using `pip` or `conda` if needed.\\n\\nThen, try to wrap the query using `text`:\\n\\n```python\\nfrom sqlalchemy import text\\n\\nquery = text(\"SELECT * FROM tbl\")\\ndf = pd.read_sql_query(query, conn)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/093_5f8cf90bb4_python-sqlalchemy-read_sql_query-throws-optionengi.md'},\n",
       " {'id': 'bd56924fcb',\n",
       "  'question': 'GCP: Static vs Ephemeral IP / Setting up static IP for VM',\n",
       "  'sort_order': 94,\n",
       "  'content': 'When you set up a VM in Google Cloud Platform (GCP), it initially uses an ephemeral IP address, which changes each time you start or stop the VM. If you need a consistent IP for your configuration file, you should set up a static IP address.\\n\\n### Steps to Set Up a Static IP Address\\n\\n1. Navigate to **VPC Network** > **IP addresses** in the GCP console.\\n2. Allocate a new static IP address.\\n3. Attach the static IP to your VM instance.\\n\\n> **Note:** You are charged for a static IP if it is not allocated to a specific VM, so make sure it is attached to avoid extra fees.\\n\\nFor detailed instructions, consult the [GCP documentation](https://cloud.google.com/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/094_bd56924fcb_gcp-static-vs-ephemeral-ip-setting-up-static-ip-fo.md'},\n",
       " {'id': '25b0348672',\n",
       "  'question': 'GCP: Unable to add Google Cloud SDK PATH to Windows',\n",
       "  'sort_order': 95,\n",
       "  'content': \"### Issue\\n\\nWindows error:\\n\\n```\\nThe installer is unable to automatically update your system PATH. Please add C:\\\\tools\\\\google-cloud-sdk\\\\bin\\n```\\n\\n### Solution\\n\\nIf you encounter this error frequently, consider the following steps:\\n\\n1. **Add Gitbash to Windows Path:**\\n   \\n   - **Using Conda:**\\n     - Download the Anaconda Navigator.\\n     - During installation, check the box to add Conda to the path (even though it's not recommended).\\n\\n2. **Install Git Bash:**\\n\\n   - If not installed, install Git Bash.\\n   - If installed, consider reinstalling it.\\n   - During installation, ensure you check:\\n     - Add GitBash to Windows Terminal\\n     - Use Git and optional Unix tools from the command prompt\\n\\n3. **Setup Git Bash:**\\n\\n   - Open Git Bash and type the following command:\\n     \\n     ```bash\\n     conda init bash\\n     ```\\n   \\n   - This will modify your bash profile.\\n\\n4. **Set Gitbash as Default Terminal:**\\n\\n   - Open the Windows Terminal.\\n   - Go to settings.\\n   - Change the default profile from Windows PowerShell to Git Bash.\\n\\nBy following these steps, you should be able to add the Google Cloud SDK path to your system on Windows without issues.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/095_25b0348672_gcp-unable-to-add-google-cloud-sdk-path-to-windows.md'},\n",
       " {'id': 'bad6a11a5a',\n",
       "  'question': 'GCP: Project creation failed: HttpError accessing … Requested entity already exists',\n",
       "  'sort_order': 96,\n",
       "  'content': 'When creating a project in GCP, you may encounter the following error:\\n\\n```json\\nWARNING: Project creation failed: HttpError accessing cloudresourcemanager.googleapis.com: response: {\\n  \\'content-type\\': \\'application/json; charset=UTF-8\\',\\n  \\'status\\': 409\\n}, content {\\n  \"error\": {\\n    \"code\": 409,\\n    \"message\": \"Requested entity already exists\",\\n    \"status\": \"ALREADY_EXISTS\"\\n  }\\n}\\n```\\n\\n### Explanation\\n\\nThis error occurs when the project ID you are trying to use is already taken. Project IDs are unique across all GCP projects. If any user ever had a project with that ID, you cannot use it.\\n\\n### Solution\\n\\n- Choose a different, more unique project ID. Avoid common names like `testproject` as they are likely to be already in use.\\n\\nFor more details, refer to the discussion: [Stack Overflow](https://stackoverflow.com/questions/52561383/gcloud-cli-cannot-create-project-the-project-id-you-specified-is-already-in-us?rq=1)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/096_bad6a11a5a_gcp-project-creation-failed-httperror-accessing-re.md'},\n",
       " {'id': 'bc90da941d',\n",
       "  'question': 'GCP: The project to be billed is associated with an absent billing account',\n",
       "  'sort_order': 97,\n",
       "  'content': 'If you receive the error:\\n\\n```\\nError 403: The project to be billed is associated with an absent billing account., accountDisabled\\n```\\n\\nIt is most likely because you did not enter your project ID correctly. The value you enter should be unique to your project. You can find this value on your GCP Dashboard when you log in.\\n\\nAnother possibility is that you have not linked your billing account to your current project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/097_bc90da941d_gcp-the-project-to-be-billed-is-associated-with-an.md'},\n",
       " {'id': '53b423d784',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_993adb67.png'}],\n",
       "  'question': 'GCP: OR-CBAT-15 ERROR Google cloud free trial account',\n",
       "  'sort_order': 98,\n",
       "  'content': 'If Google refuses your credit/debit card, try using a different one. For instance, a card from Kaspi (Kazakhstan) might not work, but a card from TBC (Georgia) does.\\n\\nUnfortunately, support assistance might not be highly effective in resolving this issue.\\n\\nAdditionally, a Pyypl web-card can be a viable alternative.\\n\\n```json\\nny-rides.json\\n```\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/098_53b423d784_gcp-or-cbat-15-error-google-cloud-free-trial-accou.md'},\n",
       " {'id': 'cdd91ef84a',\n",
       "  'question': 'GCP: Where can I find the “ny-rides.json” file?',\n",
       "  'sort_order': 99,\n",
       "  'content': 'The `ny-rides.json` is your private file in Google Cloud Platform (GCP). Here’s how to find it:\\n\\n- Navigate to GCP and select the project with your instance.\\n- Go to **IAM & Admin**.\\n- Select the **Service Accounts** tab.\\n- Click the **Keys** tab.\\n- Add a key, choosing **JSON** as the key type, then click **Create**.\\n\\n**Note:** Once in the Service Accounts, click the email associated to access the **KEYS** tab where you can add a key as a JSON key type.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/099_cdd91ef84a_gcp-where-can-i-find-the-ny-ridesjson-file.md'},\n",
       " {'id': 'a1f6167da1',\n",
       "  'question': 'GCP: \"Failed to load\" when accessing Compute Engine’s metadata section (e.g., to add a SSH key)',\n",
       "  'sort_order': 100,\n",
       "  'content': 'You likely didn’t enable the [Compute Engine API](https://console.cloud.google.com/marketplace/details/google/compute.googleapis.com).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/100_a1f6167da1_gcp-failed-to-load-when-accessing-compute-engines.md'},\n",
       " {'id': 'a697cb2dee',\n",
       "  'question': 'GCP: Do I need to delete my instance in Google Cloud?',\n",
       "  'sort_order': 101,\n",
       "  'content': '[In this lecture](https://www.youtube.com/watch?v=ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb), Alexey deleted his instance in Google Cloud. Do I have to do it?\\n\\nNo, do not delete your instance in Google Cloud Platform. Otherwise, you will have to set it up again for the week 1 readings.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/101_a697cb2dee_gcp-do-i-need-to-delete-my-instance-in-google-clou.md'},\n",
       " {'id': 'eb759226aa',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_31ceb9bc.png'}],\n",
       "  'question': 'GCP: SSH public key error - multiple users / usernames',\n",
       "  'sort_order': 102,\n",
       "  'content': \"Initially, I could not SSH into my VM from my Windows laptop. I thought it was because I did not follow the tutorial exactly. Instead of generating the SSH key using MINGW/git bash with the Linux-style command, I did it in Command Prompt using the Windows-style command. I kept getting a public key error.\\n\\n**Permanent Solution:**\\n\\nIt turns out it wasn’t an issue with the key generation at all! The problem was with the username. I had given my SSH key a different username than what appeared in my VM (my Google account username). So, I had been trying to log in with `googleacctuser@[ipaddr]` instead of `mySSHuser@[ipaddr]`. Here's how I resolved it:\\n\\n1. Retraced my steps to check the SSH key setup in the GCP console, where it showed the user and SSH key.\\n2. Changed the username to the correct one (googleacctuser) in my config file.\\n3. Updated the config file and used `mySSHuser` to log in.\\n\\nNow, the issue was that I had created two users. I made all the installations and permissions on `googleacctuser`, not accessible from `mySSHuser`. Since I didn't need `mySSHuser`, I edited the SSH key to change the username at the end and updated the GCP console and config file accordingly.\\n\\nThen, I planned to delete the `mySSHuser` account in the VM terminal to keep things clean (though I got a bit attached, so I skipped this).\\n\\n**Temporary Solution:**\\n\\nBefore figuring out my issue, I used a shortcut by SSH'ing into the VM in the browser, which worked nicely for a while. But eventually, I needed to use VSCode.\\n\\n<{IMAGE:image_1}>\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/102_eb759226aa_gcp-ssh-public-key-error-multiple-users-usernames.md'},\n",
       " {'id': '513410225e',\n",
       "  'question': 'GCP: Virtual Machine (VM) Size, Slow, Clean Up',\n",
       "  'sort_order': 103,\n",
       "  'content': 'If you are progressing through the course and find that your VM is starting to become slow, you can run the following commands to inspect and detect areas where you can improve:\\n\\n**Recommended VM Size**\\n\\n- Start with a 60GB machine. A 30GB machine may not be sufficient, as you might need to restart the project with a larger size.\\n\\n**Commands to Inspect the Health of Your VM**\\n\\n- **System Resource Usage**\\n  \\n  ```bash\\n  top\\n  htop\\n  ```\\n  Shows real-time information about system resource usage, including CPU, memory, and processes.\\n\\n  ```bash\\n  free -h\\n  ```\\n  Displays information about system memory usage and availability.\\n\\n  ```bash\\n  df -h\\n  ```\\n  Shows disk space usage of file systems.\\n\\n  ```bash\\n  du -h <directory>\\n  ```\\n  Displays disk usage of a specific directory.\\n\\n- **Running Processes**\\n  \\n  ```bash\\n  ps aux\\n  ```\\n  Lists all running processes along with detailed information.\\n\\n- **Network**\\n  \\n  ```bash\\n  ifconfig\\n  ip addr show\\n  ```\\n  Shows network interface configuration.\\n\\n  ```bash\\n  netstat -tuln\\n  ```\\n  Displays active network connections and listening ports.\\n\\n- **Hardware Information**\\n  \\n  ```bash\\n  lscpu\\n  ```\\n  Displays CPU information.\\n\\n  ```bash\\n  lsblk\\n  ```\\n  Lists block devices (disks and partitions).\\n\\n  ```bash\\n  lshw\\n  ```\\n  Lists hardware configuration.\\n\\n- **User and Permissions**\\n  \\n  ```bash\\n  who\\n  ```\\n  Shows who is logged on and their activities.\\n\\n  ```bash\\n  w\\n  ```\\n  Displays information about currently logged-in users and their processes.\\n\\n- **Package Management**\\n  \\n  ```bash\\n  apt list --installed\\n  ```\\n  Lists installed packages (for Ubuntu and Debian-based systems).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/103_513410225e_gcp-virtual-machine-vm-size-slow-clean-up.md'},\n",
       " {'id': 'e34580b954',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_366e8371.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_ee2aa0ad.png'}],\n",
       "  'question': 'Billing: Billing account has not been enabled for this project. But you’ve done it indeed!',\n",
       "  'sort_order': 104,\n",
       "  'content': 'If you’ve got the error:\\n\\n```plaintext\\nError: Error updating Dataset \"projects/<your-project-id>/datasets/demo_dataset\": googleapi: Error 403: Billing has not been enabled for this project. Enable billing at console.cloud.google.com. The default table expiration time must be less than 60 days, billingNotEnabled\\n```\\n\\nbut you’ve set your billing account, try disabling billing for the project and enabling it again. This method has been successful for others.\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/104_e34580b954_billing-billing-account-has-not-been-enabled-for-t.md'},\n",
       " {'id': '8f9f91b4de',\n",
       "  'question': 'GCP - Windows Google Cloud SDK install issue:',\n",
       "  'sort_order': 105,\n",
       "  'content': 'If you are encountering installation trouble with the Google Cloud SDK on Windows and receiving the following error:\\n\\n```\\nThese credentials will be used by any library that requests Application Default Credentials (ADC).\\n\\nWARNING:\\n\\nCannot find a quota project to add to ADC. You might receive a \"quota exceeded\" or \"API not enabled\" error. Run $ gcloud auth application-default set-quota-project to add a quota project.\\n```\\n\\nTry these steps:\\n\\n1. Reinstall the SDK using the unzip file \"install.bat\".\\n2. Check the installation by running `gcloud version`.\\n3. Run `gcloud init` to set up your project.\\n4. Execute `gcloud auth application-default login`.\\n\\nFor detailed instructions, refer to the following guide: [Windows SDK Installation Guide](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/week_1_basics_n_setup/1_terraform_gcp/windows.md)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/105_8f9f91b4de_gcp-windows-google-cloud-sdk-install-issue.md'},\n",
       " {'id': 'f29d45f419',\n",
       "  'question': 'GCP: I cannot get my Virtual Machine to start because GCP has no resources.',\n",
       "  'sort_order': 106,\n",
       "  'content': '1. Click on your VM.\\n2. Create an image of your VM.\\n3. On the page of the image, tell GCP to create a new VM instance via the image.\\n4. On the settings page, change the location.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/106_f29d45f419_gcp-i-cannot-get-my-virtual-machine-to-start-becau.md'},\n",
       " {'id': 'd8ebc91216',\n",
       "  'question': 'GCP VM: Is it necessary to use a GCP VM? When is it useful?',\n",
       "  'sort_order': 107,\n",
       "  'content': 'The reason this video about the GCP VM exists is that many students had problems configuring their environment. You can use your own environment if it works for you.\\n\\nAdvantages of using your own environment include:\\n\\n- **Commit Changes**: If you are working in a GitHub repository, you will be able to commit changes directly. In the VM, the repo is cloned via HTTPS, so it is not possible to commit directly, even if you are the owner of the repo.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/107_d8ebc91216_gcp-vm-is-it-necessary-to-use-a-gcp-vm-when-is-it.md'},\n",
       " {'id': '51bc7e280d',\n",
       "  'question': 'GCP VM - mkdir: cannot create directory ‘.ssh’: Permission denied',\n",
       "  'sort_order': 108,\n",
       "  'content': 'If you encounter an error while trying to create a directory:\\n\\n```bash\\nUser1@DESKTOP-PD6UM8A MINGW64 /\\n\\n$ mkdir .ssh\\n\\nmkdir: cannot create directory ‘.ssh’: Permission denied\\n```\\n\\nThis error occurs because you are attempting to create the directory in the root folder (`/`).\\n\\nTo resolve this, create the directory in your home directory instead. Use the following steps:\\n\\n1. Navigate to your home directory using:\\n   \\n   ```bash\\n   cd ~\\n   ```\\n\\n2. Create the `.ssh` directory:\\n   \\n   ```bash\\n   mkdir .ssh\\n   ```\\n\\nFor further guidance, watch [this video](https://www.youtube.com/watch?v=ae-CV2KfoN0&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/108_51bc7e280d_gcp-vm-mkdir-cannot-create-directory-ssh-permissio.md'},\n",
       " {'id': '1ba58ae0e0',\n",
       "  'question': 'GCP VM: Error while saving the file in VM via VS Code',\n",
       "  'sort_order': 109,\n",
       "  'content': \"```plaintext\\nFailed to save '<file>': Unable to write file 'vscode-remote://ssh-remote+de-zoomcamp/home/<user>/data_engineering_course/week_2/airflow/dags/<file>' (NoPermissions (FileSystemError): Error: EACCES: permission denied, open '/home/<user>/data_engineering_course/week_2/airflow/dags/<file>')\\n```\\n\\nTo resolve this issue, you need to change the owner of the files you are trying to edit via VS Code. Follow these steps:\\n\\n1. Connect to your VM using SSH.\\n\\n2. Run the following command to change the ownership:\\n\\n   ```bash\\n   sudo chown -R <user> <path to your directory>\\n   ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/109_1ba58ae0e0_gcp-vm-error-while-saving-the-file-in-vm-via-vs-co.md'},\n",
       " {'id': 'a63d2734b2',\n",
       "  'question': 'GCP VM: VM connection request timeout',\n",
       "  'sort_order': 110,\n",
       "  'content': '**Question:** I connected to my VM perfectly fine last week (SSH) but when I tried again this week, the connection request keeps timing out.\\n\\n**Answer:**\\n\\n1. **Start Your VM:** Make sure the VM is running in your GCP console.\\n\\n2. **Update External IP:**\\n   \\n   - Copy its External IP once the VM is running.\\n   - Update your SSH configuration file with this IP.\\n\\n3. **Edit SSH Config:**\\n   \\n   ```bash\\n   cd ~/.ssh\\n   code config\\n   ```\\n   \\n   This command opens the config file in VSCode for editing.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/110_a63d2734b2_gcp-vm-vm-connection-request-timeout.md'},\n",
       " {'id': '68f8a4b307',\n",
       "  'question': 'GCP VM: connect to host port 22 no route to host',\n",
       "  'sort_order': 111,\n",
       "  'content': 'Go to edit your VM.\\n\\n1. Navigate to the **Automation** section.\\n2. Add the following Startup script:\\n   \\n   ```bash\\n   #!/bin/bash\\n   sudo ufw allow ssh\\n   ```\\n3. Stop and Start the VM.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/111_68f8a4b307_gcp-vm-connect-to-host-port-22-no-route-to-host.md'},\n",
       " {'id': 'd6fa41adb6',\n",
       "  'question': 'GCP VM: Port forwarding from GCP without using VS Code',\n",
       "  'sort_order': 112,\n",
       "  'content': 'You can easily forward the ports of pgAdmin, PostgreSQL, and Jupyter Notebook using the built-in tools in Ubuntu without any additional client:\\n\\n1. **On the VM machine:**\\n   - Launch Docker and Jupyter Notebook in the correct folder using:\\n     ```bash\\n     docker-compose up -d\\n     jupyter notebook\\n     ```\\n\\n2. **From the local machine:**\\n   - Execute:\\n     ```bash\\n     ssh -i ~/.ssh/gcp -L 5432:localhost:5432 username@external_ip_of_vm\\n     ```\\n   - Execute the same command for ports 8080 and 8888.\\n\\n3. **Accessing Applications Locally:**\\n   - For pgAdmin, open a browser and go to `localhost:8080`.\\n   - For Jupyter Notebook, open a browser and go to `localhost:8888`.\\n     - If you encounter issues with credentials, you may need to copy the link with the access token from the terminal logs on the VM when you launched the Jupyter Notebook.\\n\\n4. **Forwarding Both pgAdmin and PostgreSQL:**\\n   - Use:\\n     ```bash\\n     ssh -i ~/.ssh/gcp -L 5432:localhost:5432 -L 8080:localhost:8080 modito@35.197.218.128\\n     ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/112_d6fa41adb6_gcp-vm-port-forwarding-from-gcp-without-using-vs-c.md'},\n",
       " {'id': 'af2b85f346',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_df9492cb.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_6b01ae01.png'},\n",
       "   {'description': 'image #3',\n",
       "    'id': 'image_3',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_bc858c4b.png'},\n",
       "   {'description': 'image #4',\n",
       "    'id': 'image_4',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_a231d54c.png'},\n",
       "   {'description': 'image #5',\n",
       "    'id': 'image_5',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_2f5bf08c.png'}],\n",
       "  'question': 'GCP gcloud + MS VS Code - gcloud auth hangs',\n",
       "  'sort_order': 113,\n",
       "  'content': 'If you are using MS VS Code and running `gcloud` in WSL2, when you first try to login to GCP via the `gcloud` CLI with `gcloud auth application-default login`, you may encounter an issue where a message appears and nothing happens:\\n\\n<{IMAGE:image_1}>\\n\\nThere might be a prompt asking if you want to open it via a browser. If you click on it, it will open a page with an error message:\\n\\n<{IMAGE:image_2}>\\n\\n**Solution:**\\n\\n- Hover over the long link.\\n- `Ctrl + Click` the long link.\\n- Click \"Configure Trusted Domains here.\"\\n- A popup will appear; pick the first or second entry.\\n\\n<{IMAGE:image_3}>\\n\\n<{IMAGE:image_4}>\\n\\n<{IMAGE:image_5}>\\n\\nNext time you run `gcloud auth`, the login page should pop up via the default browser without issues.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/113_af2b85f346_gcp-gcloud-ms-vs-code-gcloud-auth-hangs.md'},\n",
       " {'id': 'dec5edee6a',\n",
       "  'question': 'Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later',\n",
       "  'sort_order': 114,\n",
       "  'content': 'This error typically occurs due to internet connectivity issues. Terraform is unable to access the online registry.\\n\\n**Solution:**\\n\\n- Check your VPN/Firewall settings.\\n- Clear cookies or restart your network.\\n- Run `terraform init` again after addressing the connection issues.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/114_dec5edee6a_terraform-error-failed-to-query-available-provider.md'},\n",
       " {'id': '31053272e0',\n",
       "  'question': 'Terraform: Error: Post \"[storage.googleapis.com](https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901): oauth2: cannot fetch token: Post \"[oauth2.googleapis.com](https://oauth2.googleapis.com/token): dial tcp 172.217.163.42:443: i/o timeout',\n",
       "  'sort_order': 115,\n",
       "  'content': 'The issue was related to network restrictions, as Google is not accessible in my country. I used a VPN and discovered that the terminal program does not automatically follow the system proxy, requiring separate proxy configuration settings.\\n\\n**Solution:**\\n\\n1. Open an Enhanced Mode in your VPN application, such as Clash.\\n2. Run `terraform apply` again.\\n\\nIf you encounter this issue, consult your VPN provider for assistance with configuration.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/115_31053272e0_terraform-error-post-storagegoogleapiscomhttpsstor.md'},\n",
       " {'id': 'a5a163f51d',\n",
       "  'question': 'Terraform: Install for WSL',\n",
       "  'sort_order': 116,\n",
       "  'content': 'You can configure Terraform on Windows 10 using the Linux Subsystem (WSL) by following this guide: [Configuring Terraform on Windows 10 Linux Subsystem](https://techcommunity.microsoft.com/t5/azure-developer-community-blog/configuring-terraform-on-windows-10-linux-sub-system/ba-p/393845).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/116_a5a163f51d_terraform-install-for-wsl.md'},\n",
       " {'id': 'feb57d99da',\n",
       "  'question': 'Terraform: Error acquiring the state lock',\n",
       "  'sort_order': 117,\n",
       "  'content': 'For more information, you can refer to the following issue on GitHub:\\n\\n[HashiCorp Terraform Issue #14513](https://github.com/hashicorp/terraform/issues/14513)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/117_feb57d99da_terraform-error-acquiring-the-state-lock.md'},\n",
       " {'id': 'dd3e6999fd',\n",
       "  'question': 'Terraform: Error 400 Bad Request. Invalid JWT Token on WSL.',\n",
       "  'sort_order': 118,\n",
       "  'content': 'When running:\\n\\n```bash\\nterraform apply\\n```\\n\\non WSL2, you might encounter the following error:\\n\\n```\\nError: Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=<your-project-id>\": oauth2: cannot fetch token: 400 Bad Request\\n\\nResponse: {\"error\":\"invalid_grant\",\"error_description\":\"Invalid JWT: Token must be a short-lived token (60 minutes) and in a reasonable timeframe. Check your iat and exp values in the JWT claim.\"}\\n```\\n\\nThis issue occurs due to potential time desynchronization on your machine, affecting JWT computation.\\n\\nTo fix this, run the following command to synchronize your system time:\\n\\n```bash\\nsudo hwclock -s\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/118_dd3e6999fd_terraform-error-400-bad-request-invalid-jwt-token.md'},\n",
       " {'id': '55c0047f8b',\n",
       "  'question': 'Terraform - Error 403 : Access denied',\n",
       "  'sort_order': 119,\n",
       "  'content': '```\\n│ Error: googleapi: Error 403: Access denied., forbidden\\n```\\n\\nYour `$GOOGLE_APPLICATION_CREDENTIALS` might not be pointing to the correct file. Try the following steps:\\n\\n1. Set the correct path for your credentials:\\n   \\n   ```bash\\n   export GOOGLE_APPLICATION_CREDENTIALS=~/.gc/YOUR_JSON.json\\n   ```\\n\\n2. Activate the service account:\\n   \\n   ```bash\\n   gcloud auth activate-service-account --key-file $GOOGLE_APPLICATION_CREDENTIALS\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/119_55c0047f8b_terraform-error-403-access-denied.md'},\n",
       " {'id': 'de1924abdb',\n",
       "  'question': 'Terraform: Do I need to make another service account for Terraform before I get the keys (.json file)?',\n",
       "  'sort_order': 120,\n",
       "  'content': \"One service account is enough for all the services/resources you'll use in this course. After you get the file with your credentials and set your environment variable, you should be good to go.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/120_de1924abdb_terraform-do-i-need-to-make-another-service-accoun.md'},\n",
       " {'id': 'd0f76f4669',\n",
       "  'question': 'Terraform: Where can I find the Terraform 1.1.3 Linux (AMD 64)?',\n",
       "  'sort_order': 121,\n",
       "  'content': 'Here: [https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip](https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/121_d0f76f4669_terraform-where-can-i-find-the-terraform-113-linux.md'},\n",
       " {'id': '76f95c1d28',\n",
       "  'question': 'Terraform: Terraform initialized in an empty directory! The directory has no Terraform configuration files. You may begin working with Terraform immediately by creating Terraform configuration files.',\n",
       "  'sort_order': 122,\n",
       "  'content': 'This error occurs when `terraform init` is run outside the working directory.\\n\\nTo resolve this issue:\\n\\n1. Navigate to the working directory that contains your Terraform configuration files.\\n2. Run the `terraform init` command inside the correct directory.\\n\\nMake sure your configuration files (e.g., .tf files) are present in the directory before running the command.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/122_76f95c1d28_terraform-terraform-initialized-in-an-empty-direct.md'},\n",
       " {'id': '4a6d04a2c3',\n",
       "  'question': 'Terraform - Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes',\n",
       "  'sort_order': 123,\n",
       "  'content': 'The error:\\n\\n```\\nError: googleapi: Error 403: Access denied., forbidden\\n\\nError: Error creating Dataset: googleapi: Error 403: Request had insufficient authentication scopes.\\n```\\n\\nSolution:\\n\\n1. Verify your credentials by running:\\n   \\n   ```bash\\n   echo $GOOGLE_APPLICATION_CREDENTIALS\\n   echo $?\\n   ```\\n\\n2. Ensure you have set the `GOOGLE_APPLICATION_CREDENTIALS` environment variable correctly, as demonstrated in the environment setup video in week 1:\\n\\n   ```bash\\n   export GOOGLE_APPLICATION_CREDENTIALS=\"<path/to/your/service-account-authkeys>.json\"\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/123_4a6d04a2c3_terraform-error-creating-dataset-googleapi-error-4.md'},\n",
       " {'id': '5849afe5f6',\n",
       "  'question': 'stoTerraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’',\n",
       "  'sort_order': 124,\n",
       "  'content': \"The error:\\n\\n```\\nError: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\\n```\\n\\nThe solution:\\n\\nYou have to declare the project name as your Project ID, not your Project name, available on the GCP console Dashboard.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/124_5849afe5f6_stoterraform-error-creating-bucket-googleapi-error.md'},\n",
       " {'id': 'fa8cbc8f40',\n",
       "  'question': 'Terraform: google provider requires credentials.',\n",
       "  'sort_order': 125,\n",
       "  'content': 'To ensure the sensitivity of the credentials file, use the following configuration:\\n\\n```hcl\\nprovider \"google\" {\\n  project     = var.projectId\\n  credentials = file(\"${var.gcpkey}\")\\n  #region      = var.region\\n  zone = var.zone\\n}\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/125_fa8cbc8f40_terraform-google-provider-requires-credentials.md'},\n",
       " {'id': '9f9a1b9e4f',\n",
       "  'question': 'Terraform: Teardown of BigQuery Dataset',\n",
       "  'sort_order': 126,\n",
       "  'content': \"When running `terraform destroy`, the following error can occur:\\n\\n```\\nDo you really want to destroy all resources?\\n\\nTerraform will destroy all your managed infrastructure, as shown above.\\n\\nThere is no undo. Only 'yes' will be accepted to confirm.\\n\\nEnter a value: yes\\n\\ngoogle_bigquery_dataset.homework_dataset: Destroying... [id=projects/terraform-demo-449214/datasets/homework_dataset]\\n\\n╷\\n\\n│ Error: Error when reading or editing Dataset: googleapi: Error 400: Dataset terraform-demo-449214:homework_dataset is still in use, resourceInUse\\n```\\n\\nThis is because the dataset is still in use by a table. To delete the dataset, set the `delete_contents_on_destroy` property to `true` in the `main.tf` file.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/126_9f9a1b9e4f_terraform-teardown-of-bigquery-dataset.md'},\n",
       " {'id': 'bac852d170',\n",
       "  'question': \"SQL: SELECT * FROM zones_taxi WHERE Zone='Astoria Zone'; Error Column Zone doesn't exist\",\n",
       "  'sort_order': 127,\n",
       "  'content': 'For this issue, you can use the following solution:\\n\\n```sql\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria Zone\\';\\n```\\n\\nColumns that start with uppercase sometimes need to be enclosed in double quotes.\\n\\nAdditionally, check your dataset for the existence of `\\'Astoria Zone\\'`. You might find only `\\'Astoria\\'`:\\n\\n```sql\\nSELECT * FROM zones AS z WHERE z.\"Zone\" = \\'Astoria\\';\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/127_bac852d170_sql-select-from-zones_taxi-where-zoneastoria-zone.md'},\n",
       " {'id': '358cb2fd4d',\n",
       "  'question': \"SQL: SELECT Zone FROM taxi_zones Error Column Zone doesn't exist\",\n",
       "  'sort_order': 128,\n",
       "  'content': \"It is inconvenient to use quotation marks all the time, so it is better to put the data in the database all in lowercase. In Pandas, after:\\n\\n```python\\nimport pandas as pd\\n\\ndf = pd.read_csv('taxi+_zone_lookup.csv')\\n```\\n\\nAdd the row:\\n\\n```python\\ndf.columns = df.columns.str.lower()\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/128_358cb2fd4d_sql-select-zone-from-taxi_zones-error-column-zone.md'},\n",
       " {'id': '48cae101d6',\n",
       "  'question': 'CURL: curl: (6) Could not resolve host: output.csv',\n",
       "  'sort_order': 129,\n",
       "  'content': '```python\\nos.system(f\"curl {url} --output {csv_name}\")\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/129_48cae101d6_curl-curl-6-could-not-resolve-host-outputcsv.md'},\n",
       " {'id': '10407876a1',\n",
       "  'question': 'SSH Error: ssh: Could not resolve hostname linux: Name or service not known',\n",
       "  'sort_order': 130,\n",
       "  'content': 'To resolve this, ensure that your config file is in `C/User/Username/.ssh/config`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/130_10407876a1_ssh-error-ssh-could-not-resolve-hostname-linux-nam.md'},\n",
       " {'id': 'd02af042b1',\n",
       "  'question': \"'pip' is not recognized as an internal or external command, operable program or batch file.\",\n",
       "  'sort_order': 131,\n",
       "  'content': 'If you use Anaconda (recommended for the course), it comes with `pip`, so the issue is probably that Anaconda’s Python is not on the PATH.\\n\\n\\n**For Linux and MacOS:**\\n\\n1. Open a terminal.\\n2. Find the path to your Anaconda installation. This is typically `~/anaconda3` or `~/opt/anaconda3`.\\n3. Add Anaconda to your PATH with the command:\\n   \\n   ```bash\\n   export PATH=\"/path/to/anaconda3/bin:$PATH\"\\n   ```\\n\\n4. To make this change permanent, add the command to your `.bashrc` (Linux) or `.bash_profile` (MacOS) file.\\n\\n**On Windows, using Git Bash:**\\n\\n1. Locate your Anaconda installation. The default path is usually `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3`.\\n2. Convert the Windows path to a Unix-style path for Git Bash, e.g., `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` becomes `/c/Users/[YourUsername]/Anaconda3`.\\n3. Add Anaconda to your PATH with the command:\\n\\n   ```bash\\n   export PATH=\"/c/Users/[YourUsername]/Anaconda3/:/c/Users/[YourUsername]/Anaconda3/Scripts/$PATH\"\\n   ```\\n\\n4. To make this change permanent, add the command to your `.bashrc` file in your home directory.\\n5. Refresh your environment with the command:\\n\\n   ```bash\\n   source ~/.bashrc\\n   ```\\n\\n**For Windows (without Git Bash):**\\n\\n1. Right-click on \\'This PC\\' or \\'My Computer\\' and select \\'Properties\\'.\\n2. Click on \\'Advanced system settings\\'.\\n3. In the System Properties window, click on \\'Environment Variables\\'.\\n4. In the Environment Variables window, select the \\'Path\\' variable in the \\'System variables\\' section and click \\'Edit\\'.\\n5. In the Edit Environment Variable window, click \\'New\\' and add the path to your Anaconda installation (typically `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3` and `C:\\\\Users\\\\[YourUsername]\\\\Anaconda3\\\\Scripts`).\\n6. Click \\'OK\\' in all windows to apply the changes.\\n\\nAfter adding Anaconda to the PATH, you should be able to use `pip` from the command line. Remember to restart your terminal (or command prompt in Windows) to apply these changes.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/131_d02af042b1_pip-is-not-recognized-as-an-internal-or-external-c.md'},\n",
       " {'id': '8b082e74c0',\n",
       "  'question': 'Error: error starting userland proxy: listen tcp4 0.0.0.0:8080: bind: address already in use',\n",
       "  'sort_order': 132,\n",
       "  'content': 'Resolution: You need to stop the service using the port.\\n\\nRun the following:\\n\\n```bash\\nsudo kill -9 `sudo lsof -t -i:<port>`\\n```\\n\\nReplace `<port>` with `8080` in this case. This will free up the port for use.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/132_8b082e74c0_error-error-starting-userland-proxy-listen-tcp4-00.md'},\n",
       " {'id': 'a9081d1a79',\n",
       "  'question': 'Anaconda to PIP',\n",
       "  'sort_order': 133,\n",
       "  'content': 'To get a pip-friendly `requirements.txt` file from Anaconda, use the following steps:\\n\\n1. Install pip in your Anaconda environment:\\n   ```bash\\n   conda install pip\\n   ```\\n2. Generate the `requirements.txt` file:\\n   ```bash\\n   pip list --format=freeze > requirements.txt\\n   ```\\n\\nNote:\\n- `conda list -d > requirements.txt` will not work.\\n- `pip freeze > requirements.txt` may give odd pathing.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/133_a9081d1a79_anaconda-to-pip.md'},\n",
       " {'id': '35d7874a8d',\n",
       "  'question': 'Jupyter: Install, open Jupyter and convert Jupyter notebook to Python script',\n",
       "  'sort_order': 134,\n",
       "  'content': '### Install and Open Jupyter Notebook\\n\\nTo install Jupyter Notebook, run:\\n\\n```bash\\npip install jupyter\\n```\\n\\nTo open Jupyter Notebook, use:\\n\\n```bash\\npython3 -m notebook\\n```\\n\\n### Convert Jupyter Notebook to Python Script\\n\\nFirst, ensure `nbconvert` is installed and upgraded:\\n\\n```bash\\npip install nbconvert --upgrade\\n```\\n\\nThen, convert a Jupyter Notebook to a Python script with the following command:\\n\\n```bash\\npython3 -m jupyter nbconvert --to=script upload-data.ipynb\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/134_35d7874a8d_jupyter-install-open-jupyter-and-convert-jupyter-n.md'},\n",
       " {'id': 'c2f39b0ef3',\n",
       "  'question': 'Alternative way to convert Jupyter notebook to Python script (via jupytext)',\n",
       "  'sort_order': 135,\n",
       "  'content': 'If you keep getting errors with nbconvert after executing:\\n\\n```bash\\njupyter nbconvert --to script <your_notebook.ipynb>\\n```\\n\\nYou could try converting your Jupyter notebook using another tool called Jupytext. Jupytext is an excellent tool for converting Jupyter Notebooks to Python scripts, similar to nbconvert.\\n\\n1. **Install Jupytext**\\n   \\n   ```bash\\n   pip install jupytext\\n   ```\\n\\n2. **Convert your Notebook to a Python script**\\n\\n   ```bash\\n   jupytext --to py <your_notebook.ipynb>\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-1/135_c2f39b0ef3_alternative-way-to-convert-jupyter-notebook-to-pyt.md'},\n",
       " {'id': '7889d2bad5',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_0f494026.png'}],\n",
       "  'question': 'SSH error in VS Code - “Could not establish connection to \"de-zoomcamp\": Permission denied (publickey).”',\n",
       "  'sort_order': 1,\n",
       "  'content': 'If you are using Windows, try the following steps to resolve the error:\\n\\n1. Copy the `.ssh` folder from the Linux file path to Windows.\\n2. In the `config` file, use:\\n   \\n   ```\\n   IdentityFile C:\\\\Users\\\\<username>\\\\.ssh\\\\gcp\\n   ```\\n   \\n   Instead of:\\n   \\n   ```\\n   IdentityFile ~/.ssh/gcp\\n   ```\\n\\n3. Ensure the private key file located at `C:\\\\Users\\\\<username>\\\\.ssh\\\\gcp` has an extra line at the end:\\n\\n   <{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/001_7889d2bad5_ssh-error-in-vs-code-could-not-establish-connectio.md'},\n",
       " {'id': 'f1c31510e6',\n",
       "  'question': 'Where are the FAQ questions from the previous cohorts for the orchestration module?',\n",
       "  'sort_order': 2,\n",
       "  'content': '- [Prefect FAQ Document](https://docs.google.com/document/d/1K_LJ9RhAORQk3z4Qf_tfGQCDbu8wUWzru62IUscgiGU/edit?usp=sharing)\\n- [Airflow FAQ Document](https://docs.google.com/document/d/1-BwPAsyDH_mAsn8HH5z_eNYVyBMAtawJRjHHsjEKHyY/edit?usp=sharing)\\n- [Mage FAQ Document](https://docs.google.com/document/d/1CkHVelbYYTMbwuj2eurNIwWVqXWzH-9-AqKETD9IC3I/edit?tab=t.0)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/002_f1c31510e6_where-are-the-faq-questions-from-the-previous-coho.md'},\n",
       " {'id': 'bcc6781b6e',\n",
       "  'question': 'How do I launch Kestra?',\n",
       "  'sort_order': 3,\n",
       "  'content': 'To launch Kestra, follow these instructions:\\n\\n### For Linux\\n\\nStart Docker with the following command:\\n\\n```bash\\ndocker run \\\\\\n  --pull=always \\\\\\n  --rm \\\\\\n  -it \\\\\\n  -p 8080:8080 \\\\\\n  --user=root \\\\\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\\\\n  -v /tmp:/tmp \\\\\\n  kestra/kestra:latest server local\\n```\\n\\nOnce it is running, you can log in to the dashboard at `localhost:8080`.\\n\\n### For Windows\\n\\nRefer to the Kestra GitHub repository for detailed instructions: [https://github.com/kestra-io/kestra](https://github.com/kestra-io/kestra)\\n\\n\\nSample `docker-compose` for Kestra:\\n\\n```yaml\\nkestra:\\n  build: .\\n  image: kestra/kestra:latest\\n  container_name: kestra\\n  user: \"0:0\"\\n  environment:\\n    DOCKER_HOST: tcp://host.docker.internal:2375  # for Windows\\n    KESTRA_CONFIGURATION: |\\n      kestra:\\n        repository:\\n          type: h2\\n        queue:\\n          type: memory\\n        storage:\\n          type: local\\n          local:\\n            basePath: /app/storage\\n        tasks:\\n          tmp-dir:\\n            path: /app/tmp\\n        plugins:\\n          repositories:\\n            - id: central\\n              type: maven\\n              url: [repo.maven.apache.org](https://repo.maven.apache.org/maven2)\\n          definitions:\\n            - io.kestra.plugin.core:core:latest\\n            - io.kestra.plugin.scripts:python:1.3.4\\n            - io.kestra.plugin.http:http:latest\\n    KESTRA_TASKS_TMP_DIR_PATH: /app/tmp\\n  ports:\\n    - \"8080:8080\"\\n  volumes:\\n    - //var/run/docker.sock:/var/run/docker.sock  # Windows path\\n    - /yourpath/.dbt:/app/.dbt\\n    - /yourpath/kestra/plugins:/app/plugins\\n    - /yourpath/kestra/workflows:/app/workflows\\n    - /yourpath/kestra/storage:/app/storage\\n    - /yourpath//kestra/tmp:/app/tmp\\n    - /yourpath//dbt_prj:/app/workflows/dbt_project\\n    - /yourpath//my-creds.json:/app/.dbt/my-creds.json\\n  command: server standalone\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/003_bcc6781b6e_how-do-i-launch-kestra.md'},\n",
       " {'id': '16481ac8f7',\n",
       "  'question': 'docker: Error response from daemon: mkdir C:\\\\Program Files\\\\Git\\\\var: Access is denied.',\n",
       "  'sort_order': 4,\n",
       "  'content': '### Description:\\n\\nWhen running the following Docker command in Bash with Docker and WSL2 installed, you may encounter an error. Running Bash as admin will not resolve the issue:\\n\\n```bash\\ndocker run \\\\\\n  --pull=always \\\\\\n  --rm \\\\\\n  -it \\\\\\n  -p 8080:8080 \\\\\\n  --user=root \\\\\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\\\\n  -v /tmp:/tmp \\\\\\n  kestra/kestra:latest server local\\n```\\n\\n```\\nlatest: Pulling from kestra/kestra\\nDigest: sha256:af02a309ccbb52c23ad1f1551a1a6db8cf0523cf7aac7c7eb878d7925bc85a62\\nStatus: Image is up to date for kestra/kestra:latest\\ndocker: Error response from daemon: mkdir C:\\\\\\\\Program Files\\\\\\\\Git\\\\\\\\var: Access is denied.\\nSee \\'docker run --help\\'.\\n```\\n\\n\\nTo resolve this issue, run Command Prompt as an administrator and use the following command:\\n\\n```bash\\ndocker run \\\\\\n  --pull=always \\\\\\n  --rm \\\\\\n  -it \\\\\\n  -p 8080:8080 \\\\\\n  --user=root \\\\\\n  -v \"/var/run/docker.sock:/var/run/docker.sock\" \\\\\\n  -v \"C:/Temp:/tmp\" \\\\\\n  kestra/kestra:latest server local\\n```\\n\\nAfter executing the command as described, the localhost should display the Kestra UI as expected.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/004_16481ac8f7_docker-error-response-from-daemon-mkdir-cprogram-f.md'},\n",
       " {'id': 'e4bce3ff6b',\n",
       "  'question': 'Error when running Kestra flow connecting to postgres',\n",
       "  'sort_order': 5,\n",
       "  'content': '### Error Message\\n```plaintext\\norg.postgresql.util.psqlexception the connection attempt failed due to this config on kestra flow -> jdbc:postgresql://host.docker.internal:5432/postgres-zoomcamp\\n```\\n\\n### Solution\\n- Replace `host.docker.internal` with the name of the service for Postgres in your Docker Compose file.\\n\\n---\\n\\n### Additional Error Message\\n```plaintext\\norg.postgresql.util.PSQLException: The connection attempt failed. 2025-01-29 22:52:22.281 green_create_table The connection attempt failed. host.docker.internal\\n```\\n\\n### Analysis and Solution\\n- If using Linux, the PostgreSQL database URL differs from the tutorial. Instead of `host.docker.internal`, Linux users should use the service or container name for Postgres. For example, use:\\n  \\n  ```plaintext\\n  jdbc:postgresql://postgres:5432/kestra\\n  ```\\n- Double-check the database name in your Docker Compose file. It might be different from the tutorial; for example, `kestra` instead of `postgres-zoomcamp`.\\n\\n### Reminder\\n- Ensure that the PostgreSQL database name in the Docker Compose matches what you configure in your flow.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/005_e4bce3ff6b_error-when-running-kestra-flow-connecting-to-postg.md'},\n",
       " {'id': 'db53f69d74',\n",
       "  'question': 'Adding a pgadmin service with volume mounting to the docker-compose:',\n",
       "  'sort_order': 6,\n",
       "  'content': 'I encountered an error where the localhost URL for pgAdmin would just hang (I chose `localhost:8080` for my pgAdmin, and made kestra `localhost:8090`, personal preference).\\n\\nThe associated issue involved permissions. The resolution was to change the ownership of my local directory to the user \"5050,\" which is pgAdmin. Unlike Postgres, pgAdmin requires explicit permission. Apparently, the Postgres user inside the Docker container creates the Postgres volume/dir, so it has permissions already.\\n\\nThis is a useful resource:\\n\\n[Stack Overflow: Permission denied /var/lib/pgadmin/sessions in Docker](https://stackoverflow.com/questions/64781245/permission-denied-var-lib-pgadmin-sessions-in-docker)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/006_db53f69d74_adding-a-pgadmin-service-with-volume-mounting-to-t.md'},\n",
       " {'id': '00e8093b90',\n",
       "  'question': 'Running out of storage when using kestra with postgres on GCP VM',\n",
       "  'sort_order': 7,\n",
       "  'content': 'Running out of storage while trying to backfill. I realized my GCP VM only has 30GB of storage and I was using it up quickly. Here are a couple of suggestions for managing storage:\\n\\n- **Clean up your GCP VM drive:** Use the command below to identify what is taking up the most space:\\n\\n  ```bash\\n  sudo du -sh *\\n  ```\\n\\n  - **(~1GB)** For me, the Anaconda installer was consuming a lot of space. If you no longer need it, you can delete it:\\n  \\n    ```bash\\n    rm -rf <anacondainstaller_fpath>\\n    ```\\n\\n  - **(~3GB)** Anaconda itself takes up a lot of space. You can’t delete it entirely if you need Python, but you can clean it up significantly:\\n    \\n    ```bash\\n    conda clean --all -y\\n    ```\\n\\n- **Clean up your Kestra files:** Use a purge flow. You can find a generic example here:\\n  \\n  [https://kestra.io/docs/administrator-guide/purge](https://kestra.io/docs/administrator-guide/purge)\\n  \\n  I wanted to perform the cleanup immediately, rather than waiting until the end of the month, so I adjusted the `endDate` to `\"{{ now() }}\"` and removed the trigger block. You can also choose whether to remove FAILED state executions.\\n\\n- **Clean up your PostgreSQL database:** You can manually delete tables in pgAdmin, or set up a workflow in Kestra for it. I found it easy to do manually.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/007_00e8093b90_running-out-of-storage-when-using-kestra-with-post.md'},\n",
       " {'id': '06775a8677',\n",
       "  'question': 'How can Kestra access service account credential?',\n",
       "  'sort_order': 8,\n",
       "  'content': \"Do not directly add the content of service account credential JSON in Kestra script, especially if you are pushing to GitHub. Follow the instruction to add the service account as a secret [Configure Google Service Account](https://kestra.io/docs/how-to-guides/google-credentials#add-service-account-as-a-secret).\\n\\nWhen you need to use it in Kestra, you can pull it through `{{ secret('GCP_SERVICE_ACCOUNT') }}` in the `pluginDefaults`.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/008_06775a8677_how-can-kestra-access-service-account-credential.md'},\n",
       " {'id': 'dda62d0ef0',\n",
       "  'question': 'Storage: Bucket Permission Denied Error when running the gcp_setup flow',\n",
       "  'sort_order': 9,\n",
       "  'content': 'When following the [YouTube lesson](https://www.youtube.com/watch?v=nKqjjLJ7YXs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23) and then running the [gcp_setup flow](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/flows/05_gcp_setup.yaml), you might encounter a permission denied error.\\n\\nTo resolve this:\\n\\n1. Verify if the bucket already exists using the GCP console.\\n2. If it exists, choose a different name for the bucket.\\n\\n**Note:** The GCP bucket name must be unique globally across all buckets, as the bucket will be accessible by URL.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/009_dda62d0ef0_storage-bucket-permission-denied-error-when-runnin.md'},\n",
       " {'id': 'c18119ba32',\n",
       "  'question': 'Invalid dataset ID Error when running the gcp_setup flow',\n",
       "  'sort_order': 10,\n",
       "  'content': 'When following the [YouTube lesson](https://www.youtube.com/watch?v=nKqjjLJ7YXs&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=23) and then running the [gcp_setup flow](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/02-workflow-orchestration/flows/05_gcp_setup.yaml), the error occurs during the `create_bq_dataset` task.\\n\\nThe error is less clear, but it stems from using a dash in the dataset name. To resolve this, change the dataset name to something like \"de_zoomcamp\" to avoid using a dash. This should resolve the error.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/010_c18119ba32_invalid-dataset-id-error-when-running-the-gcp_setu.md'},\n",
       " {'id': 'aeaede4fd1',\n",
       "  'question': 'How do I properly authenticate a Google Cloud Service Account in Kestra?',\n",
       "  'sort_order': 11,\n",
       "  'content': 'Several authentication methods are available; here are some of the most straightforward approaches:\\n\\n### Method 1:\\n\\nUpdate your `docker-compose.yml` file as needed.\\n\\n### Method 2:\\n\\n1. **Store the Service Account as a Secret**  \\n   Run this command, specifying the correct path to your `service-account.json` file and `.env_encoded`:\\n\\n   ```bash\\n   # Example command: Adjust according to your environment\\n   base64 /path/to/service-account.json > .env_encoded\\n   ```\\n\\n2. **Modify `docker-compose.yml` to Include the Encoded Secrets**  \\n   Insert the relevant configuration within your `docker-compose.yml`.\\n\\n3. **Configure Kestra Plugin Defaults**  \\n   This ensures all GCP tasks use the secret automatically.\\n\\n4. **Verify it’s Working in a Testing GCP Workflow**\\n\\n### Additional FAQs:\\n\\n**Question:** How do I update the Service Account key?\\n\\n**Answer:** Generate a new key, re-run the Base64 command, and restart Kestra.\\n\\n**Question:** Why use secrets instead of embedding the JSON key in the task?\\n\\n**Answer:** Secrets prevent credential exposure and make workflows easier to manage.\\n\\n**Question:** Can I apply this method to other GCP tasks?\\n\\n**Answer:** Yes, all GCP plugins will automatically inherit the secret.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/011_aeaede4fd1_how-do-i-properly-authenticate-a-google-cloud-serv.md'},\n",
       " {'id': 'a489fb34ac',\n",
       "  'question': 'Should I include my .env_encoded file in my .gitignore?',\n",
       "  'sort_order': 12,\n",
       "  'content': \"Yes, you should definitely include the `.env_encoded` file in your `.gitignore` file. Here's why:\\n\\n- **Security:** The `.env_encoded` file contains sensitive information, namely the base64 encoded version of your GCP Service Account key. Even though it's encoded, it's not secure to share this in a public repository as anyone can decode it back to the original JSON.\\n\\n- **Best Practices:** It's common practice to avoid committing environment files or any files containing secrets to version control systems like Git. This prevents accidental exposure of sensitive data.\\n\\n### How to do it:\\n\\n- Add this line to your `.gitignore`:\\n\\n  ```\\n  .env_encoded\\n  ```\\n\\n### More on Security\\n\\nBase64 encoding is easily reversible. Base64 is an encoding scheme, not an encryption method. It's designed to encode binary data into ASCII characters that can be safely transmitted over systems that are designed to deal with text. Here's why it's not secure for protecting sensitive information:\\n\\n- **Reversibility:** Base64 encoding simply translates binary data into a text string using a specific set of 64 characters. Decoding it back to the original data is straightforward and doesn't require any secret key or password.\\n\\n- **Public Availability of Tools:** Numerous online tools, software libraries, and command-line utilities exist that can decode base64 with just a few clicks or commands.\\n\\n- **No Security:** Since base64 encoding does not change or hide the actual content of the data, anyone with access to the encoded string can decode it back to the original data.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/012_a489fb34ac_should-i-include-my-env_encoded-file-in-my-gitigno.md'},\n",
       " {'id': '5db9bca6a9',\n",
       "  'question': 'Getting SIGILL in JRE when running latest kestra image on Mac M4 MacOS 15.2/3',\n",
       "  'sort_order': 13,\n",
       "  'content': 'SIGILL in Java Runtime Environment on MacOS M4\\n\\nAdd the following environment variable to your Kestra container: `-e JAVA_OPTS=\"-XX:UseSVE=0\"`:\\n\\n```bash\\ndocker run --rm -it \\\\\\n  --pull=always \\\\\\n  -p 8080:8080 \\\\\\n  --user=root \\\\\\n  -e JAVA_OPTS=\"-XX:UseSVE=0\" \\\\\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\\\\n  -v /tmp:/tmp \\\\\\n  kestra/kestra:latest server local\\n```\\nThe same in a Docker Compose file:\\n\\n```yaml\\nservices:\\n  kestra:\\n    image: kestra/kestra:latest\\n    environment:\\n      JAVA_OPTS: \"-XX:UseSVE=0\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/013_5db9bca6a9_getting-sigill-in-jre-when-running-latest-kestra-i.md'},\n",
       " {'id': '5d66421473',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_e66c0b8d.png'}],\n",
       "  'question': 'taskid: yellow_create_table The connection attempt failed. Host.docker.internal',\n",
       "  'sort_order': 14,\n",
       "  'content': 'If you\\'re using Linux, you might encounter \"Connection Refused\" errors when connecting to the Postgres DB from within Kestra. This is because `host.docker.internal` works differently on Linux.\\n\\nTo address this issue:\\n\\n- Use the modified Docker Compose file mentioned in the \"02-workflow-orchestration\" README troubleshooting tips.\\n- Run both Kestra and its dedicated Postgres DB, along with the Postgres DB for exercises, all together using Docker Compose.\\n- Access the Postgres DB within Kestra by using the container name `postgres_zoomcamp` instead of `host.docker.internal` in `pluginDefaults`.\\n\\nMake sure to modify the `pluginDefaults` in the following files:\\n\\n- `2_postgres_taxi_scheduled.yaml`\\n- `02_postgres_taxi.yaml`\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/014_5d66421473_taskid-yellow_create_table-the-connection-attempt.md'},\n",
       " {'id': '48b99a69ff',\n",
       "  'question': 'Fix: Add extra_hosts for host.docker.internal on Linux',\n",
       "  'sort_order': 15,\n",
       "  'content': 'This update corrects the Docker Compose configuration to resolve the error when using the alias `host.docker.internal` on Linux systems. Since this alias does not resolve natively on Linux, the following entry was added to the affected container:\\n\\n```\\nyaml\\nkestra:\\n  image: kestra/kestra:latest\\n  pull_policy: always\\n  user: \"root\"\\n  command: server standalone\\n  volumes:\\n    # Add volume configurations here\\n  environment:\\n    # Add environment variables here\\n  ports:\\n    # Add ports here\\n  depends_on:\\n    # Add dependencies here\\n  extra_hosts:\\n    - \"host.docker.internal:host-gateway\"\\n```\\n\\nWith this change, containers that need to access host services via `host.docker.internal` will be able to do so correctly. For inter-container communication within the same network, it is recommended to use the service name directly.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/015_48b99a69ff_fix-add-extra_hosts-for-hostdockerinternal-on-linu.md'},\n",
       " {'id': 'df4171cea7',\n",
       "  'question': 'Fix: Add extra_hosts for taskRunner in the dbt-build',\n",
       "  'sort_order': 16,\n",
       "  'content': 'To resolve the issue with `host.docker.internal` not being recognized on Linux, add the `extraHosts` configuration to the `taskRunner` in the `dbt-build` task:\\n\\n```yaml\\ntaskRunner:\\n  type: io.kestra.plugin.scripts.runner.docker.Docker\\n  extraHosts:\\n    - \"host.docker.internal:host-gateway\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/016_df4171cea7_fix-add-extra_hosts-for-taskrunner-in-the-dbt-buil.md'},\n",
       " {'id': '9abc27b002',\n",
       "  'question': 'Kestra: Don’t forget to set GCP_CREDS variable',\n",
       "  'sort_order': 17,\n",
       "  'content': 'If you plan on using Kestra with Google Cloud Platform, make sure you set up the `GCP_CREDS` that will be used in flows with \"gcp\" in their name.\\n\\nTo set it:\\n\\n1. Go to **Namespaces** and select \"zoomcamp\" if you are using the examples from the lessons.\\n2. In the **KV Store** tab, create a new key as `GCP_CREDS`.\\n3. Set the type to JSON and paste the content of the `.json` file with credentials for the service account created.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-2/017_9abc27b002_kestra-dont-forget-to-set-gcp_creds-variable.md'},\n",
       " {'id': 'c717810bc6',\n",
       "  'question': 'Kestra: Backfill showing getting executed but not getting results or showing up in executions',\n",
       "  'sort_order': 1,\n",
       "  'content': 'It seems to be a bug. The current fix is to remove the timezone from triggers in the script. More on this bug is [here](https://github.com/kestra-io/kestra/issues/7227).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/001_c717810bc6_kestra-backfill-showing-getting-executed-but-not-g.md'},\n",
       " {'id': '687d54c6ba',\n",
       "  'question': 'Docker: Docker-compose takes infinitely long to install zip unzip packages for linux, which are required to unpack datasets.',\n",
       "  'sort_order': 2,\n",
       "  'content': \"To resolve the issue, you can try the following solutions:\\n\\n1. Add the `-Y` flag to `apt-get` to automatically agree to install additional packages.\\n   \\n   ```bash\\n   sudo apt-get install -y zip unzip\\n   ```\\n\\n2. Use the Python `ZipFile` package, which is included in all modern Python distributions. This can bypass the need to install `zip` and `unzip` packages.\\n\\n   ```python\\n   from zipfile import ZipFile\\n\\n   with ZipFile('file.zip', 'r') as zip_ref:\\n       zip_ref.extractall('destination_folder')\\n   ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/002_687d54c6ba_docker-docker-compose-takes-infinitely-long-to-ins.md'},\n",
       " {'id': 'ee4a3bc34d',\n",
       "  'question': 'GCS Bucket - error when writing data from web to GCS:',\n",
       "  'sort_order': 3,\n",
       "  'content': 'Make sure to use Nullable data types, such as [Int64](https://pandas.pydata.org/docs/user_guide/integer_na.html) when applicable.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/003_ee4a3bc34d_gcs-bucket-error-when-writing-data-from-web-to-gcs.md'},\n",
       " {'id': 'd2902a7227',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_5924f19e.png'}],\n",
       "  'question': \"GCS Bucket - te table: Error while reading data, error message: Parquet column 'XYZ' has type INT which does not match the target cpp_type DOUBLE. File: gs://path/to/some/blob.parquet\",\n",
       "  'sort_order': 4,\n",
       "  'content': 'Ultimately, when trying to ingest data into a BigQuery table, all files within a given directory must have the same schema.\\n\\nWhen dealing with datasets, such as the FHV Datasets from 2019, you may encounter schema inconsistencies. For example, the files for \\'2019-05\\' and \\'2019-06\\' have the columns \"PUlocationID\" and \"DOlocationID\" as integers, while for the period of \\'2019-01\\' through \\'2019-04\\', the same columns are defined as floats.\\n\\nWhen importing these files as Parquet to BigQuery, the first file will define the table schema. All subsequent files must have the same schema to append data correctly.\\n\\n<{IMAGE:image_1}>\\n\\nTo prevent errors like this, enforce the data types for the columns on the DataFrame before serializing/uploading them to BigQuery:\\n\\n```python\\npd.read_csv(\"path_or_url\").astype({\"col1_name\": \"datatype\", \"col2_name\": \"datatype\", ..., \"colN_name\": \"datatype\"})\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/004_d2902a7227_gcs-bucket-te-table-error-while-reading-data-error.md'},\n",
       " {'id': '950192cbcc',\n",
       "  'question': 'GCS Bucket: Fix Error when importing FHV data to GCS',\n",
       "  'sort_order': 5,\n",
       "  'content': \"If you receive the error \\n\\n```python\\ngzip.BadGzipFile: Not a gzipped file (b'\\\\n\\\\n')\\n```\\n\\nthis is because you have specified the wrong URL to the FHV dataset. Make sure to use:\\n\\n```\\nhttps://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/{dataset_file}.csv.gz\\n```\\n\\nEmphasize the `/releases/download` part of the URL.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/005_950192cbcc_gcs-bucket-fix-error-when-importing-fhv-data-to-gc.md'},\n",
       " {'id': '4ae927f8a0',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_d49e30cd.png'}],\n",
       "  'question': 'GCS Bucket - Load Data From URL list in to GCP Bucket',\n",
       "  'sort_order': 6,\n",
       "  'content': '<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/006_4ae927f8a0_gcs-bucket-load-data-from-url-list-in-to-gcp-bucke.md'},\n",
       " {'id': '33184c75bd',\n",
       "  'question': 'GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?',\n",
       "  'sort_order': 7,\n",
       "  'content': '- **Check the Schema**: Ensure that the schema of your dataset is correctly defined.\\n\\n- **Formatting Issues**: You might have incorrect formatting in your files.\\n\\n- **Upload Method**: Try uploading the CSV.GZ files without formatting or processing them through pandas. Use `wget` to download if necessary.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/007_33184c75bd_gcs-bucket-i-query-my-dataset-and-get-a-bad-charac.md'},\n",
       " {'id': 'a07b793f65',\n",
       "  'question': 'GCP BQ: \"bq: command not found\"',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Run the following command to check if \"BigQuery Command Line Tool\" is installed or not:\\n\\n```bash\\ngcloud components list\\n```\\n\\nYou can also use `bq.cmd` instead of `bq` to make it work.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/008_a07b793f65_gcp-bq-bq-command-not-found.md'},\n",
       " {'id': '0131ac93ac',\n",
       "  'question': 'GCP: Caution in using BigQuery - bigquery:no',\n",
       "  'sort_order': 9,\n",
       "  'content': \"Use BigQuery carefully:\\n\\n- I created my BigQuery dataset on an account where my free trial was exhausted and received a bill of $80.\\n- Use BigQuery under free credits and destroy all the datasets after creation.\\n- Check your billing daily, especially if you've spun up a VM.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/009_0131ac93ac_gcp-caution-in-using-bigquery-bigqueryno.md'},\n",
       " {'id': '61b908fe84',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_924b3959.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_4d72f30c.png'}],\n",
       "  'question': 'GCP BQ - Cannot read and write in different locations: source: EU, destination: US - Loading data from GCS into BigQuery (different Region):',\n",
       "  'sort_order': 10,\n",
       "  'content': 'Be careful when you create your resources on GCP; all of them must share the same region to load data from a GCS Bucket to BigQuery. If you forgot this step, you can create a new dataset in BigQuery using the same region as your GCS Bucket.\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>\\n\\nThis error indicates that your GCS Bucket and the BigQuery dataset are placed in different regions. You need to create a new dataset in BigQuery in the same region as your GCS Bucket and store the data in this newly created dataset.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/010_61b908fe84_gcp-bq-cannot-read-and-write-in-different-location.md'},\n",
       " {'id': 'c8d29f6862',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_1d6e2776.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_0bec8845.png'}],\n",
       "  'question': 'GCP BQ - Cannot read and write in different locations: source: <REGION_HERE>, destination: <ANOTHER_REGION_HERE>',\n",
       "  'sort_order': 11,\n",
       "  'content': 'Make sure to create the BigQuery dataset in the same location as your GCS Bucket. For instance, if your GCS Bucket was created in `us-central1`, then the BigQuery dataset must also be created in `us-central1`.\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/011_c8d29f6862_gcp-bq-cannot-read-and-write-in-different-location.md'},\n",
       " {'id': 'b98d573ce2',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_85e101a4.png'}],\n",
       "  'question': 'GCP BQ: Remember to save your queries',\n",
       "  'sort_order': 12,\n",
       "  'content': \"It's important to save your progress in the BigQuery SQL Editor frequently.\\n\\nHere are some tips:\\n\\n- **Save Regularly:** Use the save button at the top bar in the BigQuery SQL Editor. Your saved queries will be available on the left panel.\\n\\n  <{IMAGE:image_1}>\\n\\n- **Alternative Method:** Copy and paste your queries into a file using a text editor like Notepad++ or VS Code. Save it with a `.sql` extension to benefit from syntax highlighting.\\n\\nBy following these methods, you can avoid losing your work in case of unexpected browser issues.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/012_b98d573ce2_gcp-bq-remember-to-save-your-queries.md'},\n",
       " {'id': '57303f9d80',\n",
       "  'question': 'GCP BQ: Can I use BigQuery for real-time analytics in this project?',\n",
       "  'sort_order': 13,\n",
       "  'content': 'While real-time analytics might not be explicitly mentioned, BigQuery has real-time data streaming capabilities, allowing for potential integration in future project iterations.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/013_57303f9d80_gcp-bq-can-i-use-bigquery-for-real-time-analytics.md'},\n",
       " {'id': '1724e08426',\n",
       "  'question': 'GCP BQ: Unable to load data from external tables into a materialized table in BigQuery due to an invalid timestamp error that are added while appending data to the file in Google Cloud Storage',\n",
       "  'sort_order': 14,\n",
       "  'content': \"```plaintext\\ncould not parse 'pickup_datetime' as timestamp for field pickup_datetime (position 2)\\n```\\n\\nThis error is caused by invalid data in the timestamp column. To resolve this issue:\\n\\n1. Define the schema of the external table using the `STRING` datatype for the timestamp column. This allows queries to execute without errors.\\n2. Filter out the invalid timestamp rows during data import.\\n3. Insert the filtered rows into the materialized table, specifying the `TIMESTAMP` datatype for the timestamp fields.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/014_1724e08426_gcp-bq-unable-to-load-data-from-external-tables-in.md'},\n",
       " {'id': 'c2a18da218',\n",
       "  'question': 'GCP BQ: Error Message in BigQuery: annotated as a valid Timestamp, please annotate it as TimestampType(MICROS) or TimestampType(MILLIS)',\n",
       "  'sort_order': 15,\n",
       "  'content': 'When you encounter this BigQuery error, it typically relates to how timestamps are stored in Parquet files.\\n\\n### Solution:\\n\\nTo resolve this issue, you can modify the Parquet writing configuration by adding `use_deprecated_int96_timestamps=True` to the `pq.write_to_dataset` function. This setting writes timestamps in the INT96 format, which can be more compatible with BigQuery.\\n\\nHere’s how you can adjust the function:\\n\\n```python\\npq.write_to_dataset(\\n    table,\\n    root_path=root_path,\\n    filesystem=gcs,\\n    use_deprecated_int96_timestamps=True  # Write timestamps to INT96 Parquet format\\n)\\n```\\n\\n### References\\n\\n- [Stack Overflow - Parquet compatibility with PyArrow vs PySpark](https://stackoverflow.com/questions/48314880/are-parquet-file-created-with-pyarrow-vs-pyspark-compatible)\\n- [Stack Overflow - Editing Parquet files and datetime format errors](https://stackoverflow.com/questions/57798479/editing-parquet-files-with-python-causes-errors-to-datetime-format)\\n- [Reddit - Parquet Timestamp to BQ issues](https://www.reddit.com/r/bigquery/comments/16aoq0u/parquet_timestamp_to_bq_coming_across_as_int/?share_id=YXqCs5Jl6hQcw-kg6-VgF&utm_content=1&utm_medium=ios_app&utm_name=ioscss&utm_source=share&utm_term=1)\\n\\nUse the above configuration to ensure compatibility with Google BigQuery when dealing with timestamps in Parquet files.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/015_c2a18da218_gcp-bq-error-message-in-bigquery-annotated-as-a-va.md'},\n",
       " {'id': 'a83247e572',\n",
       "  'question': 'GCP BQ - Datetime columns in Parquet files created from Pandas show up as integer columns in BigQuery',\n",
       "  'sort_order': 16,\n",
       "  'content': '### Solution:\\n\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, use PyArrow to generate the Parquet file with the correct logical type for the datetime columns. Otherwise, they won\\'t be converted to a timestamp when loaded by BigQuery later on.\\n\\n```python\\nimport pyarrow as pa\\nimport pyarrow.parquet as pq\\nimport os\\n\\nif \\'data_exporter\\' not in globals():\\n    from mage_ai.data_preparation.decorators import data_exporter\\n\\n# Replace with the location of your service account key JSON file.\\nos.environ[\\'GOOGLE_APPLICATION_CREDENTIALS\\'] = \\'/home/src/personal-gcp.json\\'\\n\\nbucket_name = \"<YOUR_BUCKET_NAME>\"\\nobject_key = \\'nyc_taxi_data_2022.parquet\\'\\nwhere = f\\'{bucket_name}/{object_key}\\'\\n\\n@data_exporter\\ndef export_data(data, *args, **kwargs):\\n    table = pa.Table.from_pandas(data, preserve_index=False)\\n    gcs = pa.fs.GcsFileSystem()\\n    pq.write_table(\\n        table,\\n        where,\\n        # Convert integer columns in Epoch milliseconds\\n        # to Timestamp columns in microseconds (\\'us\\') so\\n        # they can be loaded into BigQuery with the right\\n        # data type\\n        coerce_timestamps=\\'us\\',\\n        filesystem=gcs\\n    )\\n```\\n\\n### Solution 2:\\n\\nIf you’re using Mage, in the last Data Exporter that writes to Google Cloud Storage, provide PyArrow with an explicit schema to generate the Parquet file with the correct logical type for the datetime columns.\\n\\n```python\\nschema = pa.schema([\\n    (\\'vendor_id\\', pa.int64()),\\n    (\\'lpep_pickup_datetime\\', pa.timestamp(\\'ns\\')),\\n    (\\'lpep_dropoff_datetime\\', pa.timestamp(\\'ns\\')),\\n    (\\'store_and_fwd_flag\\', pa.string()),\\n    (\\'ratecode_id\\', pa.int64()),\\n    (\\'pu_location_id\\', pa.int64()),\\n    (\\'do_location_id\\', pa.int64()),\\n    (\\'passenger_count\\', pa.int64()),\\n    (\\'trip_distance\\', pa.float64()),\\n    (\\'fare_amount\\', pa.float64()),\\n    (\\'extra\\', pa.float64()),\\n    (\\'mta_tax\\', pa.float64()),\\n    (\\'tip_amount\\', pa.float64()),\\n    (\\'tolls_amount\\', pa.float64()),\\n    (\\'improvement_surcharge\\', pa.float64()),\\n    (\\'total_amount\\', pa.float64()),\\n    (\\'payment_type\\', pa.int64()),\\n    (\\'trip_type\\', pa.int64()),\\n    (\\'congestion_surcharge\\', pa.float64()),\\n    (\\'lpep_pickup_month\\', pa.int64())\\n])\\n\\ntable = pa.Table.from_pandas(data, schema=schema)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/016_a83247e572_gcp-bq-datetime-columns-in-parquet-files-created-f.md'},\n",
       " {'id': '7c97a61529',\n",
       "  'question': 'GCP: BQ - Create External Table using Python',\n",
       "  'sort_order': 17,\n",
       "  'content': 'Reference:\\n\\n[https://cloud.google.com/bigquery/docs/external-data-cloud-storage](https://cloud.google.com/bigquery/docs/external-data-cloud-storage)\\n\\nSolution:\\n\\n```python\\nfrom google.cloud import bigquery\\n\\n# Set table_id to the ID of the table to create\\n\\ntable_id = f\"{project_id}.{dataset_name}.{table_name}\"\\n\\n# Construct a BigQuery client object\\n\\nclient = bigquery.Client()\\n\\n# Set the external source format of your table\\n\\nexternal_source_format = \"PARQUET\"\\n\\n# Set the source_uris to point to your data in Google Cloud\\n\\nsource_uris = [ f\\'gs://{bucket_name}/{object_key}/*\\']\\n\\n# Create ExternalConfig object with external source format\\n\\nexternal_config = bigquery.ExternalConfig(external_source_format)\\n\\n# Set source_uris that point to your data in Google Cloud\\n\\nexternal_config.source_uris = source_uris\\n\\nexternal_config.autodetect = True\\n\\ntable = bigquery.Table(table_id)\\n\\n# Set the external data configuration of the table\\n\\ntable.external_data_configuration = external_config\\n\\ntable = client.create_table(table)  # Make an API request.\\n\\nprint(f\\'Created table with external source: {table_id}\\')\\n\\nprint(f\\'Format: {table.external_data_configuration.source_format}\\')\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/017_7c97a61529_gcp-bq-create-external-table-using-python.md'},\n",
       " {'id': '88f2fb38b0',\n",
       "  'question': 'GCP BQ: Check BigQuery Table Exist And Delete',\n",
       "  'sort_order': 18,\n",
       "  'content': '### Reference\\n\\n[Stack Overflow - BigQuery Overwrite Table](https://stackoverflow.com/questions/60941726/can-bigquery-api-overwrite-existing-table-view-with-create-table-tables-inser)\\n\\n### Solution\\n\\nTo check if a BigQuery table exists and possibly delete it, utilize the following Python function before using `client.create_table`:\\n\\n```python\\nfrom google.cloud import bigquery\\n\\n# Initialize client\\nclient = bigquery.Client()\\n\\ndef table_exists(table_id, client):\\n    \"\"\"\\n    Check if a table already exists using the tableID.\\n\\n    :param table_id: str, the ID of the table\\n    :param client: bigquery.Client instance\\n    :return: Boolean\\n    \"\"\"\\n    try:\\n        client.get_table(table_id)\\n        return True\\n    except Exception as e:  # NotFound:\\n        return False\\n```\\n\\nUse this function to check table existence before creating a new table or taking further actions.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/018_88f2fb38b0_gcp-bq-check-bigquery-table-exist-and-delete.md'},\n",
       " {'id': '21e010f826',\n",
       "  'question': 'GCP BQ - Error: Missing close double quote (\") character',\n",
       "  'sort_order': 19,\n",
       "  'content': 'To avoid this error, you can upload data from Google Cloud Storage to BigQuery through BigQuery Cloud Shell using the command:\\n\\n```bash\\nbq load --autodetect --allow_quoted_newlines --source_format=CSV dataset_name.table_name \"gs://dtc-data-lake-bucketname/fhv/fhv_tripdata_2019-*.csv.gz\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/019_21e010f826_gcp-bq-error-missing-close-double-quote-character.md'},\n",
       " {'id': '852b1d09a0',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_199a39eb.jpg'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_e2a4f6bc.jpg'},\n",
       "   {'description': 'image #3',\n",
       "    'id': 'image_3',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_9f3cd4ef.jpg'}],\n",
       "  'question': 'GCP BQ - Cannot read and write in different locations: source: asia-south2, destination: US',\n",
       "  'sort_order': 20,\n",
       "  'content': 'Solution: This problem arises if your GCS and BigQuery storage are in different regions.\\n\\nOne potential way to solve it:\\n\\n- Go to your Google Cloud bucket and check the region in the field named \"Location.\"\\n\\n  <{IMAGE:image_1}>\\n\\n- In BigQuery, click on the three-dot icon near your project name and select \"Create dataset.\"\\n\\n  <{IMAGE:image_2}>\\n\\n- In the region field, choose the same region as your Google Cloud bucket.\\n\\n  <{IMAGE:image_3}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/020_852b1d09a0_gcp-bq-cannot-read-and-write-in-different-location.md'},\n",
       " {'id': '7cbadfe131',\n",
       "  'question': 'GCP BQ - Tip: Using Cloud Function to read csv.gz files from github directly to BigQuery in Google Cloud:',\n",
       "  'sort_order': 21,\n",
       "  'content': 'There are multiple benefits of using Cloud Functions to automate tasks in Google Cloud.\\n\\nUse the following Cloud Function Python script to load files directly into BigQuery. Set your project ID, dataset ID, and table ID accordingly.\\n\\n```python\\nimport tempfile\\nimport requests\\nimport logging\\nfrom google.cloud import bigquery\\n\\ndef hello_world(request):\\n    # table_id = <project_id.dataset_id.table_id>\\n    table_id = \\'de-zoomcap-project.dezoomcamp.fhv-2019\\'\\n\\n    # Create a new BigQuery client\\n    client = bigquery.Client()\\n\\n    for month in range(4, 13):\\n        # Define the schema for the data in the CSV.gz files\\n        url = \\'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-{:02d}.csv.gz\\'.format(month)\\n\\n        # Download the CSV.gz file from Github\\n        response = requests.get(url)\\n\\n        # Create new table if loading first month data else append\\n        write_disposition_string = \"WRITE_APPEND\" if month > 1 else \"WRITE_TRUNCATE\"\\n\\n        # Defining LoadJobConfig with schema of table to prevent it from changing with every table\\n        job_config = bigquery.LoadJobConfig(\\n            schema=[\\n                bigquery.SchemaField(\"dispatching_base_num\", \"STRING\"),\\n                bigquery.SchemaField(\"pickup_datetime\", \"TIMESTAMP\"),\\n                bigquery.SchemaField(\"dropOff_datetime\", \"TIMESTAMP\"),\\n                bigquery.SchemaField(\"PUlocationID\", \"STRING\"),\\n                bigquery.SchemaField(\"DOlocationID\", \"STRING\"),\\n                bigquery.SchemaField(\"SR_Flag\", \"STRING\"),\\n                bigquery.SchemaField(\"Affiliated_base_number\", \"STRING\"),\\n            ],\\n            skip_leading_rows=1,\\n            write_disposition=write_disposition_string,\\n            autodetect=True,\\n            source_format=\"CSV\",\\n        )\\n\\n        # Load the data into BigQuery\\n        # Create a temporary file to prevent the exception: AttributeError: \\'bytes\\' object has no attribute \\'tell\\'\\n        with tempfile.NamedTemporaryFile() as f:\\n            f.write(response.content)\\n            f.seek(0)\\n\\n            job = client.load_table_from_file(\\n                f,\\n                table_id,\\n                location=\"US\",\\n                job_config=job_config,\\n            )\\n\\n            job.result()\\n\\n        logging.info(\"Data for month %d successfully loaded into table %s.\", month, table_id)\\n\\n    return \\'Data loaded into table {}.\\'.format(table_id)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/021_7cbadfe131_gcp-bq-tip-using-cloud-function-to-read-csvgz-file.md'},\n",
       " {'id': '8bfd81c403',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_d9986dba.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_e9103d16.png'}],\n",
       "  'question': 'GCP BQ: When querying two different tables, external and materialized, why do you get the same result with count(distinct(*))?',\n",
       "  'sort_order': 22,\n",
       "  'content': 'You need to uncheck cache preferences in query settings\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/022_8bfd81c403_gcp-bq-when-querying-two-different-tables-external.md'},\n",
       " {'id': '66d3c42dd4',\n",
       "  'question': 'GCP BQ: How to handle type error from BigQuery and Parquet data?',\n",
       "  'sort_order': 23,\n",
       "  'content': 'When injecting data into GCS using Pandas, some datasets might have missing values in the `DOlocationID` and `PUlocationID` columns. By default, Pandas will cast these columns as `float`, leading to inconsistent data types between the Parquet files in GCS and the schema defined in BigQuery. You might encounter the following error:\\n\\n```bash\\nerror: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column \\'DOlocationID\\' has type INT64 which does not match the target cpp_type DOUBLE.\\n```\\n\\n### Solution:\\nFix the data type issue in the data pipeline:\\n\\n1. Before injecting data into GCS, use `astype` and `Int64` (which is different from `int64` and accepts both missing values and integers) to cast the columns.\\n\\n    Example:\\n    ```python\\n    df[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\\n    df[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\\n    ```\\n\\n2. It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/023_66d3c42dd4_gcp-bq-how-to-handle-type-error-from-bigquery-and.md'},\n",
       " {'id': '625c1f542a',\n",
       "  'question': 'GCP BQ: Invalid project ID. Project IDs must contain 6-63 lowercase letters, digits, or dashes.',\n",
       "  'sort_order': 24,\n",
       "  'content': 'The problem occurs when there is a misplacement of content after the `FROM` clause in BigQuery SQLs. Check to remove any extra spaces or symbols; ensure project IDs are in lowercase, digits, and dashes only.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/024_625c1f542a_gcp-bq-invalid-project-id-project-ids-must-contain.md'},\n",
       " {'id': '45b587e597',\n",
       "  'question': 'GCP BQ: Does BigQuery support multiple columns partition?',\n",
       "  'sort_order': 25,\n",
       "  'content': 'No. Based on the documentation for BigQuery, it does not support more than one column to be partitioned.\\n\\n[Source](https://cloud.google.com/bigquery/docs/partitioned-tables#limitations)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/025_45b587e597_gcp-bq-does-bigquery-support-multiple-columns-part.md'},\n",
       " {'id': '49b5277d77',\n",
       "  'question': 'GCP BQ: DATE() Error in BigQuery',\n",
       "  'sort_order': 26,\n",
       "  'content': '**Error Message:**\\n\\n```\\nPARTITION BY expression must be DATE(<timestamp_column>), DATE(<datetime_column>), DATETIME_TRUNC(<datetime_column>, DAY/HOUR/MONTH/YEAR), a DATE column, TIMESTAMP_TRUNC(<timestamp_column>, DAY/HOUR/MONTH/YEAR), DATE_TRUNC(<date_column>, MONTH/YEAR), or RANGE_BUCKET(<int64_column>, GENERATE_ARRAY(<int64_value>, <int64_value>[, <int64_value>]))\\n```\\n\\n**Solution:**\\n\\nConvert the column to datetime first:\\n\\n```python\\n# Convert pickup_datetime to datetime\\ndf[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"])\\n\\n# Convert dropOff_datetime to datetime\\ndf[\"dropOff_datetime\"] = pd.to_datetime(df[\"dropOff_datetime\"])\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/026_49b5277d77_gcp-bq-date-error-in-bigquery.md'},\n",
       " {'id': '8248dd9d7f',\n",
       "  'question': 'GCP BQ: When trying to cluster by DATE(tpep_pickup_datetime) it gives an error: Entries in the CLUSTER BY clause must be column names',\n",
       "  'sort_order': 27,\n",
       "  'content': 'No need to convert as you can cluster by a TIMESTAMP column directly in BigQuery. BigQuery supports clustering on TIMESTAMP, DATE, DATETIME, STRING, INT64, and BOOL types.\\n\\nClustering sorts data based on the timestamp to optimize queries with filters like `WHERE tpep_pickup_datetime BETWEEN ...`, rather than creating discrete partitions.\\n\\nIf your goal is to improve performance for time-based queries, combining partitioning by `DATE(event_time)` and clustering by `tpep_pickup_datetime` is a good approach.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/027_8248dd9d7f_gcp-bq-when-trying-to-cluster-by-datetpep_pickup_d.md'},\n",
       " {'id': '16b4ae94ef',\n",
       "  'question': 'GCP BQ - Native tables vs External tables in BigQuery?',\n",
       "  'sort_order': 28,\n",
       "  'content': 'Native tables are tables where the data is stored directly in BigQuery. In contrast, external tables store the data outside BigQuery, with BigQuery storing metadata about that external table.\\n\\n- **External tables:** These tables are not stored directly in BigQuery but are pulled in from a data lake such as Google Cloud Storage or AWS S3.\\n- **Materialized table:** This is a copy of an external table with data stored in BigQuery, consuming storage space.\\n\\nResources:\\n\\n- [External Tables Documentation](https://cloud.google.com/bigquery/docs/external-tables)\\n- [BigQuery Tables Introduction](https://cloud.google.com/bigquery/docs/tables-intro)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/028_16b4ae94ef_gcp-bq-native-tables-vs-external-tables-in-bigquer.md'},\n",
       " {'id': '24c64415c0',\n",
       "  'question': \"Why does my partitioned table in BigQuery show as non-partitioned even though BigQuery says it's partitioned?\",\n",
       "  'sort_order': 29,\n",
       "  'content': \"If your partitioned table in BigQuery shows as non-partitioned, it may be due to a delay in updating the table's details in the UI. The table is likely partitioned, but it may not show the updated information immediately.\\n\\nHere’s what you can do:\\n\\n- **Refresh your BigQuery UI:** If you're already inspecting the table in the BigQuery UI, try refreshing the page after a few minutes to ensure the table details are updated correctly.\\n\\n- **Open a new tab:** Alternatively, try opening a new tab in BigQuery and inspect the table details again. This can sometimes help to load the most up-to-date information.\\n\\n- **Be patient:** In some cases, there might be a slight delay in reflecting changes, but the table is very likely partitioned.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/029_24c64415c0_why-does-my-partitioned-table-in-bigquery-show-as.md'},\n",
       " {'id': '155949a31a',\n",
       "  'question': 'GCP BQ ML: Unable to run command (shown in video) to export ML model from BQ to GCS',\n",
       "  'sort_order': 30,\n",
       "  'content': '**Issue:**\\n\\nTried running command to export ML model from BQ to GCS from Week 3:\\n\\n```bash\\nbq --project_id taxi-rides-ny extract -m nytaxi.tip_model gs://taxi_ml_model/tip_model\\n```\\n\\nIt is failing with the following error:\\n\\n```\\nBigQuery error in extract operation: Error processing job Not found: Dataset was not found in location US\\n```\\n\\nI verified the BQ dataset and GCS bucket are in the same region, `us-west1`. Not sure why it gets location `US`. I couldn’t find the solution yet.\\n\\n**Solution:**\\n\\nPlease ensure the following:\\n\\n- Enter the correct `project_id` and `gcs_bucket` folder address.\\n- Example of a correct GCS bucket folder address:\\n  \\n  ```text\\n  gs://dtc_data_lake_optimum-airfoil-376815/tip_model\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/030_155949a31a_gcp-bq-ml-unable-to-run-command-shown-in-video-to.md'},\n",
       " {'id': 'a32ed35da6',\n",
       "  'question': 'Dim_zones.sql Dataset was not found in location US When Running fact_trips.sql',\n",
       "  'sort_order': 31,\n",
       "  'content': \"To solve this error, specify the location as `US` when creating the `dim_zones` table:\\n\\n```sql\\n{{ config(\\n\\nmaterialized='table',\\n\\nlocation='US'\\n\\n) }}\\n```\\n\\nUpdate this part, re-run the `dim_zones` creation, and then run `fact_trips`.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/031_a32ed35da6_dim_zonessql-dataset-was-not-found-in-location-us.md'},\n",
       " {'id': '47f7c8310a',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_51551549.png'}],\n",
       "  'question': 'GCP BQ ML - Export ML model to make predictions does not work for MacBook with Apple M1 chip (arm architecture).',\n",
       "  'sort_order': 32,\n",
       "  'content': '**Solution:**\\n\\nProceed with setting up `serving_dir` on your computer as described in the `extract_model.md` file. Then, instead of using:\\n\\n```bash\\ndocker pull tensorflow/serving\\n```\\n\\nuse:\\n\\n```bash\\ndocker pull emacski/tensorflow-serving\\n```\\n\\nThen run:\\n\\n```bash\\ndocker run -p 8500:8500 -p 8501:8501 --mount type=bind,source=`pwd`/serving_dir/tip_model,target=/models/tip_model -e MODEL_NAME=tip_model -t emacski/tensorflow-serving\\n```\\n\\nAfter that, run the `curl` command as instructed, and you should get a prediction.\\n\\n**Or new since Oct 2024:**\\n\\nBeta release of Docker VMM - the more performant alternative to Apple Virtualization Framework on macOS (requires Apple Silicon and macOS 12.5 or later). [https://docs.docker.com/desktop/features/vmm/](https://docs.docker.com/desktop/features/vmm/)\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/032_47f7c8310a_gcp-bq-ml-export-ml-model-to-make-predictions-does.md'},\n",
       " {'id': 'dcb8885c9b',\n",
       "  'question': 'VMs: What do I do if my VM runs out of space?',\n",
       "  'sort_order': 33,\n",
       "  'content': '- Try deleting data you’ve saved to your VM locally during ETLs.\\n- Kill processes related to deleted files.\\n- Download `ncdu` and look for large files (pay particular attention to files related to Prefect).\\n- If you delete any files related to Prefect, eliminate caching from your flow code.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/033_dcb8885c9b_vms-what-do-i-do-if-my-vm-runs-out-of-space.md'},\n",
       " {'id': 'c36b3f5196',\n",
       "  'question': 'GCP BQ - External and regular table',\n",
       "  'sort_order': 34,\n",
       "  'content': \"**External Table**\\n\\n- Data remains stored in a Google Cloud Storage (GCS) bucket.\\n\\n**Regular Table**\\n\\n- Data is copied into BigQuery storage.\\n\\n**Example of creating an external table:**\\n\\n```sql\\nCREATE OR REPLACE EXTERNAL TABLE `your_project.your_dataset.tablenamel`\\nOPTIONS (\\n  format = 'PARQUET',\\n  uris = ['gs://your-bucket-name/yellow_tripdata_2024-*.parquet']\\n);\\n```\\n\\n**Example of creating a regular table from an external table:**\\n\\n```sql\\nCREATE OR REPLACE TABLE `your_project.your_dataset.tablename`\\nAS\\nSELECT * FROM `your_project.your_dataset.yellow_taxi_external`;\\n```\\n\\n**Directly loading data from GCS into a regular BigQuery table without creating an external table:**\\n\\n```sql\\nCREATE OR REPLACE TABLE `your_project.your_dataset.yellow_taxi_table`\\nOPTIONS (\\n  format = 'PARQUET'\\n) AS\\nSELECT * FROM `your_project.your_dataset.external_table_placeholder`\\nFROM EXTERNAL_QUERY(\\n  'your_project.region-us.gcs_external',\\n  'SELECT * FROM `gs://your-bucket-name/yellow_tripdata_2024-*.parquet`'\\n);\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/034_c36b3f5196_gcp-bq-external-and-regular-table.md'},\n",
       " {'id': '18001c743b',\n",
       "  'question': 'Can BigQuery work with parquet files directly?',\n",
       "  'sort_order': 35,\n",
       "  'content': \"Yes, you can load your Parquet files directly into your GCP (Google Cloud Platform) Bucket first. Then, via BigQuery, you can create an external table of these Parquet files with a query statement like this:\\n\\n```sql\\nCREATE OR REPLACE EXTERNAL TABLE `module-3-data-warehouse.taxi_data.external_yellow_tripdata_2024`\\nOPTIONS (\\n  format = 'PARQUET',\\n  uris = ['gs://module3-dez/yellow_tripdata_2024-*.parquet']\\n);\\n```\\n\\nMake sure to adjust the SQL statement to your own situation and directories. The `*` symbol can be used as a wildcard to target Parquet files from all the months of 2024.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/035_18001c743b_can-bigquery-work-with-parquet-files-directly.md'},\n",
       " {'id': 'a9c1fd2a8f',\n",
       "  'question': 'Homework: What does it mean “Stop with loading the files into a bucket.”?',\n",
       "  'sort_order': 36,\n",
       "  'content': \"What they mean is that they don't want you to do anything more than that. You should load the files into the bucket and create an external table based on those files, but nothing like cleaning the data and putting it in parquet format.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/036_a9c1fd2a8f_homework-what-does-it-mean-stop-with-loading-the-f.md'},\n",
       " {'id': '5e0cffbc79',\n",
       "  'question': 'Homework: Reading parquets from nyc.gov directly into pandas returns Out of bounds error',\n",
       "  'sort_order': 37,\n",
       "  'content': 'If you try to read parquets directly from nyc.gov’s cloudfront into pandas, you might encounter this error:\\n\\n```python\\npyarrow.lib.ArrowInvalid: Casting from timestamp[us] to timestamp[ns] would result in out of bounds\\n```\\n\\n### Cause:\\n\\nThere is a data record where `dropOff_datetime` is set to the year 3019 instead of 2019. \\n\\nPandas uses \"timestamp[ns]\" and `int64` only allows a ~580-year range, centered on 2000. See `pd.Timestamp.max` and `pd.Timestamp.min`.\\n\\nThis becomes out of bounds when pandas tries to read it because 3019 > 2300 (approx value of `pd.Timestamp.max`).\\n\\n### Fix:\\n\\n1. **Use pyarrow to read the data:**\\n\\n   ```python\\n   import pyarrow.parquet as pq\\n   df = pq.read_table(\\'fhv_tripdata_2019-02.parquet\\').to_pandas(safe=False)\\n   ```\\n\\n   This will result in unusual timestamps for the offending record.\\n\\n2. **Read datetime columns separately:**\\n\\n   ```python\\n   table = pq.read_table(\\'taxi.parquet\\')\\n   datetimes = [\\'list of datetime column names\\']\\n   df_dts = pd.DataFrame()\\n\\n   for col in datetimes:\\n       df_dts[col] = pd.to_datetime(table.column(col), errors=\\'coerce\\')\\n   ```\\n\\n   The `errors=\\'coerce\\'` parameter will convert out-of-bounds timestamps into either the max or min.\\n\\n3. **Remove the offending rows using filter:**\\n\\n   ```python\\n   import pyarrow.compute as pc\\n\\n   table = pq.read_table(\\'taxi.parquet\\')\\n\\n   df = table.filter(\\n       pc.less_equal(table[\"dropOff_datetime\"], pa.scalar(pd.Timestamp.max))\\n   ).to_pandas()\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/037_5e0cffbc79_homework-reading-parquets-from-nycgov-directly-int.md'},\n",
       " {'id': '5f072b1cc7',\n",
       "  'question': 'Homework: Uploading files to GCS via GUI',\n",
       "  'sort_order': 38,\n",
       "  'content': \"This can help avoid schema issues in the homework. Download files locally and use the 'upload files' button in GCS at the desired path. You can upload many files at once. You can also choose to upload a folder.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/038_5f072b1cc7_homework-uploading-files-to-gcs-via-gui.md'},\n",
       " {'id': 'c46378ea2a',\n",
       "  'question': 'Homework - Qn 5: The partitioned/clustered table isn’t giving me the prediction I expected',\n",
       "  'sort_order': 39,\n",
       "  'content': 'Take a careful look at the format of the dates in the question.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/039_c46378ea2a_homework-qn-5-the-partitionedclustered-table-isnt.md'},\n",
       " {'id': '53e0e2f585',\n",
       "  'question': 'Homework: Qn 6: Did anyone get an exact match for one of the options given in Module 3 homework Q6?',\n",
       "  'sort_order': 40,\n",
       "  'content': 'Many people aren’t getting an exact match, but are very close to one of the options. It is suggested to choose the closest option.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/040_53e0e2f585_homework-qn-6-did-anyone-get-an-exact-match-for-on.md'},\n",
       " {'id': '55b6d6256f',\n",
       "  'question': 'Python: invalid start byte Error Message',\n",
       "  'sort_order': 41,\n",
       "  'content': '```python\\nUnicodeDecodeError: \\'utf-8\\' codec can\\'t decode byte 0xa0 in position 41721: invalid start byte\\n```\\n\\nSolution:\\n\\n1. When reading the data from the web into the pandas dataframe, specify the encoding:\\n   \\n   ```python\\n   pd.read_csv(dataset_url, low_memory=False, encoding=\\'latin1\\')\\n   ```\\n\\n2. When writing the dataframe from the local system to GCS as a CSV, specify the encoding:\\n   \\n   ```python\\n   df.to_csv(path_on_gsc, compression=\"gzip\", encoding=\\'utf-8\\')\\n   ```\\n\\nAlternative: Use `pd.read_parquet(url)`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/041_55b6d6256f_python-invalid-start-byte-error-message.md'},\n",
       " {'id': '0f2a26772e',\n",
       "  'question': 'Python: Generators in Python',\n",
       "  'sort_order': 42,\n",
       "  'content': 'A generator is a function in Python that returns an iterator using the `yield` keyword.\\n\\nA generator is a special type of iterable, similar to a list or a tuple, but with a crucial difference. Instead of creating and storing all the values in memory at once, a generator generates values on-the-fly as you iterate over it. This makes generators memory-efficient, particularly when dealing with large datasets.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/042_0f2a26772e_python-generators-in-python.md'},\n",
       " {'id': '577dfed594',\n",
       "  'question': 'Python: Easiest way to read multiple files at the same time?',\n",
       "  'sort_order': 43,\n",
       "  'content': 'The `read_parquet` function supports a list of files as an argument. The list of files will be merged into a single result table.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/043_577dfed594_python-easiest-way-to-read-multiple-files-at-the-s.md'},\n",
       " {'id': 'c58d3563a6',\n",
       "  'question': \"Python: These won't work. You need to make sure you use Int64.\",\n",
       "  'sort_order': 44,\n",
       "  'content': \"Incorrect:\\n\\n```python\\ndf['DOlocationID'] = pd.to_numeric(df['DOlocationID'], downcast=integer)\\n```\\n\\nor\\n\\n```python\\ndf['DOlocationID'] = df['DOlocationID'].astype(int)\\n```\\n\\nCorrect:\\n\\n```python\\ndf['DOlocationID'] = df['DOlocationID'].astype('Int64')\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-3/044_c58d3563a6_python-these-wont-work-you-need-to-make-sure-you-u.md'},\n",
       " {'id': 'b72ed00c7b',\n",
       "  'question': 'Warning when run load_yellow_data python script',\n",
       "  'sort_order': 1,\n",
       "  'content': \"### Warning Details:\\n\\n```\\nRuntimeWarning: As the c extension couldn't be imported, google-crc32c is using a pure python implementation that is significantly slower. If possible, please configure a c build environment and compile extension warnings.warn(_SLOW_CRC32C_WARNING, RuntimeWarning)\\n\\nFailed to upload ./yellow_tripdata_2024-01.parquet to GCS: Timeout of 120.0s exceeded, last exception: ('Connection aborted.', timeout('The write operation timed out'))\\n\\nFailed to upload ./yellow_tripdata_2024-03.parquet to GCS: Timeout of 120.0s exceeded, last exception: ('Connection aborted.', timeout('The write operation timed out'))\\n```\\n\\n### Issues:\\n\\n1. **google-crc32c Warning**: The Google Cloud Storage library is using a slow Python implementation instead of the optimized C version.\\n2. **Upload Timeout Error**: Your file uploads are timing out after 120 seconds.\\n\\n### Solutions:\\n\\n1. **Install the C-optimized google-crc32c**\\n   \\n   ```bash\\n   pip install --upgrade google-crc32c\\n   ```\\n\\n2. **Fix Google Cloud Storage Upload Timeout**\\n   \\n   - **Solution 1: Increase Timeout**\\n     \\n     ```python\\n     blob.upload_from_filename(file_path, timeout=300)  # Set timeout to 5 minutes\\n     ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/001_b72ed00c7b_warning-when-run-load_yellow_data-python-script.md'},\n",
       " {'id': '44ff6ae3df',\n",
       "  'question': 'dbt cloud Developer',\n",
       "  'sort_order': 2,\n",
       "  'content': \"Please be aware that the demos are done using dbt cloud Developer licensing. Although Team license is available to you upon creation of dbt cloud account for 14 days, the interface won't fully match the demo-ed experience.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/002_44ff6ae3df_dbt-cloud-developer.md'},\n",
       " {'id': 'df2ca336d5',\n",
       "  'question': 'DBT-Config ERROR on CLOUD IDE: No dbt_project.yml found at expected path',\n",
       "  'sort_order': 3,\n",
       "  'content': '### Error Details\\n\\n```\\nNo dbt_project.yml found at expected path /usr/src/develop/user-70471823426120/environment-70471823422561/repository-70471823410839/dbt_project.yml\\n```\\n\\n### Solution Steps\\n\\n1. **Verify Packages**:\\n   - Confirm that every entry in `packages.yml` (and their transitive dependencies) includes a `dbt_project.yml` file.\\n\\n2. **Initialize Project**:\\n   - Use the UI to initialize a new project.\\n\\n3. **Import Git Repo**:\\n   - For importing a Git repository of an existing dbt project, follow the instructions available at:\\n   \\n     [Import a project by Git URL](https://docs.getdbt.com/docs/cloud/git/import-a-project-by-git-url)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/003_df2ca336d5_dbt-config-error-on-cloud-ide-no-dbt_projectyml-fo.md'},\n",
       " {'id': 'c180431de3',\n",
       "  'question': 'DBT Cloud production error: prod dataset not available in location EU',\n",
       "  'sort_order': 4,\n",
       "  'content': \"**Problem:**\\n\\nI am trying to deploy my DBT models to production using DBT Cloud. The data should reside in BigQuery with the dataset location as EU. However, when running the model in production, a prod dataset is incorrectly created in BigQuery with a location of US. This leads to the build failing with the error:\\n\\n```\\nERROR 404: project.dataset:prod not available in location EU\\n```\\n\\nI have attempted various fixes, but I'm unsure if there is a simpler solution than creating my projects or buckets in the US location.\\n\\nNote: Everything functions properly in development mode; the issue arises only during job scheduling and execution in production.\\n\\n**Solution:**\\n\\n1. Manually create the `prod` dataset in BigQuery with the EU location specified.\\n2. Rerun the production job.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/004_c180431de3_dbt-cloud-production-error-prod-dataset-not-availa.md'},\n",
       " {'id': '0f6e2d3813',\n",
       "  'question': 'How do I solve the Dbt Cloud error: prod was not found in location?',\n",
       "  'sort_order': 5,\n",
       "  'content': \"You might encounter this error when trying to run dbt in production after following the instructions in the video ‘DE Zoomcamp 4.4.1 - Deployment Using dbt Cloud (Alternative A’):\\n\\n```\\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\nNot found: Dataset module-4-analytics-eng:prod was not found in location europe-west10\\n```\\n\\nThis error is easily resolved. Here are two solutions to address this issue:\\n\\n**Solution #1: Matching the dataset's data location with the source dataset**\\n\\n- Set your ‘prod’ dataset's data location to match the data location of your ‘trips_data_all’ dataset in BigQuery. The dbt process works for the instructor because her ‘prod’ dataset is in the same region as her source data. Since your ‘trips_data_all’ is in europe-west10 (or another region besides US), your prod needs to be there too, not US (which is the default setting when dbt creates a dataset for you in BigQuery).\\n\\n**Solution #2: Changing the dataset to <development dataset>**\\n\\n1. Go to: Deploy / Environments / Production (your production environment) / Settings.\\n2. Look at the Deployment credentials. There is an input field called Dataset. The input of ‘prod’ is likely here.\\n3. Replace ‘prod’ with the name of the Dataset that you worked with during development (before moving to Production). This is the Dataset name inside your BigQuery where you successfully ran ‘dbt debug’ and ‘dbt build’.\\n4. After saving, you are ready to rerun your Job!\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/005_0f6e2d3813_how-do-i-solve-the-dbt-cloud-error-prod-was-not-fo.md'},\n",
       " {'id': '29469cf158',\n",
       "  'question': 'Setup: No development environment',\n",
       "  'sort_order': 6,\n",
       "  'content': 'Error:\\n\\n```plaintext\\nThis project does not have a development environment configured. Please create a development environment and configure your development credentials to use the dbt IDE.\\n```\\n\\nThe error message provides guidance on resolving this issue. Follow the guide in the [dbt cloud setup documentation](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md). Additional instructions can be found in the [video @1:42](https://youtu.be/J0XCDyKiU64?si=2CTg3H63wyJTf5Vy&t=102).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/006_29469cf158_setup-no-development-environment.md'},\n",
       " {'id': 'b4a5d32d6b',\n",
       "  'question': 'Setup: Connecting dbt Cloud with BigQuery Error',\n",
       "  'sort_order': 7,\n",
       "  'content': \"**Runtime Error**\\n\\ndbt was unable to connect to the specified database.\\n\\nThe database returned the following error:\\n\\n```plaintext\\nDatabase Error\\n\\nAccess Denied: Project <project_name>: User does not have bigquery.jobs.create permission in project <project_name>.\\n```\\n\\nCheck your database credentials and try again. For more information, visit:\\n\\n[docs.getdbt.com](https://docs.getdbt.com/docs/configure-your-profile)\\n\\n**Steps to resolve error in Google Cloud:**\\n\\n1. Navigate to IAM & Admin and select IAM.\\n2. Click Grant Access if your newly created dbt service account isn't listed.\\n3. In the New principals field, add your service account.\\n4. Select a Role and search for BigQuery Job User to add.\\n5. Go back to dbt Cloud project setup and test your connection.\\n6. **Note:** Also add BigQuery Data Owner, Storage Object Admin, & Storage Admin to prevent permission issues later in the course.\\n\\n<{IMAGE:image_id}>\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/007_b4a5d32d6b_setup-connecting-dbt-cloud-with-bigquery-error.md'},\n",
       " {'id': '87fddc59b7',\n",
       "  'question': 'Setup: Failed to clone repository.',\n",
       "  'sort_order': 8,\n",
       "  'content': \"Error: Failed to clone repository.\\n\\n```\\n$ git clone git@github.com:DataTalksClub/data-engineering-zoomcamp.git /usr/src/develop/...\\n\\nCloning into '/usr/src/develop/...\\n\\nWarning: Permanently added '[github.com](http://github.com/),140.82.114.4' (ECDSA) to the list of known hosts.\\n\\ngit@github.com: Permission denied (publickey).\\n\\nfatal: Could not read from remote repository.\\n```\\n\\n**Issue:** You don’t have permissions to write to `DataTalksClub/data-engineering-zoomcamp.git`\\n\\n**Solutions:**\\n\\n1. **Clone the Forked Repository**\\n   \\n   Clone the repository using your forked repo, which contains your GitHub username. Then, specify the path as:\\n   \\n   ```\\n   [your github username]/data-engineering-zoomcamp.git\\n   ```\\n\\n2. **Create a Fresh Repo for dbt-lessons**\\n   \\n   Create a new repository for dbt-lessons. This approach is beneficial as it allows for branching and pull requests without affecting your other repositories. There's no need to create a subfolder for the dbt project files.\\n\\n3. **Use HTTPS Link**\\n   \\n   Switch to using an HTTPS link for cloning the repository if SSH access is not configured.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/008_87fddc59b7_setup-failed-to-clone-repository.md'},\n",
       " {'id': '32a878b1d5',\n",
       "  'question': 'Errors when I start the server in dbt cloud: Failed to start server. Permission denied (publickey)',\n",
       "  'sort_order': 9,\n",
       "  'content': '```\\nFailed to start server. Permission denied (publickey). fatal: Could not read from remote repository. Please make sure you have the correct access rights and the repository exists.\\n```\\n\\nUse the deploy keys in dbt repo details to create a public key in your repo, the issue will be solved.\\n\\nSteps in detail:\\n\\n1. **Find dbt Cloud’s SSH Key**\\n   - In dbt Cloud, go to **Settings > Account Settings > SSH Keys**\\n   - Copy the public SSH key displayed there.\\n\\n2. **Add It to GitHub**\\n   - Go to **GitHub > Settings > SSH and GPG Keys**\\n   - Click \"New SSH Key\", name it \"dbt Cloud\", and paste the key.\\n   - Click \"Add SSH Key\".\\n\\n3. **Try Restarting dbt Cloud**',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/009_32a878b1d5_errors-when-i-start-the-server-in-dbt-cloud-failed.md'},\n",
       " {'id': 'eab34fffe6',\n",
       "  'question': 'dbt: Job - Triggered by pull requests is disabled prerequisites when I try to create a new Continuous Integration job in dbt cloud.',\n",
       "  'sort_order': 10,\n",
       "  'content': \"Solution:\\n\\nCheck if you’re on the Developer Plan. As per the [prerequisites](https://docs.getdbt.com/docs/deploy/ci-jobs#prerequisites), you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\\n\\n- If you're on the Developer Plan, you'll need to upgrade to utilize CI Jobs.\\n\\nNote: A user mentioned that while on the Team Plan (trial period), the option was still disabled. In this case, you might try other troubleshooting steps for the Developer (free) plan.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/010_eab34fffe6_dbt-job-triggered-by-pull-requests-is-disabled-pre.md'},\n",
       " {'id': '99658f8d80',\n",
       "  'question': 'Setup: Your IDE session was unable to start. Please contact support.',\n",
       "  'sort_order': 11,\n",
       "  'content': '**Issue:** If the DBT cloud IDE is loading indefinitely and then giving you this error.\\n\\n**Solution:**\\n\\n1. Check the `dbt_cloud_setup.md` file.\\n2. Create an SSH Key.\\n3. Use `git clone` to import the repo into the dbt project.\\n4. Copy and paste the deploy key back in your repo settings.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/011_99658f8d80_setup-your-ide-session-was-unable-to-start-please.md'},\n",
       " {'id': 'e8fe39dd54',\n",
       "  'question': 'DBT: I am having problems with columns datatype while running DBT/BigQuery',\n",
       "  'sort_order': 12,\n",
       "  'content': '**Issue:** If you don’t define the column format while converting from CSV to Parquet, Python will \"choose\" based on the first rows.\\n\\n**Solution:** Define the schema while running the `web_to_gcp.py` pipeline.\\n\\nSebastian adapted the script:\\n\\n[GitHub Repository](https://github.com/sebastian2296/data-engineering-zoomcamp/blob/main/week_4_analytics_engineering/web_to_gcs.py)\\n\\nTo make the file work with gz files, add the following lines:\\n\\n- Ensure deletion of the file at the end of each iteration to avoid disk space issues:\\n\\n```python\\nfile_name_gz = f\"{service}_tripdata_{year}-{month}.csv.gz\"\\nopen(file_name_gz, \\'wb\\').write(r.content)\\nos.system(f\"gzip -d {file_name_gz}\")\\nos.system(f\"rm {file_name_init}.*\")\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/012_e8fe39dd54_dbt-i-am-having-problems-with-columns-datatype-whi.md'},\n",
       " {'id': '45bd267149',\n",
       "  'question': \"Parquet: “Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”\",\n",
       "  'sort_order': 13,\n",
       "  'content': \"Reason: Parquet files have their own schema. Some Parquet files for green data have records with decimals in the `ehail_fee` column.\\n\\nThere are some possible fixes:\\n\\n1. **Drop `ehail_fee` column**\\n   \\n   Drop the `ehail_fee` column, as it is not used. For instance, when creating a partitioned table from the external table in BigQuery:\\n   \\n   ```sql\\n   SELECT * EXCEPT (ehail_fee) FROM...\\n   ```\\n\\n2. **Modify SQL model**\\n   \\n   Modify `stg_green_tripdata.sql` model with this line:\\n   \\n   ```sql\\n   CAST(0 AS NUMERIC) AS ehail_fee\\n   ```\\n\\n3. **Modify Airflow DAG**\\n   \\n   Modify the Airflow DAG to make the conversion and avoid the error:\\n   \\n   ```python\\n   pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types={'ehail_fee': 'float64'}))\\n   ```\\n\\n4. **Using Pandas**\\n   \\n   Fix using Pandas when importing the file from CSV into a DataFrame:\\n\\n   ```python\\n   pd.from_csv(..., dtype=type_dict)\\n   ```\\n   \\n   Note: Regular `int64` in Pandas (from the numpy library) does not accept null values (NaN). Use Pandas `Int64` instead. The `type_dict` is a Python dictionary mapping column names to dtypes.\\n\\nSources:\\n\\n- [Pandas read_csv Documentation](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\\n- [Nullable integer data type — pandas 1.5.3 documentation](https://pandas.pydata.org/docs/user_guide/integer_na.html)\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/013_45bd267149_parquet-parquet-column-ehail_fee-has-type-double-w.md'},\n",
       " {'id': '6435e27634',\n",
       "  'question': 'Ingestion: When attempting to use the provided quick script to load trip data into GCS, you receive error Access Denied from the S3 bucket',\n",
       "  'sort_order': 14,\n",
       "  'content': 'If the provided URL isn’t working for you ([nyc-tlc.s3.amazonaws.com](https://nyc-tlc.s3.amazonaws.com/trip+data)/):\\n\\nWe can use the GitHub CLI to easily download the needed trip data from [GitHub](https://github.com/DataTalksClub/nyc-tlc-data) and manually upload to a GCS bucket.\\n\\nInstructions on how to download the CLI here: [GitHub](https://github.com/cli/cli)\\n\\nCommands to use:\\n\\n```bash\\ngh auth login\\n\\ngh release list -R DataTalksClub/nyc-tlc-data\\n\\ngh release download yellow -R DataTalksClub/nyc-tlc-data\\n\\ngh release download green -R DataTalksClub/nyc-tlc-data\\n```\\n\\nNow you can upload the files to a GCS bucket using the GUI.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/014_6435e27634_ingestion-when-attempting-to-use-the-provided-quic.md'},\n",
       " {'id': '28f6392ce5',\n",
       "  'question': 'Hack to load yellow and green trip data for 2019 and 2020',\n",
       "  'sort_order': 15,\n",
       "  'content': 'I initially followed [this script](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/03-data-warehouse/extras/web_to_gcs.py) but it was taking too long for the yellow trip data. When I downloaded and uploaded the Parquet files directly to GCS, it worked, but there was a schema inconsistency issue when creating the BigQuery table.\\n\\nI found another solution shared on YouTube, which was suggested by Victoria. You can watch it here:\\n\\n[[Optional] Hack for loading data to BigQuery for Week 4 - YouTube](https://www.youtube.com/watch?v=Mork172sK_c&t=22s&ab_channel=Victoria)\\n\\nMake sure to watch until the end, as there are some required schema changes.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/015_28f6392ce5_hack-to-load-yellow-and-green-trip-data-for-2019-a.md'},\n",
       " {'id': 'cdbabdd71a',\n",
       "  'question': 'GCP VM: All of sudden ssh stopped working for my VM after my last restart',\n",
       "  'sort_order': 16,\n",
       "  'content': 'One common cause experienced is lack of space after running Prefect several times. When running Prefect, check the folder `.prefect/storage` and delete the logs now and then to avoid the problem.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/016_cdbabdd71a_gcp-vm-all-of-sudden-ssh-stopped-working-for-my-vm.md'},\n",
       " {'id': '6022cc0440',\n",
       "  'question': 'GCP FREE TRIAL ACCOUNT ERROR',\n",
       "  'sort_order': 17,\n",
       "  'content': \"If you're encountering an error when trying to create a GCP free trial account, and it's not related to country restrictions, credit/debit card problems, or IP issues, it might be a random problem. Here’s a workaround:\\n\\n- Ask friends in your country to try signing up for the free trial using their Gmail accounts and their debit/credit cards.\\n- If one succeeds, you can temporarily use their Gmail to access the trial.\\n\\nThis method could help you bypass the issue!\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/017_6022cc0440_gcp-free-trial-account-error.md'},\n",
       " {'id': 'c01d835b44',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_da7b68d7.png'}],\n",
       "  'question': 'GCP VM: If you have lost SSH access to your machine due to lack of space. Permission denied (publickey)',\n",
       "  'sort_order': 18,\n",
       "  'content': 'You can try to do these steps:\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/018_c01d835b44_gcp-vm-if-you-have-lost-ssh-access-to-your-machine.md'},\n",
       " {'id': 'acbc9e940e',\n",
       "  'question': 'DBT: When running your first dbt model, if it fails with an error:',\n",
       "  'sort_order': 19,\n",
       "  'content': \"```\\n404 Not found: Dataset was not found in location US\\n\\n404 Not found: Dataset eighth-zenith-372015:trip_data_all was not found in location us-west1\\n```\\n\\nTo resolve this issue, follow these steps:\\n\\n1. **Verify Locations in BigQuery:**\\n   - Go to BigQuery and check the location of both:\\n     - The source dataset (e.g., `trips_data_all`).\\n     - The schema you’re writing to. The name should be in the format `dbt_<first initial><last name>` if you didn’t change the default settings.\\n\\n2. **Check Region Consistency:**\\n   - Ensure both datasets are in the same region. Typically, your source data is in your region, while the write location could be multi-regional (e.g., US).\\n   - If there's a mismatch, delete the datasets and recreate them in the specified region with the correct naming format.\\n\\n3. **Specify Single-Region Location:**\\n   - Instead of using a multi-regional location like `US`, specify the exact region (e.g., `US-east1`). Refer to [this Github comment](https://github.com/dbt-labs/dbt-bigquery/issues/19#issuecomment-635545315) for more details.\\n   - Additional guidance is available in [this post](https://learningdataengineering540969211.wordpress.com/dbt-cloud-and-bigquery-an-effort-to-try-and-resolve-location-issues/).\\n\\n4. **Update Location in DBT Cloud:**\\n   - Go to your profile page (top right drop-down --> profile).\\n   - Under Credentials --> Analytics (or your customized name), click on BigQuery >.\\n   - Hit Edit and update your location. You may need to re-upload your service account JSON to refresh your private key. Ensure the region matches exactly as specified in BigQuery.\\n\\nFollowing these steps should help resolve location-related errors when running your dbt models.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/019_acbc9e940e_dbt-when-running-your-first-dbt-model-if-it-fails.md'},\n",
       " {'id': 'e596dc3cbe',\n",
       "  'question': 'DBT: When executing dbt run after installing dbt-utils latest version i.e., 1.0.0 warning has generated',\n",
       "  'sort_order': 20,\n",
       "  'content': '**Error**: `dbt_utils.surrogate_key` has been replaced by `dbt_utils.generate_surrogate_key`\\n\\n**Fix**:\\n\\n- Replace `dbt_utils.surrogate_key` with `dbt_utils.generate_surrogate_key` in `stg_green_tripdata.sql`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/020_e596dc3cbe_dbt-when-executing-dbt-run-after-installing-dbt-ut.md'},\n",
       " {'id': '78e4da8fa6',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_a3c7073f.png'}],\n",
       "  'question': 'When executing dbt run after fact_trips.sql has been created, the task failed with error: \"Access Denied: BigQuery BigQuery: Permission denied while globbing file pattern.\"',\n",
       "  'sort_order': 21,\n",
       "  'content': '1. Fixed by adding the Storage Object Viewer role to the service account in use in BigQuery.\\n\\n2. Add the related roles to the service account in use in GCS.\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/021_78e4da8fa6_when-executing-dbt-run-after-fact_tripssql-has-bee.md'},\n",
       " {'id': '95b01285f5',\n",
       "  'question': 'When you are getting error dbt_utils not found',\n",
       "  'sort_order': 22,\n",
       "  'content': 'To resolve the \"dbt_utils not found\" error, follow these steps:\\n\\n1. Create a `packages.yml` file in the main project directory and add the package metadata:\\n   \\n   ```yaml\\n   packages:\\n     - package: dbt-labs/dbt_utils\\n       version: 0.8.0\\n   ```\\n   \\n2. Run the following command:\\n\\n   ```bash\\n   dbt deps\\n   ```\\n\\n3. Press Enter.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/022_95b01285f5_when-you-are-getting-error-dbt_utils-not-found.md'},\n",
       " {'id': 'd4ec58fd16',\n",
       "  'question': 'Lineage is currently unavailable. Check that your project does not contain compilation errors or contact support if this error persists.',\n",
       "  'sort_order': 23,\n",
       "  'content': 'Ensure you properly format your YAML file. Check the build logs if the run was completed successfully. You can expand the command history console (where you type the `--vars \\'{\"is_test_run\": \"false\"}\\'`) and click on any stage’s logs to expand and read error messages or warnings.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/023_d4ec58fd16_lineage-is-currently-unavailable-check-that-your-p.md'},\n",
       " {'id': '55fac499b0',\n",
       "  'question': 'Build: Why do my Fact_trips only contain a few days of data?',\n",
       "  'sort_order': 24,\n",
       "  'content': 'Make sure you use:\\n\\n```bash\\ndbt run --var \\'is_test_run: false\\'\\n```\\nor\\n\\n```bash\\ndbt build --var \\'is_test_run: false\\'\\n```\\n\\nWatch out for formatted text from this document: re-type the single quotes. If that does not work, use:\\n\\n```bash\\n--vars \\'{\"is_test_run\": \"false\"}\\'\\n```\\n\\nwith each phrase separately quoted.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/024_55fac499b0_build-why-do-my-fact_trips-only-contain-a-few-days.md'},\n",
       " {'id': '9cce7ad112',\n",
       "  'question': 'Build: Why do my fact_trips only contain one month of data?',\n",
       "  'sort_order': 25,\n",
       "  'content': 'Check if you specified the `if_exists` argument correctly when writing data from GCS to BigQuery. \\n\\nWhen I wrote my automated flow for each month of the years 2019 and 2020 for green and yellow data, I had specified `if_exists=\"replace\"` while experimenting with the flow setup. Once you want to run the flow for all months in 2019 and 2020, make sure to set `if_exists=\"append\"`.\\n\\n- `if_exists=\"replace\"` will replace the whole table with only the month data that you are writing into BigQuery in that one iteration -> you end up with only one month in BigQuery (the last one you inserted).\\n\\n- `if_exists=\"append\"` will append the new monthly data -> you end up with data from all months.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/025_9cce7ad112_build-why-do-my-fact_trips-only-contain-one-month.md'},\n",
       " {'id': '5a1a61b6a8',\n",
       "  'question': 'BigQuery returns an error when I try to run the dm_monthly_zone_revenue.sql model.',\n",
       "  'sort_order': 26,\n",
       "  'content': 'After the second SELECT, change this line:\\n\\n```sql\\n date_trunc(\\'month\\', pickup_datetime) as revenue_month,\\n```\\n\\nTo this line:\\n\\n```sql\\n date_trunc(pickup_datetime, month) as revenue_month,\\n```\\n\\nMake sure that \"month\" isn’t surrounded by quotes!',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/026_5a1a61b6a8_bigquery-returns-an-error-when-i-try-to-run-the-dm.md'},\n",
       " {'id': '2e2f27a6c0',\n",
       "  'question': 'Replace: {{ dbt_utils.surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}',\n",
       "  'sort_order': 27,\n",
       "  'content': 'For this use:\\n\\n```sql\\n{{ dbt_utils.generate_surrogate_key([ field_a, field_b, field_c, …, field_z ]) }}\\n```\\n\\nAdditionally, add a global variable in `dbt_project.yml`. (...)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/027_2e2f27a6c0_replace-dbt_utilssurrogate_key-field_a-field_b-fie.md'},\n",
       " {'id': 'ba210ca8ef',\n",
       "  'question': 'I changed location in dbt, but dbt run still gives me an error',\n",
       "  'sort_order': 28,\n",
       "  'content': '- Remove the dataset from BigQuery that was created by dbt.\\n- Run `dbt run` again so that it will recreate the dataset in BigQuery with the correct location.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/028_ba210ca8ef_i-changed-location-in-dbt-but-dbt-run-still-gives.md'},\n",
       " {'id': '3cc457b0de',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_71e1a1ed.png'}],\n",
       "  'question': 'DBT: I ran dbt run without specifying variable which gave me a table of 100 rows. I ran again with the variable value specified but my table still has 100 rows in BQ.',\n",
       "  'sort_order': 29,\n",
       "  'content': \"Remove the dataset from BigQuery created by dbt and run again (with test disabled) to ensure the dataset created has all the rows.\\n\\nDBT: Why am I getting a new dataset after running my CI/CD Job? / What is this new dbt dataset in BigQuery?\\n\\nWhen you create the CI/CD job, under 'Compare Changes against an environment (Deferral)' make sure that you select 'No; do not defer to another environment'. Otherwise, dbt won’t merge your dev models into production models; it will create a new environment called ‘dbt_cloud_pr_number of pull request’\\n\\n<{IMAGE:image_1}>\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/029_3cc457b0de_dbt-i-ran-dbt-run-without-specifying-variable-whic.md'},\n",
       " {'id': 'a31defac51',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_d2b29adb.png'}],\n",
       "  'question': 'Why do we need the Staging dataset?',\n",
       "  'sort_order': 30,\n",
       "  'content': '<{IMAGE:image_1}>\\n\\nStaging, as the name suggests, acts as an intermediary between raw datasets and the final fact and dimension tables. It helps in transforming raw data into a more usable format. In staging, datasets are typically materialized as views rather than tables.\\n\\nIn the project, you focus on creating production and `dbt_name + trips_data_all`; the staging dataset serves its role behind the scenes.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/030_a31defac51_why-do-we-need-the-staging-dataset.md'},\n",
       " {'id': '53afe4bee2',\n",
       "  'question': 'DBT: Docs Served but Not Accessible via Browser',\n",
       "  'sort_order': 31,\n",
       "  'content': 'Try removing the `network: host` line in `docker-compose`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/031_53afe4bee2_dbt-docs-served-but-not-accessible-via-browser.md'},\n",
       " {'id': '04f55d5846',\n",
       "  'question': 'BigQuery adapter: 404 Not found: Dataset was not found in location europe-west6',\n",
       "  'sort_order': 32,\n",
       "  'content': '1. Go to **Account settings** > **Project** > **Analytics**.\\n2. Click on your connection.\\n3. Scroll down to **Location** and type in the GCP location exactly as displayed in GCP (e.g., `europe-west6`). You might need to reupload your GCP key.\\n\\n4. Delete your dataset in Google BigQuery (GBQ).\\n5. Rebuild the project using the command:\\n   \\n   ```bash\\n   dbt build\\n   ```\\n\\n6. The newly built dataset should be in the correct location.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/032_04f55d5846_bigquery-adapter-404-not-found-dataset-was-not-fou.md'},\n",
       " {'id': 'f0ad880ffb',\n",
       "  'question': 'Dbt+git - Main branch is “read-only”',\n",
       "  'sort_order': 33,\n",
       "  'content': 'Create a new branch to edit. More on this can be found [here in the dbt docs](https://docs.getdbt.com/docs/collaborate/git/version-control-basics).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/033_f0ad880ffb_dbtgit-main-branch-is-read-only.md'},\n",
       " {'id': 'c1acd7331a',\n",
       "  'question': \"Dbt+git: It appears that I can't edit the files because I'm in read-only mode. Does anyone know how I can change that?\",\n",
       "  'sort_order': 34,\n",
       "  'content': '1. **Create a New Branch:**\\n   - You need to create a new branch for development to make changes.\\n   \\n   ```bash\\n   git checkout -b your-feature-branch\\n   ```\\n   \\n2. **Switch to the New Branch:**\\n   - This allows you to make edits.\\n\\n   ```bash\\n   git checkout your-feature-branch\\n   ```\\n\\n3. **Commit and Push Changes:**\\n   - Once you\\'ve made your changes, commit and push them to the main branch.\\n\\n   ```bash\\n   git add .\\n   git commit -m \"Your commit message\"\\n   git push origin your-feature-branch\\n   ```\\n\\n4. **Merge Changes to Main:**\\n   - Finally, merge your changes from your feature branch to the main branch.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/034_c1acd7331a_dbtgit-it-appears-that-i-cant-edit-the-files-becau.md'},\n",
       " {'id': '3d51ece7b9',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_c35c101c.png'}],\n",
       "  'question': 'Dbt deploy + Git CI: cannot create CI checks job for deployment to Production',\n",
       "  'sort_order': 35,\n",
       "  'content': \"**Error:**\\n\\n```\\nTriggered by pull requests\\n\\nThis feature is only available for dbt repositories connected through dbt Cloud's native integration with Github, Gitlab, or Azure DevOps\\n```\\n\\n**Solution:**\\n\\nContrary to the [guide on DTC repo](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/dbt_cloud_setup.md), don’t use the Git Clone option. Use the Github one instead. A step-by-step guide to unlink Git Clone and relink with Github is available in the next entry.\\n\\n<{IMAGE:image_1}>\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/035_3d51ece7b9_dbt-deploy-git-ci-cannot-create-ci-checks-job-for.md'},\n",
       " {'id': '1c4ac5d626',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_7800f401.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_8efd4f76.png'},\n",
       "   {'description': 'image #3',\n",
       "    'id': 'image_3',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_4e68416b.png'}],\n",
       "  'question': 'Dbt deploy + Git CI - Unable to configure Continuous Integration (CI) with Github',\n",
       "  'sort_order': 36,\n",
       "  'content': 'If you’re trying to configure CI with Github and on the job’s options you can’t see `Run on Pull Requests?` on triggers, follow these steps to reconnect using a native connection instead of cloning by SSH:\\n\\n1. **Connect Your Github Account**  \\n   - Go to `Profile Settings > Linked Accounts`.\\n   - Connect your Github account with the dbt project, allowing the requested permissions.  \\n   - More information can be found [here](https://docs.getdbt.com/docs/collaborate/git/connect-github).\\n   \\n   <{IMAGE:image_1}>\\n   \\n2. **Disconnect Current Configuration**  \\n   - Navigate to `Account Settings > Projects (analytics) > Github connection`.\\n   - Click the `Disconnect` button at the bottom left.\\n\\n3. **Reconfigure Github Connection**  \\n   - After disconnecting, configure again by choosing Github.\\n   - Select your repository from all allowed repositories to work with dbt. Your setup will now be ready.\\n   \\n   <{IMAGE:image_2}>\\n   \\n4. **Configure Triggers**  \\n   - Go to `Deploy > Job Configuration`.\\n   - Scroll down to `Triggers` where you can see the option `Run on Pull Requests:`\\n   \\n   <{IMAGE:image_3}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/036_1c4ac5d626_dbt-deploy-git-ci-unable-to-configure-continuous-i.md'},\n",
       " {'id': 'c37b2ed3a5',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_ea2e4bf2.png'}],\n",
       "  'question': \"Compilation Error: Model 'model.my_new_project.stg_green_tripdata' (models/staging/stg_green_tripdata.sql) depends on a source named 'staging.green_trip_external' which was not found\",\n",
       "  'sort_order': 37,\n",
       "  'content': \"If you're following video DE Zoomcamp 4.3.1 - Building the First DBT Models, you may encounter an issue at 14:25 where the Lineage graph isn't displayed and a Compilation Error occurs, as shown in the attached image.\\n\\nDon't worry—a quick fix for this is to simply save your `schema.yml` file. Once you've done this, you should be able to view your Lineage graph without any further issues.\\n\\n<{IMAGE:image_1}>\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/037_c37b2ed3a5_compilation-error-model-modelmy_new_projectstg_gre.md'},\n",
       " {'id': '8bfd724e4f',\n",
       "  'question': \"Compilation Error in test accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_ (models/staging/schema.yml)  'NoneType' object is not iterable\",\n",
       "  'sort_order': 38,\n",
       "  'content': 'In the macro `test_accepted_values` (found in `tests/generic/builtin.sql`), an error was triggered by the test `accepted_values_stg_green_tripdata_Payment_type__False___var_payment_type_values_` located in `models/staging/schema.yml`.\\n\\nTo resolve this issue, ensure the following variable is added to your `dbt_project.yml` file:\\n\\n```yaml\\nvars:\\n  payment_type_values: [1, 2, 3, 4, 5, 6]\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/038_8bfd724e4f_compilation-error-in-test-accepted_values_stg_gree.md'},\n",
       " {'id': '89733da275',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_c3c0865e.png'}],\n",
       "  'question': 'dbt: macro errors with get_payment_type_description(payment_type)',\n",
       "  'sort_order': 39,\n",
       "  'content': \"You will face this issue if you copied and pasted the exact macro directly from the data-engineering-zoomcamp repo.\\n\\n### Error Message\\n\\n```\\nBigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]; reason: invalidQuery, location: query, message: No matching signature for operator CASE for argument types: STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, INT64, STRING, NULL at [35:5]')\\n```\\n\\n### Solution\\n\\nTo resolve this issue, change the data type of the numbers (1, 2, 3, etc.) to text by enclosing them in quotes. The `payment_type` data type should be a string.\\n\\n#### Updated Macro\\n\\n```jinja\\n{#\\nThis macro returns the description of the payment_type\\n#}\\n\\n{% macro get_payment_type_description(payment_type) -%}\\n\\ncase {{ payment_type }}\\n\\nwhen '1' then 'Credit card'\\n\\nwhen '2' then 'Cash'\\n\\nwhen '3' then 'No charge'\\n\\nwhen '4' then 'Dispute'\\n\\nwhen '5' then 'Unknown'\\n\\nwhen '6' then 'Voided trip'\\n\\nend\\n\\n{%- endmacro %}\\n```\\n\\n<{IMAGE:image_1}>\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/039_89733da275_dbt-macro-errors-with-get_payment_type_description.md'},\n",
       " {'id': '2d05496ddf',\n",
       "  'question': 'Troubleshooting in dbt:',\n",
       "  'sort_order': 40,\n",
       "  'content': 'The dbt error log contains a link to BigQuery. When you follow it, you will see your query, and the problematic line will be highlighted.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/040_2d05496ddf_troubleshooting-in-dbt.md'},\n",
       " {'id': '58586a16a6',\n",
       "  'question': 'DBT: Why changing the target schema to “marts” actually creates a schema named “dbt_marts” instead?',\n",
       "  'sort_order': 41,\n",
       "  'content': 'It is a default behavior of dbt to [append custom schema to the initial schema](https://docs.getdbt.com/docs/build/custom-schemas#why-does-dbt-concatenate-the-custom-schema-to-the-target-schema). To override this behavior, create a macro named `generate_schema_name.sql`:\\n\\n```sql\\n{% macro generate_schema_name(custom_schema_name, node) -%}\\n\\n{%- set default_schema = target.schema -%}\\n\\n{%- if custom_schema_name is none -%}\\n\\n{{ default_schema }}\\n\\n{%- else -%}\\n\\n{{ custom_schema_name | trim }}\\n\\n{%- endif -%}\\n\\n{%- endmacro %}\\n```\\n\\nNow you can override the default custom schema in `dbt_project.yml`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/041_58586a16a6_dbt-why-changing-the-target-schema-to-marts-actual.md'},\n",
       " {'id': 'c7ca760fed',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_25bb53c3.png'}],\n",
       "  'question': 'How to set subdirectory of the github repository as the dbt project root',\n",
       "  'sort_order': 42,\n",
       "  'content': 'There is a project setting which allows you to set `Project subdirectory` in dbt cloud:\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/042_c7ca760fed_how-to-set-subdirectory-of-the-github-repository-a.md'},\n",
       " {'id': '3ce6d0445f',\n",
       "  'question': \"Compilation Error: Model 'model.XXX' (models/<model_path>/XXX.sql) depends on a source named '<a table name>' which was not found\",\n",
       "  'sort_order': 43,\n",
       "  'content': \"Remember to modify your `.sql` models to read from existing table names in BigQuery/Postgres DB.\\n\\nExample:\\n\\n```sql\\nselect * from {{ source('staging', '<your table name in the database>') }}\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/043_3ce6d0445f_compilation-error-model-modelxxx-modelsmodel_pathx.md'},\n",
       " {'id': '2d17397995',\n",
       "  'question': \"Compilation Error: Model '<model_name>' (<model_path>) depends on a node named '<seed_name>' which was not found (Production Environment)\",\n",
       "  'sort_order': 44,\n",
       "  'content': 'Make sure that you create a pull request from your Development branch to the Production branch (main by default). After that, check in your `seeds` folder to ensure the seed file is inside it. Another thing to check is your `.gitignore` file. Make sure that the `.csv` extension is not included.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/044_2d17397995_compilation-error-model-model_name-model_path-depe.md'},\n",
       " {'id': 'a17cb536b1',\n",
       "  'question': 'When executing dbt run after using fhv_tripdata as an external table: you get \"Access Denied: BigQuery BigQuery: Permission denied\"',\n",
       "  'sort_order': 45,\n",
       "  'content': '1. Go to your dbt cloud service account.\\n\\n2. Add the `Storage Object Admin` and `Storage Admin` roles in addition to `BigQuery Admin`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/045_a17cb536b1_when-executing-dbt-run-after-using-fhv_tripdata-as.md'},\n",
       " {'id': 'b26658a658',\n",
       "  'question': 'How to automatically infer the column data type (pandas missing value issues)?',\n",
       "  'sort_order': 46,\n",
       "  'content': 'Problem: When injecting data to BigQuery, you may face a type error. This is because pandas by default will parse integer columns with missing values as float type.\\n\\nSolution:\\n\\nOne way to solve this problem is to specify or cast the data type as `Int64` during the data transformation stage.\\n\\nIf specifying all the integer columns is inconvenient, you can use `convert_dtypes` to infer the data type automatically.\\n\\n- Make pandas infer the correct data type (as pandas parse int with missing as float):\\n\\n```python\\n# Fill missing values with a placeholder\\n df.fillna(-999999, inplace=True)\\n\\n# Infer data types\\n df = df.convert_dtypes()\\n\\n# Replace placeholder with None\\n df = df.replace(-999999, None)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/046_b26658a658_how-to-automatically-infer-the-column-data-type-pa.md'},\n",
       " {'id': 'fd607affe5',\n",
       "  'question': \"When loading GitHub repo raise exception that 'taxi_zone_lookup' not found\",\n",
       "  'sort_order': 47,\n",
       "  'content': \"Seed files are loaded from a directory named 'seed'. Therefore, you should rename the directory currently named 'data' to 'seed'.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/047_fd607affe5_when-loading-github-repo-raise-exception-that-taxi.md'},\n",
       " {'id': '9c40a115f8',\n",
       "  'question': '‘taxi_zone_lookup’ not found',\n",
       "  'sort_order': 48,\n",
       "  'content': \"Check the `.gitignore` file and make sure you don’t have `*.csv` in it.\\n\\nIf you're encountering a dbt error with the following message:\\n\\n```\\nRuntime Error in rpc request (from remote system.sql)\\n404 Not found: Table dtc-de-0315:trips_data_all.green_tripdata_partitioned was not found in location europe-west6\\nLocation: europe-west6\\nJob ID: 168ee9bd-07cd-4ca4-9ee0-4f6b0f33897c\\n```\\n\\n- Verify that all your datasets are configured with the correct region. For example, use `europe-west6` instead of a general region like `EU`.\\n- To update the region in dbt settings:\\n  - Go to `dbt -> projects -> optional settings` \\n  - Manually set the location to match the required region.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/048_9c40a115f8_taxi_zone_lookup-not-found.md'},\n",
       " {'id': '1f4fd984f5',\n",
       "  'question': 'Data type errors when ingesting with parquet files',\n",
       "  'sort_order': 49,\n",
       "  'content': \"The easiest way to avoid these errors is by ingesting the relevant data in a `.csv.gz` file type. Then, do:\\n\\n```sql\\nCREATE OR REPLACE EXTERNAL TABLE `dtc-de.trips_data_all.fhv_tripdata`\\nOPTIONS (\\n  format = 'CSV',\\n  uris = ['gs://dtc_data_lake_dtc-de-updated/data/fhv_all/fhv_tripdata_2019-*.csv.gz']\\n);\\n```\\n\\nThis example should help you avoid data type issues for week 4.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/049_1f4fd984f5_data-type-errors-when-ingesting-with-parquet-files.md'},\n",
       " {'id': 'd9072c7279',\n",
       "  'question': 'Inconsistent number of rows when re-running fact_trips model',\n",
       "  'sort_order': 50,\n",
       "  'content': 'This issue arises from the way deduplication is handled in two staging files.\\n\\n**Solution:**\\n\\n- Add an `ORDER BY` clause in the `PARTITION BY` section of both staging files.\\n- Continue adding columns to the `ORDER BY` clause until the row count in the `fact_trips` table is consistent upon re-running the model.\\n\\n**Explanation:**\\n\\nWe partition by `vendor_id` and `pickup_datetime`, selecting the first row (`rn=1`) from these partitions. These partitions lack an order, so every execution might yield a different first row. The inconsistency leads to different rows being processed, possibly with or without an unknown borough. Consequently, the `fact_trips` model discards a varying number of rows based on the presence of unknown boroughs.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/050_d9072c7279_inconsistent-number-of-rows-when-re-running-fact_t.md'},\n",
       " {'id': 'cbaecfefaf',\n",
       "  'question': 'Data Type Error when running fact table',\n",
       "  'sort_order': 51,\n",
       "  'content': \"If you encounter a data type error on the `trip_type` column, it may be due to some `nan` values that aren't null in BigQuery.\\n\\n**Solution:** Try casting it to `FLOAT` datatype instead of `NUMERIC`. \\n\\n```sql\\nSELECT CAST(trip_type AS FLOAT) FROM your_table;\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/051_cbaecfefaf_data-type-error-when-running-fact-table.md'},\n",
       " {'id': '1d2222e38a',\n",
       "  'question': 'CREATE TABLE has columns with duplicate name locationid.',\n",
       "  'sort_order': 52,\n",
       "  'content': \"This error could result if you are using a `SELECT *` query without specifying the table names.\\n\\nExample:\\n\\n```sql\\nWITH dim_zones AS (\\n  SELECT * FROM `engaged-cosine-374921`.`dbt_victoria_mola`.`dim_zones`\\n  WHERE borough != 'Unknown'\\n),\\nfhv AS (\\n  SELECT * FROM `engaged-cosine-374921`.`dbt_victoria_mola`.`stg_fhv_tripdata`\\n)\\nSELECT * FROM fhv\\nINNER JOIN dim_zones AS pickup_zone\\nON fhv.PUlocationID = pickup_zone.locationid\\nINNER JOIN dim_zones AS dropoff_zone\\nON fhv.DOlocationID = dropoff_zone.locationid;\\n```\\n\\nTo resolve, replace with:\\n\\n```sql\\nSELECT fhv.* FROM fhv\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/052_1d2222e38a_create-table-has-columns-with-duplicate-name-locat.md'},\n",
       " {'id': '9f2048fc73',\n",
       "  'question': 'Bad int64 value: 0.0 error',\n",
       "  'sort_order': 53,\n",
       "  'content': 'Some ehail fees are null, causing a `Bad int64 value: 0.0` error when casting them to integer.\\n\\nSolution:\\n\\n- Use `safe_cast`, which returns NULL instead of throwing an error. Implement `safe_cast` from the `dbt_utils` function in Jinja code for casting into integer as follows:\\n\\n  ```jinja\\n  {{ dbt_utils.safe_cast(\\'ehail_fee\\', api.Column.translate_type(\"integer\")) }} as ehail_fee,\\n  ```\\n\\n- Alternatively, use `safe_cast(ehail_fee as integer)` without relying on `dbt_utils`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/053_9f2048fc73_bad-int64-value-00-error.md'},\n",
       " {'id': '0c2badaa56',\n",
       "  'question': 'Bad int64 value: 2.0/1.0 error',\n",
       "  'sort_order': 54,\n",
       "  'content': \"You might encounter this when building the `fact_trips.sql` model. The issue may be with the `payment_type_description` field.\\n\\nUsing `safe_cast` as above would cause the entire field to become null. A better approach is to drop the offending decimal place, then cast to integer:\\n\\n```sql\\ncast(replace({{ payment_type }},'.0','') as integer)\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/054_0c2badaa56_bad-int64-value-2010-error.md'},\n",
       " {'id': '583d28267f',\n",
       "  'question': 'Bad int64 value: 1.0 error (again)',\n",
       "  'sort_order': 55,\n",
       "  'content': \"I found that there are more columns causing the bad INT64: `ratecodeid` and `trip_type` on `Green_tripdata` table. You can use the queries below to address them:\\n\\n```sql\\nCAST(\\n    REGEXP_REPLACE(CAST(rate_code AS STRING), r'\\\\.0', '') AS INT64\\n) AS ratecodeid,\\n\\nCAST(\\n    CASE\\n        WHEN REGEXP_CONTAINS(CAST(trip_type AS STRING), r'\\\\.\\\\d+') THEN NULL\\n        ELSE CAST(trip_type AS INT64)\\n    END AS INT64\\n) AS trip_type\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/055_583d28267f_bad-int64-value-10-error-again.md'},\n",
       " {'id': '0fbc18e0f2',\n",
       "  'question': \"DBT: Error on building fact_trips.sql - Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64.\",\n",
       "  'sort_order': 56,\n",
       "  'content': 'To resolve the error regarding the \\'ehail_fee\\' column type mismatch, you can use the following line in `stg_green_trips.sql` to replace the original `ehail_fee` line:\\n\\n```sql\\n{{ dbt.safe_cast(\\'ehail_fee\\', api.Column.translate_type(\"numeric\")) }} as ehail_fee,\\n```\\n\\nThis ensures that the column type is correctly converted to match the expected type.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/056_0fbc18e0f2_dbt-error-on-building-fact_tripssql-parquet-column.md'},\n",
       " {'id': '57767e983b',\n",
       "  'question': 'The - vars argument must be a YAML dictionary, but was of type str',\n",
       "  'sort_order': 57,\n",
       "  'content': \"Remember to add a space between the variable and the value. Otherwise, it won't be interpreted as a dictionary.\\n\\nCorrect usage:\\n\\n```bash\\ndbt run --var 'is_test_run: false'\\n```\\n\\nNot able to change Environment Type as it is greyed out and inaccessible. You don't need to change the environment type. If you are following the videos, you are creating a Production Deployment, so the only available option is the correct one.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/057_57767e983b_the-vars-argument-must-be-a-yaml-dictionary-but-wa.md'},\n",
       " {'id': 'ec6d89a462',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_ee3efac5.png'}],\n",
       "  'question': 'Access Denied: Table yellow_tripdata: User does not have permission to query table yellow_tripdata, or perhaps it does not exist in location US.',\n",
       "  'sort_order': 58,\n",
       "  'content': '<{IMAGE:image_1}>\\n\\n### Error Details\\n\\n```\\nDatabase Error in model stg_yellow_tripdata (models/staging/stg_yellow_tripdata.sql)\\n\\nAccess Denied: Table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata: User does not have permission to query table taxi-rides-ny-339813-412521:trips_data_all.yellow_tripdata, or perhaps it does not exist in location US.\\n\\ncompiled Code at target/run/taxi_rides_ny/models/staging/stg_yellow_tripdata.sql\\n```\\n\\n### Solution\\n\\n1. **Branch Verification**:\\n   - Ensure you are working on the correct branch. If not, switch to the appropriate branch.\\n   \\n2. **Schema Configuration**:\\n   - Edit the `04-analytics-engineering/taxi_rides_ny/models/staging/schema.yml` file.\\n   - Ensure the configuration is correct:\\n     \\n     ```yaml\\n     sources:\\n     - name: staging\\n       database: your_database_name\\n     ```\\n\\n3. **Custom Branch Setup in dbt Cloud**:\\n   - If the error persists, consider running the dbt job on a custom branch:\\n     \\n     - Navigate to the environment settings in dbt Cloud.\\n     - In General settings, select \"Only run on a custom branch\".\\n     - Enter the name of your custom branch (e.g., HW).\\n     - Click Save.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/058_ec6d89a462_access-denied-table-yellow_tripdata-user-does-not.md'},\n",
       " {'id': '8ab8329fea',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_403eb7c5.png'}],\n",
       "  'question': 'Could not parse the dbt project. Please check that the repository contains a valid dbt project.',\n",
       "  'sort_order': 59,\n",
       "  'content': 'Running the Environment on the master branch causes this error. You must activate the “Only run on a custom branch” checkbox and specify the branch you are working on when the Environment is set up.\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/059_8ab8329fea_could-not-parse-the-dbt-project-please-check-that.md'},\n",
       " {'id': 'ee3f7f89ba',\n",
       "  'question': 'Made change to your modeling files and commit to your development branch, but Job still runs on old file?',\n",
       "  'sort_order': 60,\n",
       "  'content': 'Switch to the main branch and make a pull request from the development branch. This will take you to GitHub. Approve the merge and rerun your job; it should work as planned now.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/060_ee3f7f89ba_made-change-to-your-modeling-files-and-commit-to-y.md'},\n",
       " {'id': 'dfd4ddbf39',\n",
       "  'question': 'Setup: I’ve set Github and Bigquery to dbt successfully. Why nothing showed in my Develop tab?',\n",
       "  'sort_order': 61,\n",
       "  'content': 'Before you can develop some data model on dbt, you should:\\n\\n1. **Create a Development Environment:** Ensure that your development environment is properly configured.\\n   \\n2. **Set Parameters:** Specify necessary parameters within the environment.\\n   \\nOnce the model has been developed, also create a deployment environment to create and run jobs.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/061_dfd4ddbf39_setup-ive-set-github-and-bigquery-to-dbt-successfu.md'},\n",
       " {'id': '5b7577406d',\n",
       "  'question': 'BigQuery returns an error when I try to run ‘dbt run’:',\n",
       "  'sort_order': 62,\n",
       "  'content': \"My taxi data was loaded into GCS with `etl_web_to_gcs.py`, which converts CSV data into Parquet. Then I placed raw data trips into external tables, and when I executed `dbt run`, I got an error message:\\n\\n```plaintext\\nParquet column 'passenger_count' has type INT64 which does not match the target cpp_type DOUBLE.\\n```\\n\\nThis is because several columns in the files have different data formats.\\n\\nTo resolve this error, I added the transformation:\\n\\n```python\\ndf[col] = df[col].astype('Int64')\\n```\\n\\nApply this transformation to the following columns:\\n\\n- `passenger_count`\\n- `payment_type`\\n- `RatecodeID`\\n- `VendorID`\\n- `trip_type`\\n\\nAfter making these changes, the process completed successfully.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/062_5b7577406d_bigquery-returns-an-error-when-i-try-to-run-dbt-ru.md'},\n",
       " {'id': '778635395f',\n",
       "  'question': \"DBT: Running `dbt run --models stg_green_tripdata --var 'is_test_run: false'` is not returning anything:\",\n",
       "  'sort_order': 63,\n",
       "  'content': 'Use the syntax below instead if the code in the tutorial is not working.\\n\\n```bash\\ndbt run --select stg_green_tripdata --vars \\'{\"is_test_run\": false}\\'\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/063_778635395f_dbt-running-dbt-run-models-stg_green_tripdata-var.md'},\n",
       " {'id': 'e5b8322bee',\n",
       "  'question': \"DBT: Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "  'sort_order': 64,\n",
       "  'content': \"Following dbt with [BigQuery on Docker readme.md](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/04-analytics-engineering/docker_setup/README.md), after running `docker-compose build` and `docker-compose run dbt-bq-dtc init`, you might encounter the error:\\n\\n```\\nModuleNotFoundError: No module named 'pytz'\\n```\\n\\nSolution:\\n\\nAdd the following line in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`:\\n\\n```bash\\nRUN python -m pip install --no-cache pytz\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/064_e5b8322bee_dbt-error-no-module-named-pytz-while-setting-up-db.md'},\n",
       " {'id': 'eda58f442a',\n",
       "  'question': 'VS Code: NoPermissions (FileSystemError): Error: EACCES: permission denied (linux)',\n",
       "  'sort_order': 65,\n",
       "  'content': 'If you encounter problems editing `dbt_project.yml` when using Docker after running `docker-compose run dbt-bq-dtc init`, to change the profile ‘taxi_rides_ny’ to \\'bq-dbt-workshop’, execute the following command:\\n\\n```bash\\nsudo chown -R username path\\n```\\n\\nIf you see the error \"DBT - Internal Error: Profile should not be None if loading is completed\" when running `dbt debug`, change the directory to the newly created subdirectory, such as the `taxi_rides_ny` directory, which contains the dbt project.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/065_eda58f442a_vs-code-nopermissions-filesystemerror-error-eacces.md'},\n",
       " {'id': '9179edaff3',\n",
       "  'question': 'Google Cloud BigQuery: Location Problems',\n",
       "  'sort_order': 66,\n",
       "  'content': 'When running a query on BigQuery, you might encounter the error: **\"This table is not on the specified location\"**.\\n\\nTo resolve this issue, consider the following steps:\\n\\n- **Check the Locations**: Ensure the locations of your bucket, datasets, and tables are consistent. They should all reside in the same location.\\n\\n- **Modify Query Settings**:\\n  1. Go to the query window.\\n  2. Select **More** -> **Query Settings**.\\n  3. Select the correct location.\\n\\n- **Verify Table Paths**: Double-check the paths in your query:\\n  1. Click on the table.\\n  2. Go to **Details**.\\n  3. Copy the correct path.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/066_9179edaff3_google-cloud-bigquery-location-problems.md'},\n",
       " {'id': 'dc51ef9830',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_c27ecb8e.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_8b6478c1.png'},\n",
       "   {'description': 'image #3',\n",
       "    'id': 'image_3',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_80992235.png'},\n",
       "   {'description': 'image #4',\n",
       "    'id': 'image_4',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_cd924928.png'}],\n",
       "  'question': 'DBT Deploy: This dbt Cloud run was cancelled because a valid dbt project was not found.',\n",
       "  'sort_order': 67,\n",
       "  'content': 'This issue occurs when the dbt project is moved to another directory in the repository or if you\\'re on a different branch than expected.\\n\\n**Solution:**\\n\\n1. Navigate to the projects window on dbt Cloud.\\n2. Go to Settings -> Edit.\\n3. Add the directory path where the dbt project is located. Ensure that this path matches your file explorer path. For example:\\n   \\n   ```\\n   /week5/taxi_rides_ny\\n   ```\\n\\n4. Check that there are no files waiting to be committed to GitHub if you’re running the job to deploy to PROD.\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>\\n\\n5. Ensure the PROD environment is set up to check the main branch, or the specified branch.\\n\\nIn the image below, the branch \"ella2024\" is set to be checked as \"production-ready\" by the \"freshness\" check mark in PROD environment settings. Each time a branch is merged into \"ella2024\" and a PR is triggered, the CI check job initiates. Note that merging and closing the PR must be done manually.\\n\\n<{IMAGE:image_3}>\\n\\n6. Set up the PROD custom branch (if not the default main) in the Environment setup screen.\\n\\n<{IMAGE:image_4}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/067_dc51ef9830_dbt-deploy-this-dbt-cloud-run-was-cancelled-becaus.md'},\n",
       " {'id': '5dd0cdae58',\n",
       "  'question': 'DBT Deploy + CI: Location Problems on BigQuery',\n",
       "  'sort_order': 68,\n",
       "  'content': \"When creating a pull request and running the CI, dbt creates a new schema on BigQuery. By default, this new schema is created with the 'US' location. If your datasets, schemas, and tables are in the 'EU', this will cause an error and the pull request will not be accepted. \\n\\nTo change the location to 'EU' in the connection to BigQuery from dbt, follow these steps:\\n\\n- Navigate to **Dbt -> Project -> Settings -> Connection BigQuery**\\n- Open **Optional Settings**\\n- Set **Location** to `EU`\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/068_5dd0cdae58_dbt-deploy-ci-location-problems-on-bigquery.md'},\n",
       " {'id': 'fee87c684a',\n",
       "  'question': 'DBT Deploy: Error When trying to run the dbt project on Prod',\n",
       "  'sort_order': 69,\n",
       "  'content': 'When running the dbt project on production, ensure the following steps:\\n\\n1. **Pull Request and Merge**\\n   - Make the pull request and merge the branch into the main.\\n\\n2. **Version Check**\\n   - Ensure you have the latest version if changes were made to the repository elsewhere.\\n\\n3. **Project File Accessibility**\\n   - Verify that the `dbt_project.yml` file is accessible to the project. If not, refer to the solution for the error: \"Dbt: This dbt Cloud run was cancelled because a valid dbt project was not found.\"\\n\\n4. **Dataset Consistency**\\n   - Confirm that the name assigned to the dataset on BigQuery matches the name specified in the production environment configuration on dbt Cloud.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/069_fee87c684a_dbt-deploy-error-when-trying-to-run-the-dbt-projec.md'},\n",
       " {'id': 'b209edb213',\n",
       "  'question': 'DBT: Error: “404 Not found: Dataset <dataset_name>:<dbt_schema_name> was not found in location EU” after building from stg_green_tripdata.sql',\n",
       "  'sort_order': 70,\n",
       "  'content': 'In the step in [this video](https://www.youtube.com/watch?v=ueVy2N54lyc&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=44) (DE Zoomcamp 4.3.1 - Build the First dbt Models), after creating `stg_green_tripdata.sql` and clicking `build`, I encountered an error saying the dataset was not found in location EU. The default location for dbt Bigquery is the US, so when generating the new Bigquery schema for dbt, unless specified, the schema locates in the US.\\n\\nSolution:\\n\\n- Ensure the location is set to `EU` when adding connection details:\\n  - Navigate to **Develop** -> **Configure Cloud CLI** -> **Projects** -> **taxi_rides_ny** -> (connection) **Bigquery** -> **Edit**\\n  - Under **Location (Optional)**, type `EU`\\n  - Save the changes.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/070_b209edb213_dbt-error-404-not-found-dataset-dataset_namedbt_sc.md'},\n",
       " {'id': '6783a9bddb',\n",
       "  'question': 'Homework: Ingesting FHV_20?? data',\n",
       "  'sort_order': 71,\n",
       "  'content': '### Issue\\n\\nIf you’re having problems loading the FHV_20?? data from the GitHub repo into GCS and then into BigQuery (input file not of type parquet), follow these steps:\\n\\n1. **Append URL Template**\\n   \\n   Add `?raw=true` to the `URL_TEMPLATE` link:\\n   \\n   ```python\\n   URL_TEMPLATE = URL_PREFIX + \"/fhv_tripdata_{{ execution_date.strftime(\\'%Y-%m\\') }}.parquet?raw=true\"\\n   ```\\n   \\n2. **Update URL Prefix**\\n   \\n   Ensure `URL_PREFIX` is set to the following value:\\n   \\n   ```\\n   URL_PREFIX = \"https://github.com/alexeygrigorev/datasets/blob/master/nyc-tlc/fhv\"\\n   ```\\n   \\n   It is critical to use this link with the keyword `blob`. If the link contains `tree`, replace it with `blob`. Everything else, including the `curl -sSLf` command, can remain unchanged.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/071_6783a9bddb_homework-ingesting-fhv_20-data.md'},\n",
       " {'id': 'b459de9135',\n",
       "  'question': 'Ingesting FHV: alternative with kestra',\n",
       "  'sort_order': 72,\n",
       "  'content': 'Add this task based on the previous ones:\\n\\n- id: if_fhv_taxi\\n  \\n  type: io.kestra.plugin.core.flow.If\\n  \\n  condition: \"{{inputs.taxi == \\'fhv\\'}}\"\\n  \\n  then:\\n  \\n  - id: bq_fhv_tripdata\\n  \\n    type: io.kestra.plugin.gcp.bigquery.Query\\n  \\n    sql: |\\n      \\n      ```sql\\n      CREATE TABLE IF NOT EXISTS `{{kv(\\'GCP_PROJECT_ID\\')}}.{{kv(\\'GCP_DATASET\\')}}.fhv_tripdata`\\n      \\n      (\\n      \\n      unique_row_id BYTES OPTIONS (description = \\'A unique identifier for the trip, generated by hashing key trip attributes.\\'),\\n      \\n      filename STRING OPTIONS (description = \\'The source filename from which the trip data was loaded.\\'),\\n      \\n      dispatching_base_num STRING,\\n      \\n      pickup_datetime TIMESTAMP,\\n      \\n      dropoff_datetime TIMESTAMP,\\n      \\n      PUlocationID NUMERIC,\\n      \\n      DOlocationID NUMERIC,\\n      \\n      SR_Flag STRING,\\n      \\n      Affiliated_base_number STRING\\n      \\n      )\\n      \\n      PARTITION BY DATE(pickup_datetime);\\n      ```  \\n\\n- id: bq_fhv_table_ext\\n  \\n  type: io.kestra.plugin.gcp.bigquery.Query\\n  \\n  sql: |\\n    \\n    ```sql\\n    CREATE OR REPLACE EXTERNAL TABLE `{{kv(\\'GCP_PROJECT_ID\\')}}.{{render(vars.table)}}_ext`\\n    \\n    (\\n    \\n    dispatching_base_num STRING,\\n    \\n    pickup_datetime TIMESTAMP,\\n    \\n    dropoff_datetime TIMESTAMP,\\n    \\n    PUlocationID NUMERIC,\\n    \\n    DOlocationID NUMERIC,\\n    \\n    SR_Flag STRING,\\n    \\n    Affiliated_base_number STRING\\n    \\n    )\\n    \\n    OPTIONS (\\n    \\n    format = \\'CSV\\',\\n    \\n    uris = [\\'{{render(vars.gcs_file)}}\\'],\\n    \\n    skip_leading_rows = 1,\\n    \\n    ignore_unknown_values = TRUE\\n    \\n    );\\n    ```\\n\\n- id: bq_fhv_table_tmp\\n  \\n  type: io.kestra.plugin.gcp.bigquery.Query\\n  \\n  sql: |\\n    \\n    ```sql\\n    CREATE OR REPLACE TABLE `{{kv(\\'GCP_PROJECT_ID\\')}}.{{render(vars.table)}}`\\n    \\n    AS\\n    \\n    SELECT\\n    \\n    MD5(CONCAT(\\n    \\n    COALESCE(CAST(pickup_datetime AS STRING), \"\"),\\n    \\n    COALESCE(CAST(dropoff_datetime AS STRING), \"\"),\\n    \\n    COALESCE(CAST(PUlocationID AS STRING), \"\"),\\n    \\n    COALESCE(CAST(DOLocationID AS STRING), \"\")\\n    \\n    )) AS unique_row_id,\\n    \\n    \"{{render(vars.file)}}\" AS filename,\\n    \\n    *\\n    \\n    FROM `{{kv(\\'GCP_PROJECT_ID\\')}}.{{render(vars.table)}}_ext`;\\n    ```\\n\\n- id: bq_fhv_merge\\n  \\n  type: io.kestra.plugin.gcp.bigquery.Query\\n  \\n  sql: |\\n    \\n    ```sql\\n    MERGE INTO `{{kv(\\'GCP_PROJECT_ID\\')}}.{{kv(\\'GCP_DATASET\\')}}.fhv_tripdata` T\\n    \\n    USING `{{kv(\\'GCP_PROJECT_ID\\')}}.{{render(vars.table)}}` S\\n    \\n    ON T.unique_row_id = S.unique_row_id\\n    \\n    WHEN NOT MATCHED THEN\\n    \\n    INSERT (unique_row_id, filename, dispatching_base_num, pickup_datetime, dropoff_datetime, PUlocationID, DOlocationID, SR_Flag, Affiliated_base_number)\\n    \\n    VALUES (S.unique_row_id, S.filename, S.dispatching_base_num, S.pickup_datetime, S.dropoff_datetime, S.PUlocationID, S.DOlocationID, S.SR_Flag, S.Affiliated_base_number);\\n    ```\\n\\nAdd a trigger too:\\n\\n- id: fhv_schedule\\n\\n  type: io.kestra.plugin.core.trigger.Schedule\\n\\n  cron: \"0 11 1 * *\"\\n\\n  inputs:\\n\\n  taxi: fhv\\n\\nAnd modify inputs:\\n\\n```yaml\\ninputs:\\n\\n- id: taxi\\n\\n  type: SELECT\\n\\n  displayName: Select taxi type\\n\\n  values: [yellow, green, fhv]\\n\\n  defaults: green\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/072_b459de9135_ingesting-fhv-alternative-with-kestra.md'},\n",
       " {'id': 'c549d24645',\n",
       "  'question': 'Homework: Ingesting NYC TLC Data',\n",
       "  'sort_order': 73,\n",
       "  'content': 'The easiest way to upload datasets from GitHub for the homework is by utilizing this script: [git_csv_to_gcs.py](https://github.com/inner-outer-space/de-zoomcamp-2024/blob/main/4-analytics-engineering/git_csv_to_gcs.py). It is similar to a script provided in `03-data-warehouse/extras/web_to_gcs.py`. <{IMAGE:image_id}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/073_c549d24645_homework-ingesting-nyc-tlc-data.md'},\n",
       " {'id': 'fb43c209d9',\n",
       "  'question': 'How to set environment variable easily for any credentials',\n",
       "  'sort_order': 74,\n",
       "  'content': 'If you need to securely set up credentials for a project and intend to push it to a git repository, using an environment variable is a recommended option.\\n\\nFor example, for scripts like `web_to_gcs.py` or `git_csv_to_gcs.py`, you may need to set these variables:\\n\\n- `GOOGLE_APPLICATION_CREDENTIALS`\\n- `GCP_GCS_BUCKET`\\n\\nThe easiest option to manage this is to use a `.env` file with [dotenv](https://pypi.org/project/python-dotenv/).\\n\\nTo install and utilize this package, follow these steps:\\n\\n1. Install `python-dotenv`:\\n\\n   ```bash\\n   pip install python-dotenv\\n   ```\\n\\n2. Add the following code to inject these variables into your project:\\n\\n   ```python\\n   from dotenv import load_dotenv\\n   import os\\n\\n   # Load environment variables from .env file\\n   load_dotenv()\\n\\n   # Now you can access environment variables like GCP_GCS_BUCKET and GOOGLE_APPLICATION_CREDENTIALS\\n   credentials_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\\n   BUCKET = os.environ.get(\"GCP_GCS_BUCKET\")\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/074_fb43c209d9_how-to-set-environment-variable-easily-for-any-cre.md'},\n",
       " {'id': '8c9b0fdaa5',\n",
       "  'question': \"Invalid date types after Ingesting FHV data through CSV files: Could not parse 'pickup_datetime' as a timestamp\",\n",
       "  'sort_order': 75,\n",
       "  'content': \"If you uploaded manually the FHV 2019 CSV files, you may face errors regarding date types. Try to create an external table in BigQuery but define the `pickup_datetime` and `dropoff_datetime` to be strings:\\n\\n```sql\\nCREATE OR REPLACE EXTERNAL TABLE `gcp_project.trips_data_all.fhv_tripdata`  (\\n\\n    dispatching_base_num STRING,\\n\\n    pickup_datetime STRING,\\n\\n    dropoff_datetime STRING,\\n\\n    PUlocationID STRING,\\n\\n    DOlocationID STRING,\\n\\n    SR_Flag STRING,\\n\\n    Affiliated_base_number STRING\\n\\n)\\n\\nOPTIONS (\\n\\n    format = 'csv',\\n\\n    uris = ['gs://bucket/*.csv']\\n\\n);\\n```\\n\\nThen, when creating the FHV core model in dbt, use `TIMESTAMP(CAST(())` to ensure it first parses as a string and then converts it to a timestamp:\\n\\n```sql\\nWITH fhv_tripdata AS (\\n\\n    SELECT * FROM {{ ref('stg_fhv_tripdata') }}\\n\\n),\\n\\ndim_zones AS (\\n\\n    SELECT * FROM {{ ref('dim_zones') }}\\n\\n    WHERE borough != 'Unknown'\\n\\n)\\n\\nSELECT fhv_tripdata.dispatching_base_num,\\n\\n    TIMESTAMP(CAST(fhv_tripdata.pickup_datetime AS STRING)) AS pickup_datetime,\\n\\n    TIMESTAMP(CAST(fhv_tripdata.dropoff_datetime AS STRING)) AS dropoff_datetime,\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/075_8c9b0fdaa5_invalid-date-types-after-ingesting-fhv-data-throug.md'},\n",
       " {'id': '05aad03ef3',\n",
       "  'question': 'Invalid data types after Ingesting FHV data through parquet files: Could not parse SR_Flag as Float64, Couldn’t parse datetime column as timestamp, couldn’t handle NULL values in PULocationID, DOLocationID',\n",
       "  'sort_order': 76,\n",
       "  'content': \"If you uploaded the FHV 2019 parquet files manually after downloading from [this source](https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2019-*.parquet), you may face errors regarding data types while loading the data into a landing table (e.g., `fhv_tripdata`). To avoid these errors, create an external table with the schema defined as follows and load each month in a loop:\\n\\n```sql\\nCREATE OR REPLACE EXTERNAL TABLE `dw-bigquery-week-3.trips_data_all.external_tlc_fhv_trips_2019` (\\n\\n  dispatching_base_num STRING,\\n\\n  pickup_datetime TIMESTAMP,\\n\\n  dropoff_datetime TIMESTAMP,\\n\\n  PUlocationID FLOAT64,\\n\\n  DOlocationID FLOAT64,\\n\\n  SR_Flag FLOAT64,\\n\\n  Affiliated_base_number STRING\\n\\n)\\n\\nOPTIONS (\\n\\n  format = 'PARQUET',\\n\\n  uris = ['gs://project id/fhv_2019_8.parquet']\\n\\n);\\n```\\n\\nYou can also use:\\n\\n```sql\\nuris = ['gs://project id/fhv_2019_*.parquet']\\n```\\n\\nThis approach removes the need for a loop and allows you to process all months in a single run.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/076_05aad03ef3_invalid-data-types-after-ingesting-fhv-data-throug.md'},\n",
       " {'id': 'faddbcb675',\n",
       "  'question': 'Join Error on LocationID: \"Unable to find common supertype for templated argument\"',\n",
       "  'sort_order': 77,\n",
       "  'content': 'No matching signature for operator `=` for argument types: `STRING`, `INT64`\\n\\n**Signature**: `T1 = T1`\\n\\n**Error:** Unable to find common supertype for templated argument.\\n\\n**Solution:**\\n\\nMake sure the `LocationID` field is of the same type in both tables. If it is in string format in one table, use the following dbt code to convert it to an integer:\\n\\n```sql\\n{{ dbt.safe_cast(\"PULocationID\", api.Column.translate_type(\"integer\")) }} as pickup_locationid\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/077_faddbcb675_join-error-on-locationid-unable-to-find-common-sup.md'},\n",
       " {'id': '17be084780',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_2c1e7560.png'}],\n",
       "  'question': 'Google Looker Studio: you have used up your 30-day trial',\n",
       "  'sort_order': 78,\n",
       "  'content': 'When accessing Looker Studio through the Google Cloud Project console, you may be prompted to subscribe to the Pro version and receive the following errors:\\n\\n<{IMAGE:image_1}>\\n\\nInstead, navigate to [https://lookerstudio.google.com/navigation/reporting](https://lookerstudio.google.com/navigation/reporting) which will take you to the free version.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/078_17be084780_google-looker-studio-you-have-used-up-your-30-day.md'},\n",
       " {'id': '78fda262c6',\n",
       "  'question': 'How does dbt handle dependencies between models?',\n",
       "  'sort_order': 79,\n",
       "  'content': 'Dbt provides a mechanism called `ref` to manage dependencies between models. By referencing other models using the `ref` keyword in SQL, dbt automatically understands the dependencies and ensures the correct execution order.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/079_78fda262c6_how-does-dbt-handle-dependencies-between-models.md'},\n",
       " {'id': '41463c387b',\n",
       "  'question': 'Loading FHV Data goes into slumber using Mage?',\n",
       "  'sort_order': 80,\n",
       "  'content': 'Try loading the data using Jupyter notebooks in a local environment. There might be bandwidth issues with Mage.\\n\\nLoad the data into a pandas DataFrame using the URLs, make necessary transformations, upload to the GCP bucket, or alternatively download the Parquet/CSV files locally and then upload to GCP manually.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/080_41463c387b_loading-fhv-data-goes-into-slumber-using-mage.md'},\n",
       " {'id': '30da3ec9c5',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_6e795821.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_522c20d6.png'}],\n",
       "  'question': 'Region Mismatch in DBT and BigQuery',\n",
       "  'sort_order': 81,\n",
       "  'content': 'If you are using the datasets copied into BigQuery from BigQuery public datasets, the region will be set as `US` by default. It is much easier to set your dbt profile location as `US` while transforming the tables and views. You can change the location as follows:\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/081_30da3ec9c5_region-mismatch-in-dbt-and-bigquery.md'},\n",
       " {'id': 'a250f11737',\n",
       "  'question': 'What is the fastest way to upload taxi data to dbt-postgres?',\n",
       "  'sort_order': 82,\n",
       "  'content': \"Use the PostgreSQL `COPY FROM` feature, which is compatible with CSV files.\\n\\n### Steps:\\n\\n1. **Create the Table**\\n   \\n   First, create your table (example):\\n   \\n   ```sql\\n   CREATE TABLE taxis (\\n   \\n   …\\n   \\n   );\\n   ```\\n\\n2. **Use COPY Functionality**\\n\\n   Use the `COPY` command (example):\\n\\n   ```sql\\n   COPY taxis FROM PROGRAM\\n   'url'\\n   WITH (\\n   FORMAT csv,\\n   HEADER true,\\n   ENCODING utf8\\n   );\\n   ```\\n\\n   - Syntax for `COPY`:\\n\\n   ```sql\\n   COPY table_name [ ( column_name [, ...] ) ]\\n   FROM { 'filename' | PROGRAM 'command' | STDIN }\\n   [ [ WITH ] ( option [, ...] ) ]\\n   [ WHERE condition ]\\n   ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/082_a250f11737_what-is-the-fastest-way-to-upload-taxi-data-to-dbt.md'},\n",
       " {'id': '49e1ec9d84',\n",
       "  'question': 'dbt: Where should we create `profiles.yml`?',\n",
       "  'sort_order': 83,\n",
       "  'content': \"For local environments using dbt-core, the profile configuration is valid for all projects. Note: dbt Cloud doesn’t require it.\\n\\nThe `~/.dbt/profiles.yml` file should be located in your user's home directory. On Windows, this would typically be:\\n\\n```\\nC:\\\\Users\\\\<YourUsername>\\\\.dbt\\\\profiles.yml\\n```\\n\\nReplace `<YourUsername>` with your actual Windows username. This file is used by dbt to store connection profiles for different projects.\\n\\nHere's how you can create the `profiles.yml` file in the appropriate directory:\\n\\n1. Open File Explorer and navigate to `C:\\\\Users\\\\<YourUsername>\\\\`.\\n2. Create a new folder named `.dbt` if it doesn't already exist.\\n3. Inside the `.dbt` folder, create a new file named `profiles.yml`.\\n\\nUsage example can be found [here](https://gist.github.com/pizofreude/ff4d0601f1eb353683d8af8f4b5aac27?permalink_comment_id=5457712#gistcomment-5457712).\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/083_49e1ec9d84_dbt-where-should-we-create-profilesyml.md'},\n",
       " {'id': 'a8e992c143',\n",
       "  'question': 'dbt: Are there UI options for dbt Core like dbt Cloud?',\n",
       "  'sort_order': 84,\n",
       "  'content': \"While dbt Core does not have an official UI like dbt Cloud, there are several tools available that provide UI functionality:\\n\\n- **Altimate's VS Code Extension**: \\n  - Use the [VS Code dbt Power User extension](https://github.com/AltimateAI/vscode-dbt-power-user).\\n  - Sign up for the community plan for free usage at [Altimate](https://app.myaltimate.com/register) and add the API into your VS Code extension.\\n\\n- **VSCode Snippets for dbt and Jinja**: \\n  - Access the snippets package for SQL, YAML, and Markdown [here](https://github.com/bastienboutonnet/vscode-dbt).\\n\\n- **Monitoring with Elementary**:\\n  - Monitor dbt projects using [Elementary](https://github.com/elementary-data/elementary).\\n  - Learn more about its setup and features in this [Medium article](https://medium.com/@srinivas.dataengineer/supercharge-your-dbt-monitoring-with-elementary-data-0fac140a6f60).\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/084_a8e992c143_dbt-are-there-ui-options-for-dbt-core-like-dbt-clo.md'},\n",
       " {'id': '5d92a0583a',\n",
       "  'question': 'When configuring the profiles.yml file for dbt-postgres with jinja templates with environment variables, I\\'m getting \"Credentials in profile \\\\\"PROFILE_NAME\\\\\", target: \\'dev\\', invalid: \\'5432\\' is not of type \\'integer\\'\"',\n",
       "  'sort_order': 85,\n",
       "  'content': \"Make sure that the port number is set as an integer in your `profiles.yml` file. Environment variables are usually strings, so you need to explicitly convert them to integers in Jinja. Update the line that sets the port with something like:\\n\\n```yaml\\nport: {{ env_var('DB_PORT') | int }}\\n```\\n\\nThis will ensure that the value is treated as an integer.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/085_5d92a0583a_when-configuring-the-profilesyml-file-for-dbt-post.md'},\n",
       " {'id': '23687d0f93',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_48537290.png'}],\n",
       "  'question': 'DBT: The database is correct but I get Error with Incorrect Schema in Models',\n",
       "  'sort_order': 86,\n",
       "  'content': 'What to do if your dbt model fails with an error similar to:\\n\\n```\\nDBT-CORE\\n```\\n\\nCheck **profiles.yml**:\\n\\n- Ensure your `profiles.yml` file is correctly configured with the correct schema and database under your target. This file is typically located in `~/.dbt/`.\\n\\nExample configuration:\\n\\n**DBT-CLOUD-IDE**\\n\\nCheck Credentials in dbt Cloud UI:\\n\\n- Navigate to the Credentials section in the dbt Cloud project settings.\\n- Ensure the correct database and schema are set (e.g., ‘my_dataset’).\\n\\n<{IMAGE:image_1}>\\n\\nVerify Environment Settings:\\n\\n- Double-check that you are working in the correct environment (dev, prod, etc.), as dbt Cloud allows different settings for different environments.\\n\\n**No Need for profiles.yml**:\\n\\n- In dbt Cloud, you don’t need to configure `profiles.yml` manually. All connection settings are handled via the UI.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/086_23687d0f93_dbt-the-database-is-correct-but-i-get-error-with-i.md'},\n",
       " {'id': '7cec1857a9',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_789bb06f.png'}],\n",
       "  'question': 'DBT: DBT allows only 1 project in free developer version.',\n",
       "  'sort_order': 87,\n",
       "  'content': 'Yes, DBT allows only 1 project under one account. But you can create multiple accounts as shown below:\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-4/087_7cec1857a9_dbt-dbt-allows-only-1-project-in-free-developer-ve.md'},\n",
       " {'id': '98b6a15ece',\n",
       "  'question': 'Documentation or book sign not shown even after doing `dbt docs generate`.',\n",
       "  'sort_order': 1,\n",
       "  'content': \"In the free version, it does not show the docs when models are run in the development environment. Create a production job and tick the 'generate docs' section. Execute it, and it will generate the documentation.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/001_98b6a15ece_documentation-or-book-sign-not-shown-even-after-do.md'},\n",
       " {'id': 'd327cff06d',\n",
       "  'question': 'Setting up Java and Spark (with PySpark) on Linux (Alternative option using SDKMAN):',\n",
       "  'sort_order': 2,\n",
       "  'content': '1. **Install SDKMAN:**\\n\\n   ```bash\\n   curl -s \"https://get.sdkman.io\" | bash\\n   \\n   source \"$HOME/.sdkman/bin/sdkman-init.sh\"\\n   ```\\n\\n2. **Using SDKMAN, install Java 11 and Spark 3.3.2:**\\n\\n   ```bash\\n   sdk install java 11.0.22-tem\\n   \\n   sdk install spark 3.3.2\\n   ```\\n\\n3. **Open a new terminal or run the following in the same shell:**\\n\\n   ```bash\\n   source \"$HOME/.sdkman/bin/sdkman-init.sh\"\\n   ```\\n\\n4. **Verify the locations and versions of Java and Spark that were installed:**\\n\\n   ```bash\\n   echo $JAVA_HOME\\n   \\n   java -version\\n   \\n   echo $SPARK_HOME\\n   \\n   spark-submit --version\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/002_d327cff06d_setting-up-java-and-spark-with-pyspark-on-linux-al.md'},\n",
       " {'id': '83b9009569',\n",
       "  'question': 'PySpark: Setting Spark up in Google Colab',\n",
       "  'sort_order': 3,\n",
       "  'content': 'If you’re struggling to set things up \"locally\" (meaning non-managed environments like your laptop, a VM, or Codespaces), you can follow this guide to use Spark in Google Colab:\\n\\n[Launch Spark on Google Colab and Connect to SparkUI](https://medium.com/gitconnected/launch-spark-on-google-colab-and-connect-to-sparkui-342cad19b304)\\n\\nStarter notebook:\\n\\n[GitHub Repository - Spark in Colab](https://github.com/aaalexlit/medium_articles/blob/main/Spark_in_Colab.ipynb)\\n\\nIt’s advisable to spend some time setting up locally rather than using this solution immediately.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/003_83b9009569_pyspark-setting-spark-up-in-google-colab.md'},\n",
       " {'id': 'a3790900cc',\n",
       "  'question': 'Spark-shell: unable to load native-hadoop library for platform - Windows',\n",
       "  'sort_order': 4,\n",
       "  'content': 'If after installing Java (either JDK or OpenJDK), Hadoop, and Spark, and setting the corresponding environment variables, you encounter the following error when running `spark-shell` in CMD:\\n\\n```java\\njava.lang.IllegalAccessError: class org.apache.spark.storage.StorageUtils$ (in unnamed module @0x3c947bc5) cannot access class sun.nio.ch.DirectBuffer (in module java.base) because module java.base does not export sun.nio.ch to unnamed module @0x3c947bc5\\n```\\n\\nSolution:\\n- Java 17 or 19 is not supported by Spark. \\n- Spark 3.x requires Java 8, 11, or 16.\\n- Install Java 11 from the website provided in the windows.md setup file.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/004_a3790900cc_spark-shell-unable-to-load-native-hadoop-library-f.md'},\n",
       " {'id': '59ad389756',\n",
       "  'question': 'PySpark: Python was not found; run without arguments to install from the Microsoft Store, or disable this shortcut from Settings > Manage App Execution Aliases.',\n",
       "  'sort_order': 5,\n",
       "  'content': 'I encountered this error while executing a user-defined function in Spark (`crazy_stuff_udf`). I am working on Windows and using conda. After following the setup instructions, I discovered that the `PYSPARK_PYTHON` environment variable was not set correctly, as conda has different Python paths for each environment.\\n\\n**Solution:**\\n\\n1. Run the following command inside the appropriate environment:\\n   \\n   ```bash\\n   pip install findspark\\n   ```\\n\\n2. Add the following to the top of your script:\\n   \\n   ```python\\n   import findspark\\n   \\n   findspark.init()\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/005_59ad389756_pyspark-python-was-not-found-run-without-arguments.md'},\n",
       " {'id': 'd5d38534f6',\n",
       "  'question': 'PySpark: TypeError: code() argument 13 must be str, not int, while executing `import pyspark`  (Windows/ Spark 3.0.3 - Python 3.11)',\n",
       "  'sort_order': 6,\n",
       "  'content': 'This error occurs because Python 3.11 has some inconsistencies with the older Spark 3.0.3 version.\\n\\n### Solutions:\\n\\n1. **Downgrade Python Version:**\\n   - Use Python 3.9. A conda environment can help in managing different Python versions.\\n\\n2. **Upgrade PySpark Version:**\\n   - Install a newer PySpark version, such as 3.5.1 or above, which is compatible with Python 3.11.\\n\\n```bash\\n# Example commands\\nconda create -n pyspark_env python=3.9\\nconda activate pyspark_env\\npip install pyspark==3.5.1\\n```\\n\\nEnsure that your environment is set up correctly to avoid version mismatches.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/006_d5d38534f6_pyspark-typeerror-code-argument-13-must-be-str-not.md'},\n",
       " {'id': '84f10086ab',\n",
       "  'question': 'Import pyspark - Error: No Module named ‘pyspark’',\n",
       "  'sort_order': 7,\n",
       "  'content': 'Ensure that your `PYTHONPATH` is set correctly to include the PySpark library. You can check if PySpark is pointing to the correct location by running:\\n\\n```python\\nimport pyspark\\n\\nprint(pyspark.__file__)\\n```\\n\\nIt should point to the location where PySpark is installed (e.g., `/home/<your username>/spark/spark-3.x.x-bin-hadoop3.x/python/pyspark/__init__.py`).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/007_84f10086ab_import-pyspark-error-no-module-named-pyspark.md'},\n",
       " {'id': '14b039801e',\n",
       "  'question': 'Cannot find Spark jobs UI at localhost',\n",
       "  'sort_order': 8,\n",
       "  'content': 'This is because the current port is in use, so the Spark UI will run on a different port. You can check which port Spark is using by running this command:\\n\\n```\\nspark.sparkContext.uiWebUrl\\n```\\n\\nIf it indicates a different port, you should access that specific port instead. Additionally, ensure that there are no other notebooks or processes that might be using the same port. Clean up unused resources to avoid port conflicts.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/008_14b039801e_cannot-find-spark-jobs-ui-at-localhost.md'},\n",
       " {'id': '0de998a04e',\n",
       "  'question': 'Java+Spark - Easy setup with miniconda env (worked on MacOS)',\n",
       "  'sort_order': 9,\n",
       "  'content': 'If you want to manage all Python dependencies within the same virtual environment (e.g., conda), along with Java and Spark, follow these steps:\\n\\n1. **Install OpenJDK 11:**\\n   \\n   On MacOS, run:\\n   \\n   ```bash\\n   brew install java11\\n   ```\\n\\n   Add the following line to your `~/.bashrc` or `~/zshrc`:\\n   \\n   ```bash\\n   export PATH=\"/opt/homebrew/opt/openjdk@11/bin:$PATH\"\\n   ```\\n\\n2. **Activate Your Environment:**\\n\\n   Use your preferred tool (pipenv, poetry, or conda) to activate your working environment.\\n\\n3. **Install PySpark:**\\n\\n   Run:\\n   \\n   ```bash\\n   pip install pyspark\\n   ```\\n\\n4. **Proceed with Exercises:**\\n\\n   Work with your Spark exercises as usual. All default Spark commands will be available in the shell session under the activated environment.\\n\\n   Note: You don’t need `findspark` for initialization.\\n\\n5. **Resolving Py4J Errors:**\\n\\n   If you encounter an error like:\\n   \\n   ```plaintext\\n   Py4JJavaError: An error occurred while calling (...)  java.net.ConnectException: Connection refused: no further information;\\n   ```\\n\\n   Ensure you\\'re using compatible versions of JDK or Python with Spark. Spark 3.5.0 supports JDK 8/11/17 and Python 3.8+.\\n\\n   Use SDKMan! to install:\\n   \\n   ```bash\\n   sdk install java 17.0.10-librca\\n   sdk install spark 3.5.0\\n   sdk install hadoop 3.3.5\\n   ```\\n\\n   Create and activate a conda environment with:\\n\\n   ```bash\\n   conda create -n ENV_NAME python=3.11\\n   conda activate ENV_NAME\\n   pip install pyspark==3.5.0\\n   ```\\n\\n6. **Windows Py4J Setup:**\\n\\n   Ensure correct PATH settings in your `~/.bashrc`:\\n\\n   ```bash\\n   export JAVA_HOME=\"/c/tools/jdk-11.0.21\"\\n   export PATH=\"${JAVA_HOME}/bin:${PATH}\"\\n   export HADOOP_HOME=\"/c/tools/hadoop-3.2.0\"\\n   export PATH=\"${HADOOP_HOME}/bin:${PATH}\"\\n   export SPARK_HOME=\"/c/tools/spark-3.3.2-bin-hadoop3\"\\n   export PATH=\"${SPARK_HOME}/bin:${PATH}\"\\n   export PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\n   \\n   export PYTHONPATH=\"${SPARK_HOME}spark-3.5.1-bin-hadoop3py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\n   ```\\n\\n   Download `winutils` from [Stephenlaye2/winutils3.3.0](https://github.com/Stephenlaye2/winutils3.3.0) and place them in `/c/tools/hadoop-3.2.0/bin`.\\n\\n   Check out this video for a solution: [How To Resolve Issue with Writing DataFrame to Local File](https://www.youtube.com/watch?v=yFem0Pu0gC8)\\n\\n   Restart your IDE and computer to apply the changes. Be aware that fixing one error might result in new ones like `o31.parquet`. Address these as needed.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/009_0de998a04e_javaspark-easy-setup-with-miniconda-env-worked-on.md'},\n",
       " {'id': '0f8512fc2a',\n",
       "  'question': 'Spark - Installation Error Code 1603',\n",
       "  'sort_order': 10,\n",
       "  'content': \"**Issue:** Spark installation on Windows completed but failed to run.\\n\\nThis is a common Windows Installer error code indicating that there was a fatal error during installation. It often occurs due to issues like insufficient permissions, conflicts with other software, or problems with the installer package.\\n\\n### Step to Solve the Issue:\\n\\n#### Installing Chocolatey\\n\\nChocolatey is a package manager for Windows, which makes it easy to install, update, and manage software.\\n\\n**Installation Steps**\\n\\n1. **Open PowerShell as an Administrator**\\n   - Press `Win + X` and select **Windows PowerShell (Admin)** or search for PowerShell, right-click, and select **Run as administrator**.\\n\\n2. **Run the following command to install Chocolatey:**\\n   \\n   ```bash\\n   Set-ExecutionPolicy Bypass -Scope Process -Force; \\\\\\n   [System.Net.ServicePointManager]::SecurityProtocol = [System.Net.ServicePointManager]::SecurityProtocol -bor 3072; \\\\\\n   iex ((New-Object System.Net.WebClient).DownloadString('https://community.chocolatey.org/install.ps1'))\\n   ```\\n\\n3. **Verify the installation**\\n   - Close and reopen PowerShell as an administrator and run:\\n   \\n   ```bash\\n   choco -v\\n   ```\\n   - You should see the Chocolatey version number indicating that it has been installed successfully.\\n\\n4. **Command for Global Acceptance**\\n   - To globally accept all licenses for all packages installed using Chocolatey, run the following command:\\n   \\n   ```bash\\n   choco feature enable -n allowGlobalConfirmation\\n   ```\\n   - This command configures Chocolatey to automatically accept license agreements for all packages, streamlining the installation process and avoiding prompts for each package.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/010_0f8512fc2a_spark-installation-error-code-1603.md'},\n",
       " {'id': 'ce188e5db2',\n",
       "  'question': 'RuntimeError: Java gateway process exited before sending its port number',\n",
       "  'sort_order': 11,\n",
       "  'content': 'After installing everything, including PySpark, when running this script in a Jupyter Notebook:\\n\\n```python\\nimport pyspark\\nfrom pyspark.sql import SparkSession\\n\\nspark = SparkSession.builder \\\\\\n    .master(\"local[*]\") \\\\\\n    .appName(\\'test\\') \\\\\\n    .getOrCreate()\\n\\n# Read the CSV file\\ndf = spark.read \\\\\\n    .option(\"header\", \"true\") \\\\\\n    .csv(\\'taxi+_zone_lookup.csv\\')\\n\\ndf.show()\\n```\\n\\nYou might encounter the error:\\n\\n```\\nRuntimeError: Java gateway process exited before sending its port number\\n```\\n\\n### Solutions\\n\\n- **Solution 1:**\\n  1. Install `findspark` by running:\\n     \\n     ```bash\\n     pip install findspark\\n     ```\\n  2. Add the following lines to the top of your script:\\n\\n     ```python\\n     import findspark\\n     findspark.init()\\n     ```\\n\\n- **Solution 2:**\\n  1. Ensure that PySpark points to the correct location. Run:\\n     \\n     ```python\\n     pyspark.__file__\\n     ```\\n     \\n     It should list a path like `/home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py` if you followed the setup correctly.\\n  2. If it points to your Python site-packages, remove the PySpark directory there.\\n  3. Verify `.bashrc` for correct exports, ensuring no conflicting variables are present.\\n\\n- **Alternative Solution:**\\n  - Set environment variables permanently at the system and user levels by following a tutorial.\\n\\n     Once installed, skip to 7:14 in the tutorial to help set up environment variables.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/011_ce188e5db2_runtimeerror-java-gateway-process-exited-before-se.md'},\n",
       " {'id': '675d60dadd',\n",
       "  'question': 'Module Not Found Error in Jupyter Notebook.',\n",
       "  'sort_order': 12,\n",
       "  'content': 'Even after installing `pyspark` correctly on a Linux machine (VM) as instructed in the course, a module not found error was encountered in the Jupyter Notebook.\\n\\nThe solution that worked:\\n\\n1. Run the following in the Jupyter Notebook:\\n\\n   ```bash\\n   !pip install findspark\\n   ```\\n\\n2. Import and initialize `findspark`:\\n\\n   ```python\\n   import findspark\\n   findspark.init()\\n   ```\\n\\n3. Thereafter, import `pyspark` and create the Spark context as usual.\\n\\nIf the above solution does not work, try:\\n\\n- Using `!pip3 install pyspark` instead of `!pip install pyspark`.\\n\\nTo filter based on conditions across multiple columns:\\n\\n```python\\nfrom pyspark.sql.functions import col\\n\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/012_675d60dadd_module-not-found-error-in-jupyter-notebook.md'},\n",
       " {'id': 'd87bb0e38f',\n",
       "  'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j' while executing `import pyspark`\",\n",
       "  'sort_order': 13,\n",
       "  'content': 'To resolve the `ModuleNotFoundError: No module named \\'py4j\\'` when executing `import pyspark`, follow these steps:\\n\\n1. **Check for Py4J File Version:**\\n   - Run the command:\\n     ```bash\\n     ls ${SPARK_HOME}/python/lib/\\n     ```\\n   - Note the version of the `py4j` file.\\n\\n2. **Update the `PYTHONPATH`:**\\n   - Use the version identified above to update the `PYTHONPATH` accordingly. For example:\\n     ```bash\\n     export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"\\n     ```\\n   - Ensure that the version matches the filename under `${SPARK_HOME}/python/lib/`.\\n\\n3. **Verify Spark\\'s Py4J Version:**\\n   - Check the Py4J version of the Spark you\\'re using from the [Apache Spark documentation](https://spark.apache.org/docs/latest/api/python/getting_started/install.html).\\n\\n4. **Install Py4J (if needed):**\\n   - If the above steps do not resolve the issue, run:\\n     ```bash\\n     pip install py4j\\n     ```\\n   - This can resolve version mismatches or missing installations.\\n\\nThis should address the `ModuleNotFoundError` for Py4J when using PySpark. Ensure all versions are consistent and correct.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/013_d87bb0e38f_py4jjavaerror-modulenotfounderror-no-module-named.md'},\n",
       " {'id': '590e5d8c20',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_be1c5cff.png'}],\n",
       "  'question': \"Py4J Error - ModuleNotFoundError: No module named 'py4j' (Solve with latest version)\",\n",
       "  'sort_order': 14,\n",
       "  'content': 'To resolve the Py4J error, follow these steps:\\n\\n1. Install the latest available Py4J version using conda:\\n   \\n   ```bash\\n   conda install -c conda-forge py4j\\n   ```\\n   \\n   Make sure to replace with the latest version number as found on the website.\\n\\n   <{IMAGE:image_1}>\\n\\n2. Add the following lines to your `.bashrc` file:\\n   \\n   ```bash\\n   export PYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\n   export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.7-src.zip:$PYTHONPATH\"\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/014_590e5d8c20_py4j-error-modulenotfounderror-no-module-named-py4.md'},\n",
       " {'id': 'e269cf9e38',\n",
       "  'question': 'Exception: Jupyter command `jupyter-notebook` not found.',\n",
       "  'sort_order': 15,\n",
       "  'content': 'Even after exporting paths correctly, you may find that Jupyter is installed but `jupyter-notebook` is not found. Follow these steps to resolve the issue:\\n\\nFull steps:\\n\\n1. **Update and Upgrade Packages:**\\n   ```bash\\n   sudo apt update && sudo apt -y upgrade\\n   ```\\n\\n2. **Install Python:**\\n   ```bash\\n   sudo apt install python3-pip python3-dev\\n   ```\\n\\n3. **Install Python Virtualenv:**\\n   ```bash\\n   sudo -H pip3 install --upgrade pip\\n   sudo -H pip3 install virtualenv\\n   ```\\n\\n4. **Create a Python Virtual Environment:**\\n   ```bash\\n   mkdir notebook\\n   cd notebook\\n   virtualenv jupyterenv\\n   source jupyterenv/bin/activate\\n   ```\\n\\n5. **Install Jupyter Notebook:**\\n   ```bash\\n   pip install jupyter\\n   ```\\n\\n6. **Run Jupyter Notebook:**\\n   ```bash\\n   jupyter notebook\\n   ```\\n   \\nFor full instructions, refer to [this guide](https://learningdataengineering540969211.wordpress.com/2022/02/24/week-5-de-zoomcamp-5-2-1-installing-spark-on-linux/) or the original instructions [here](https://speedysense.com/install-jupyter-notebook-on-ubuntu-20-04/).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/015_e269cf9e38_exception-jupyter-command-jupyter-notebook-not-fou.md'},\n",
       " {'id': '6cab9c338e',\n",
       "  'question': 'Following 5.2.1, I am getting an error - Head:cannot open ‘taxi+_zone_lookup.csv’ for reading: No such file or directory',\n",
       "  'sort_order': 16,\n",
       "  'content': 'The latest filename is just `taxi_zone_lookup.csv`, so it should work after removing the `+` now.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/016_6cab9c338e_following-521-i-am-getting-an-error-headcannot-ope.md'},\n",
       " {'id': 'ef01c39d63',\n",
       "  'question': 'Error: java.io.FileNotFoundException',\n",
       "  'sort_order': 17,\n",
       "  'content': '```python\\n# Code executed:\\ndf = spark.read.parquet(pq_path)\\n\\n# … some operations on df …\\n\\ndf.write.parquet(pq_path, mode=\"overwrite\")\\n\\n# Error:\\n# java.io.FileNotFoundException: File file:/home/xxx/code/data/pq/fhvhv/2021/02/part-00021-523f9ad5-14af-4332-9434-bdcb0831f2b7-c000.snappy.parquet does not exist\\n```\\n\\nThe problem is that Spark performs lazy transformations, so the actual action that triggers the job is `df.write`, which deletes the parquet files it tries to read (mode=\"overwrite\").\\n\\n**Solution:**\\n\\n- Write to a different directory:\\n\\n  ```python\\n  df.write.parquet(pq_path_temp, mode=\"overwrite\")\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/017_ef01c39d63_error-javaiofilenotfoundexception.md'},\n",
       " {'id': 'cbf0755be8',\n",
       "  'question': 'Hadoop: FileNotFoundException: Hadoop bin directory does not exist, when trying to write (Windows)',\n",
       "  'sort_order': 18,\n",
       "  'content': 'Create the Hadoop `/bin` directory manually and add the downloaded files there. The shell script for Windows installation typically places them in `/c/tools/hadoop-3.2.0/`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/018_cbf0755be8_hadoop-filenotfoundexception-hadoop-bin-directory.md'},\n",
       " {'id': 'b024daf322',\n",
       "  'question': 'Which type of SQL is used in Spark? Postgres? MySQL? SQL Server?',\n",
       "  'sort_order': 19,\n",
       "  'content': 'Spark uses its own type of SQL, known as Spark SQL.\\n\\nThe SQL syntax across various providers is generally similar, as shown below:\\n\\n```sql\\nSELECT [attributes]\\nFROM [table]\\nWHERE [filter]\\nGROUP BY [grouping attributes]\\nHAVING [filtering the groups]\\nORDER BY [attribute to order]\\n(INNER/FULL/LEFT/RIGHT) JOIN [table2]\\nON [attributes table joining table2] (...)\\n```\\n\\nWhat differs most between SQL providers are the built-in functions.\\n\\nFor built-in Spark SQL functions, check this link: [Spark SQL Functions](https://spark.apache.org/docs/latest/api/sql/index.html)\\n\\nExtra information on Spark SQL:\\n\\n[What is Spark SQL?](https://databricks.com/glossary/what-is-spark-sql#:~:text=Spark%20SQL%20is%20a%20Spark,on%20existing%20deployments%20and%20data)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/019_b024daf322_which-type-of-sql-is-used-in-spark-postgres-mysql.md'},\n",
       " {'id': '772aea0210',\n",
       "  'question': 'The spark viewer on localhost:4040 was not showing the current run',\n",
       "  'sort_order': 20,\n",
       "  'content': '**Solution:**\\n\\n- Ensure you have identified all running Spark notebooks. If multiple notebooks are running, each will attempt to use available ports starting from 4040.\\n- If a port is in use, Spark automatically assigns the next available port (e.g., 4041, 4042, etc.).\\n- To find the exact port being used by your current Spark application, run the following command:\\n\\n  ```python\\n  spark.sparkContext.uiWebUrl\\n  ```\\n\\n- The result will provide the URL, for example: `[172.19.10.61:4041](http://172.19.10.61:4041)`.\\n- If the expected port does not show your current run, verify that cleanup has been performed on closed or non-running containers.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/020_772aea0210_the-spark-viewer-on-localhost4040-was-not-showing.md'},\n",
       " {'id': '11fc890b73',\n",
       "  'question': 'Java: java.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner Error during repartition call (conda pyspark installation)',\n",
       "  'sort_order': 21,\n",
       "  'content': '```java\\njava.lang.NoSuchMethodError: sun.nio.ch.DirectBuffer.cleaner()Lsun/misc/Cleaner;\\n```\\n\\n**Solution:** Replace Java Developer Kit 11 with Java Developer Kit 8.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/021_11fc890b73_java-javalangnosuchmethoderror-sunniochdirectbuffe.md'},\n",
       " {'id': '78c8446eae',\n",
       "  'question': 'Java - RuntimeError: Java gateway process exited before sending its port number',\n",
       "  'sort_order': 22,\n",
       "  'content': 'This error indicates that the `JAVA_HOME` environment variable is not set correctly in the notebook log.\\n\\nFor more information, you can refer to the following resource:\\n\\n[PySpark Exception - Java gateway process exited before sending the driver its port number](https://sparkbyexamples.com/pyspark/pyspark-exception-java-gateway-process-exited-before-sending-the-driver-its-port-number/)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/022_78c8446eae_java-runtimeerror-java-gateway-process-exited-befo.md'},\n",
       " {'id': 'a340ad48c5',\n",
       "  'question': 'Spark fails when reading from BigQuery and using `.show()` on `SELECT` queries',\n",
       "  'sort_order': 23,\n",
       "  'content': 'I got it working using `gcs-connector-hadoop-2.2.5-shaded.jar` and Spark 3.1.\\n\\nI also added the `google_credentials.json` and `.p12` to authenticate with GCS. These files are downloadable from the GCP Service account.\\n\\nTo create the SparkSession:\\n\\n```python\\nspark = SparkSession.builder.master(\\'local[*]\\') \\\\\\n    .appName(\\'spark-read-from-bigquery\\') \\\\\\n    .config(\\'BigQueryProjectId\\', \\'razor-project-xxxxxxx\\') \\\\\\n    .config(\\'BigQueryDatasetLocation\\', \\'de_final_data\\') \\\\\\n    .config(\\'parentProject\\', \\'razor-project-xxxxxxx\\') \\\\\\n    .config(\"google.cloud.auth.service.account.enable\", \"true\") \\\\\\n    .config(\"credentialsFile\", \"google_credentials.json\") \\\\\\n    .config(\"GcpJsonKeyFile\", \"google_credentials.json\") \\\\\\n    .config(\"spark.driver.memory\", \"4g\") \\\\\\n    .config(\"spark.executor.memory\", \"2g\") \\\\\\n    .config(\"spark.memory.offHeap.enabled\", True) \\\\\\n    .config(\"spark.memory.offHeap.size\", \"5g\") \\\\\\n    .config(\\'google.cloud.auth.service.account.json.keyfile\\', \"google_credentials.json\") \\\\\\n    .config(\"fs.gs.project.id\", \"razor-project-xxxxxxx\") \\\\\\n    .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\\\\n    .config(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\") \\\\\\n    .getOrCreate()\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/023_a340ad48c5_spark-fails-when-reading-from-bigquery-and-using-s.md'},\n",
       " {'id': '0a431f8863',\n",
       "  'question': 'Spark: BigQuery connector Automatic configuration',\n",
       "  'sort_order': 24,\n",
       "  'content': 'To automatically configure the Spark BigQuery connector, you can create a `SparkSession` by specifying the `spark.jars.packages` configuration.\\n\\n```python\\nspark = SparkSession.builder \\n    .master(\\'local\\') \\n    .appName(\\'bq\\') \\n    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-bigquery-with-dependencies_2.12:0.23.2\") \\n    .getOrCreate()\\n```\\n\\nThis approach automatically downloads the required dependency jars and configures the connector, eliminating the need to manually manage this dependency. More details are available [here](https://github.com/GoogleCloudDataproc/spark-bigquery-connector).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/024_0a431f8863_spark-bigquery-connector-automatic-configuration.md'},\n",
       " {'id': '49ee668514',\n",
       "  'question': 'Spark: How to read from GCP data lake using PySpark?',\n",
       "  'sort_order': 25,\n",
       "  'content': 'There are a few steps to read from Google Cloud Storage (GCS) using PySpark:\\n\\n1. **Download the Cloud Storage connector for Hadoop**\\n   - You can download it [here](https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters).\\n   - This `.jar` file connects PySpark with GCS.\\n\\n2. **Move the .jar file to your Spark file directory**\\n   - If you installed Spark using Homebrew on MacOS, create a `/jars` directory under your Spark directory, e.g., `/opt/homebrew/Cellar/apache-spark/3.2.1/jars`.\\n\\n3. **Import necessary classes in your Python script**\\n   ```python\\n   import pyspark\\n   from pyspark.sql import SparkSession\\n   from pyspark.conf import SparkConf\\n   from pyspark.context import SparkContext\\n   ```\\n\\n4. **Set up your configurations before building the SparkSession**\\n   ```python\\n   conf = SparkConf() \\\\\\n       .setMaster(\\'local[*]\\') \\\\\\n       .setAppName(\\'test\\') \\\\\\n       .set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\\\\n       .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\\\\n       .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\n   \\n   sc = SparkContext(conf=conf)\\n\\n   sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\\n   sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\\n   sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\\n   sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\\n   ```\\n\\n5. **Build your SparkSession with the new configurations**\\n   ```python\\n   spark = SparkSession.builder \\\\\\n       .config(conf=sc.getConf()) \\\\\\n       .getOrCreate()\\n   ```\\n\\n6. **You can now read files directly from GCS!**\\n\\nNote: If you encounter `start-slave.sh: command not found`, ensure Spark is correctly installed and paths are set.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/025_49ee668514_spark-how-to-read-from-gcp-data-lake-using-pyspark.md'},\n",
       " {'id': '6e251d34b6',\n",
       "  'question': 'How can I read a small number of rows from the parquet file directly?',\n",
       "  'sort_order': 26,\n",
       "  'content': 'To read a small number of rows from a parquet file, you can use PyArrow or Apache Spark:\\n\\n### Using PyArrow\\n\\n```python\\nfrom pyarrow.parquet import ParquetFile\\n\\npf = ParquetFile(\\'fhvhv_tripdata_2021-01.parquet\\')\\n\\n# PyArrow builds tables, not dataframes\\n\\ntbl_small = next(pf.iter_batches(batch_size=1000))\\n\\n# Convert the table to a DataFrame\\n\\ndf = tbl_small.to_pandas()\\n```\\n\\n### Alternatively, without PyArrow\\n\\n```python\\ndf = spark.read.parquet(\\'fhvhv_tripdata_2021-01.parquet\\')\\n\\ndf1 = df.sort(\\'DOLocationID\\').limit(1000)\\n\\npdf = df1.select(\"*\").toPandas()\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/026_6e251d34b6_how-can-i-read-a-small-number-of-rows-from-the-par.md'},\n",
       " {'id': '3e355d74fd',\n",
       "  'question': 'DataType error when creating Spark DataFrame with a specified schema?',\n",
       "  'sort_order': 27,\n",
       "  'content': \"When defining the schema for a Spark DataFrame, you might encounter a data type error if you're using a Parquet file with the schema definition from the TLC example. The error occurs because the `PULocationID` and `DOLocationID` columns are defined as `IntegerType`, but the Parquet file uses `INT64`.\\n\\nYou'll get an error like:\\n\\n```plaintext\\nParquet column cannot be converted in file [...] Column [...] Expected: int, Found: INT64\\n```\\n\\nTo resolve this issue:\\n\\n- Change the schema definition from `IntegerType` to `LongType`. This adjustment should align the expected and actual data types, allowing the DataFrame to be created successfully.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/027_3e355d74fd_datatype-error-when-creating-spark-dataframe-with.md'},\n",
       " {'id': '7fa4e026d6',\n",
       "  'question': 'Remove white spaces from column names in Pyspark',\n",
       "  'sort_order': 28,\n",
       "  'content': '```python\\ndf_finalx = df_finalw.select([col(x).alias(x.replace(\" \", \"\")) for x in df_finalw.columns])\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/028_7fa4e026d6_remove-white-spaces-from-column-names-in-pyspark.md'},\n",
       " {'id': '9b3ee420d4',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'sort_order': 29,\n",
       "  'content': 'This error occurs in the Spark video 5.3.1 - First Look at Spark/PySpark because the example utilizes CSV files from 2021, but the current data is in parquet format.\\n\\nWhen running the command:\\n\\n```python\\nspark.createDataFrame(df1_pandas).show()\\n```\\n\\nYou may encounter the Attribute error due to incompatibility between pandas version 2.0.0 and Spark 3.3.2. To resolve this, you can:\\n\\n- Downgrade pandas to version 1.5.3 using the following command:\\n  \\n  ```bash\\n  pip install -U pandas==1.5.3\\n  ```\\n  \\n- Alternatively, add the following line after importing pandas if you prefer not to downgrade:\\n  \\n  ```python\\n  pd.DataFrame.iteritems = pd.DataFrame.items\\n  ```\\n\\nThis issue is resolved in Spark versions from 3.4.1 onwards.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/029_9b3ee420d4_attributeerror-dataframe-object-has-no-attribute-i.md'},\n",
       " {'id': '639151c1d1',\n",
       "  'question': \"AttributeError: 'DataFrame' object has no attribute 'iteritems'\",\n",
       "  'sort_order': 30,\n",
       "  'content': 'Another alternative is to install pandas 2.0.1 (it worked well at the time of writing this), and it is compatible with Pyspark 3.5.1. Make sure to add or edit your environment variable like this:\\n\\n```bash\\nexport SPARK_HOME=\"${HOME}/spark/spark-3.5.1-bin-hadoop3\"\\n\\nexport PATH=\"${SPARK_HOME}/bin:${PATH}\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/030_639151c1d1_attributeerror-dataframe-object-has-no-attribute-i.md'},\n",
       " {'id': '01092d660a',\n",
       "  'question': 'Spark Standalone Mode on Windows',\n",
       "  'sort_order': 31,\n",
       "  'content': 'To set up Spark in standalone mode on Windows, follow these steps:\\n\\n1. Open a CMD terminal in administrator mode.\\n\\n2. Navigate to your Spark home directory:\\n   \\n   ```bash\\n   cd %SPARK_HOME%\\n   ```\\n\\n3. Start a master node:\\n   \\n   ```bash\\n   bin\\\\spark-class org.apache.spark.deploy.master.Master\\n   ```\\n\\n4. Start a worker node:\\n   \\n   ```bash\\n   bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://<master_ip>:<port> --host <IP_ADDR>\\n   ```\\n   \\n   Example:\\n   \\n   ```bash\\n   bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host <IP_ADDR>\\n   ```\\n\\n   - `spark://<master_ip>:<port>`: Copy the address from the previous command. For example, `spark://localhost:7077`\\n   - Use `--host <IP_ADDR>` if you want to run the worker on a different machine. You can leave it empty for local setup.\\n\\n5. Access the Spark UI through `localhost:8080`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/031_01092d660a_spark-standalone-mode-on-windows.md'},\n",
       " {'id': 'e4654d1d0b',\n",
       "  'question': 'Export PYTHONPATH command in Linux is temporary',\n",
       "  'sort_order': 32,\n",
       "  'content': 'To make the `export PYTHONPATH` command permanent, consider the following options:\\n\\n1. **Add to `.bashrc`:** \\n   - Open the `.bashrc` file located in your home directory using a text editor, such as `nano` or `vim`.\\n   \\n   ```bash\\n   nano ~/.bashrc\\n   ```\\n   \\n   - Add the `export PYTHONPATH` line, such as:\\n   \\n   ```bash\\n   export PYTHONPATH=\"/your/custom/path\"\\n   ```\\n   \\n   - Save the changes and source the file to apply the changes:\\n   \\n   ```bash\\n   source ~/.bashrc\\n   ```\\n\\n2. **Run specific commands in Python scripts:**\\n   - You can initialize the environment directly in a Python script using:\\n   \\n   ```python\\n   import findspark\\n   findspark.init()\\n   ```\\n\\nBy using these methods, you ensure that the `PYTHONPATH` is set up for every session automatically.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/032_e4654d1d0b_export-pythonpath-command-in-linux-is-temporary.md'},\n",
       " {'id': '7bff88dbd7',\n",
       "  'question': 'Compression Error: zcat output is gibberish, seems like still compressed',\n",
       "  'sort_order': 33,\n",
       "  'content': 'In the code along from Video 5.3.3, Alexey downloads the CSV files from the NYT website and gzips them in their bash script. Currently (2023), if we download the data from the GH course repo, it is already zipped as `csv.gz` files. Following the video exactly would zip them again, leading to gibberish output when using `zcat`, as it only unzips the file once.\\n\\n**Solution:** Do not gzip the files downloaded from the course repo. Simply use `wget` to download and save them as `csv.gz` files. Then the `zcat` command and the `showSchema` command will work correctly.\\n\\n```bash\\nURL=\"${URL_PREFIX}/${TAXI_TYPE}/${TAXI_TYPE}_tripdata_${YEAR}-${FMONTH}.csv.gz\"\\n\\nLOCAL_PREFIX=\"data/raw/${TAXI_TYPE}/${YEAR}/${FMONTH}\"\\n\\nLOCAL_FILE=\"${TAXI_TYPE}_tripdata_${YEAR}_${FMONTH}.csv.gz\"\\n\\nLOCAL_PATH=\"${LOCAL_PREFIX}/${LOCAL_FILE}\"\\n\\necho \"downloading ${URL} to ${LOCAL_PATH}\"\\n\\nmkdir -p ${LOCAL_PREFIX}\\n\\nwget ${URL} -O ${LOCAL_PATH}\\n\\necho \"compressing ${LOCAL_PATH}\"\\n\\n# gzip ${LOCAL_PATH} <- uncomment this line\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/033_7bff88dbd7_compression-error-zcat-output-is-gibberish-seems-l.md'},\n",
       " {'id': '91df9f5c6f',\n",
       "  'question': 'PicklingError: Could not serialise object: IndexError: tuple index out of range.',\n",
       "  'sort_order': 34,\n",
       "  'content': 'This error occurs while running:\\n\\n```python\\nspark.createDataFrame(df_pandas).show()\\n```\\n\\n### Cause\\nThis issue is usually related to the Python version. As of March 2, 2023, Spark does not support Python 3.11.\\n\\n### Solution\\n\\n1. **Create a New Environment with a Supported Python Version:**\\n\\n   Using Conda, you can create a virtual environment with Python 3.10:\\n   \\n   ```bash\\n   conda create -n myenv python=3.10 anaconda\\n   ```\\n\\n2. **Activate the Environment:**\\n\\n   To use Python 3.10, activate the environment:\\n   \\n   ```bash\\n   conda activate myenv\\n   ```\\n\\n3. **Deactivate the Environment:**\\n\\n   If you need to return to the initial setup, you can deactivate the environment:\\n   \\n   ```bash\\n   conda deactivate\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/034_91df9f5c6f_picklingerror-could-not-serialise-object-indexerro.md'},\n",
       " {'id': '497269a77f',\n",
       "  'question': 'Connecting from local Spark to GCS: Spark does not find my Google credentials as shown in the video?',\n",
       "  'sort_order': 35,\n",
       "  'content': 'Make sure you have your credentials for your GCP in your VM under the location defined in the script.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/035_497269a77f_connecting-from-local-spark-to-gcs-spark-does-not.md'},\n",
       " {'id': '6d0a7d749a',\n",
       "  'question': 'Spark docker-compose setup',\n",
       "  'sort_order': 36,\n",
       "  'content': 'To run Spark in a Docker setup:\\n\\n1. **Build Bitnami Spark Docker**\\n   \\n   a. Clone the Bitnami repository using the command:\\n   \\n   ```bash\\n   git clone https://github.com/bitnami/containers.git\\n   ```\\n   \\n   *(Tested on commit 9cef8b892d29c04f8a271a644341c8222790c992)*\\n   \\n   b. Edit the file `bitnami/spark/3.3/debian-11/Dockerfile` and update the Java and Spark version as follows:\\n   \\n   ```\\n   \"python-3.10.10-2-linux-${OS_ARCH}-debian-11\" \\n   \"java-17.0.5-8-3-linux-${OS_ARCH}-debian-11\" \\n   ```\\n   \\n   Reference: [GitHub](https://github.com/bitnami/containers/issues/13409)\\n\\n   c. Build the Docker image by navigating to the above directory and running the Docker build command:\\n   \\n   ```bash\\n   cd bitnami/spark/3.3/debian-11/\\n   docker build -t spark:3.3-java-17 .\\n   ```\\n\\n2. **Run Docker Compose**\\n\\n   Use the following `docker-compose.yml` file:\\n\\n   ```yaml\\n   version: \\'2\\'\\n\\n   services:\\n\\n     spark:\\n       image: spark:3.3-java-17\\n       environment:\\n         - SPARK_MODE=master\\n         - SPARK_RPC_AUTHENTICATION_ENABLED=no\\n         - SPARK_RPC_ENCRYPTION_ENABLED=no\\n         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n         - SPARK_SSL_ENABLED=no\\n       volumes:\\n         - \"./:/home/jovyan/work:rw\"\\n       ports:\\n         - \\'8080:8080\\'\\n         - \\'7077:7077\\'\\n\\n     spark-worker:\\n       image: spark:3.3-java-17\\n       environment:\\n         - SPARK_MODE=worker\\n         - SPARK_MASTER_URL=spark://spark:7077\\n         - SPARK_WORKER_MEMORY=1G\\n         - SPARK_WORKER_CORES=1\\n         - SPARK_RPC_AUTHENTICATION_ENABLED=no\\n         - SPARK_RPC_ENCRYPTION_ENABLED=no\\n         - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no\\n         - SPARK_SSL_ENABLED=no\\n       volumes:\\n         - \"./:/home/jovyan/work:rw\"\\n       ports:\\n         - \\'8081:8081\\'\\n\\n     spark-nb:\\n       image: jupyter/pyspark-notebook:java-17.0.5\\n       environment:\\n         - SPARK_MASTER_URL=spark://spark:7077\\n       volumes:\\n         - \"./:/home/jovyan/work:rw\"\\n       ports:\\n         - \\'8888:8888\\'\\n         - \\'4040:4040\\'\\n   ```\\n\\n   Run the command to deploy Docker Compose:\\n\\n   ```bash\\n   docker-compose up\\n   ```\\n\\n   Access the Jupyter notebook using the link logged in Docker Compose logs.\\n\\n   The Spark master URL is `spark://spark:7077`.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/036_6d0a7d749a_spark-docker-compose-setup.md'},\n",
       " {'id': 'e3ce42af1a',\n",
       "  'question': 'How do you read data stored in GCS on pandas with your local computer?',\n",
       "  'sort_order': 37,\n",
       "  'content': \"To do this:\\n\\n1. Install `gcsfs`:\\n   \\n   ```bash\\n   pip install gcsfs\\n   ```\\n\\n2. Copy the URI path to the file and use the following command to read it:\\n   \\n   ```python\\n   df = pandas.read_csv('gs://path/to/your/file.csv')\\n   ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/037_e3ce42af1a_how-do-you-read-data-stored-in-gcs-on-pandas-with.md'},\n",
       " {'id': '34ebc2c6de',\n",
       "  'question': 'TypeError when using spark.createDataFrame function on a pandas df',\n",
       "  'sort_order': 38,\n",
       "  'content': '**Error:**\\n\\n```python\\nspark.createDataFrame(df_pandas).schema\\n\\nTypeError: field Affiliated_base_number: Can not merge type <class \\'pyspark.sql.types.StringType\\'> and <class \\'pyspark.sql.types.DoubleType\\'>\\n```\\n\\n**Solution:**\\n\\n- **Reason:**\\n  - The `Affiliated_base_number` field is a mix of letters and numbers, so it cannot be set to `DoubleType`. The suitable type would be `StringType`. Spark\\'s `inferSchema` is more accurate than Pandas\\' infer type method in this case. Set `inferSchema` to `true` when reading the CSV to prevent data removal.\\n\\n- **Implementation:**\\n  \\n  ```python\\n  df = spark.read \\\\\\n    .options(\\n      header = \"true\", \\\\\\n      inferSchema = \"true\"\\n    ) \\\\\\n    .csv(\\'path/to/your/csv/file/\\')\\n  ````\\n\\n- **Alternative Solution:**\\n  \\n  - **Problem:** Some rows in `affiliated_base_number` are null, and therefore are assigned the datatype `String`, which cannot be converted to `Double`.\\n  - **Solution:** Only take rows from the pandas df that are not null in the \\'Affiliated_base_number\\' column before converting it to a PySpark DataFrame.\\n\\n  ```python\\n  # Only take rows that have no null values\\n  pandas_df = pandas_df[pandas_df.notnull().all(1)]\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/038_34ebc2c6de_typeerror-when-using-sparkcreatedataframe-function.md'},\n",
       " {'id': '14cc6e2060',\n",
       "  'question': 'MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory',\n",
       "  'sort_order': 39,\n",
       "  'content': 'Default executor memory is 1GB. This error appeared when working with the homework dataset.\\n\\nError:\\n\\n```plaintext\\nMemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memoryScaling row group sizes to 95.00% for 8 writers\\n```\\n\\nSolution:\\n\\nIncrease the memory of the executor when creating the Spark session like this:\\n\\n```python\\nspark = SparkSession.builder \\\\\\n    .master(\"local[*]\") \\\\\\n    .appName(\\'test\\') \\\\\\n    .config(\"spark.executor.memory\", \"4g\") \\\\\\n    .config(\"spark.driver.memory\", \"4g\") \\\\\\n    .getOrCreate()\\n```\\n\\nRemember to restart the Jupyter session (i.e., close the Spark session) or the config won’t take effect.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/039_14cc6e2060_memorymanager-total-allocation-exceeds-9500-102005.md'},\n",
       " {'id': '46b39cdb17',\n",
       "  'question': 'How to spark standalone cluster is run on windows OS',\n",
       "  'sort_order': 40,\n",
       "  'content': '- Change the working directory to the Spark directory:\\n\\n  - If you have set up your `SPARK_HOME` variable, use the following:\\n    \\n    ```bash\\n    cd %SPARK_HOME%\\n    ```\\n\\n  - If not, use the following:\\n\\n    ```bash\\n    cd <path to spark installation>\\n    ```\\n\\n- Creating a Local Spark Cluster:\\n\\n  1. To start Spark Master:\\n  \\n     ```bash\\n     bin\\\\spark-class org.apache.spark.deploy.master.Master --host localhost\\n     ```\\n  \\n  2. Starting up a cluster:\\n     \\n     ```bash\\n     bin\\\\spark-class org.apache.spark.deploy.worker.Worker spark://localhost:7077 --host localhost\\n     ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/040_46b39cdb17_how-to-spark-standalone-cluster-is-run-on-windows.md'},\n",
       " {'id': 'efa85f71b6',\n",
       "  'question': 'Env variables set in ~/.bashrc are not loaded to Jupyter in VS Code',\n",
       "  'sort_order': 41,\n",
       "  'content': 'I added `PYTHONPATH`, `JAVA_HOME`, and `SPARK_HOME` to `~/.bashrc`. Importing `pyspark` worked in iPython in the terminal but couldn\\'t be found in a `.ipynb` file opened in VS Code.\\n\\nAfter adding new lines to `~/.bashrc`, you need to restart the shell to activate the changes. You can do either of the following:\\n\\n```bash\\nsource ~/.bashrc\\n```\\n\\nor\\n\\n```bash\\nexec bash\\n```\\n\\nInstead of configuring paths in `~/.bashrc`, you can create a `.env` file in the root of your workspace with the following content:\\n\\n```bash\\nJAVA_HOME=\"${HOME}/app/java/jdk-11.0.2\"\\n\\nPATH=\"${JAVA_HOME}/bin:${PATH}\"\\n\\nSPARK_HOME=\"${HOME}/app/spark/spark-3.3.2-bin-hadoop3\"\\n\\nPATH=\"${SPARK_HOME}/bin:${PATH}\"\\n\\nPYTHONPATH=\"${SPARK_HOME}/python/:$PYTHONPATH\"\\n\\nPYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.5-src.zip:$PYTHONPATH\"\\n\\nPYTHONPATH=\"${SPARK_HOME}/python/lib/pyspark.zip:$PYTHONPATH\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/041_efa85f71b6_env-variables-set-in-bashrc-are-not-loaded-to-jupy.md'},\n",
       " {'id': 'cd5d3e8423',\n",
       "  'question': 'hadoop “wc -l” is giving a different result than shown in the video',\n",
       "  'sort_order': 42,\n",
       "  'content': 'If you are using `wc -l fhvhv_tripdata_2021-01.csv.gz` with the gzip file as the file argument, you will get a different result, obviously, since the file is compressed.\\n\\nUnzip the file and then use:\\n\\n```bash\\nwc -l fhvhv_tripdata_2021-01.csv\\n```\\n\\nto get the right results.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/042_cd5d3e8423_hadoop-wc-l-is-giving-a-different-result-than-show.md'},\n",
       " {'id': '05fc1b38f6',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_073b1786.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_57fd99e0.png'}],\n",
       "  'question': 'Hadoop: Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z',\n",
       "  'sort_order': 43,\n",
       "  'content': 'If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\\n\\nFor Windows, follow these steps:\\n\\n1. Create a new User Variable \"HADOOP_HOME\" that points to your Hadoop directory.\\n2. Add \"%HADOOP_HOME%\\\\bin\" to the PATH variable.\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>\\n\\nAdditional tips can be found here: [Stack Overflow](https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/043_05fc1b38f6_hadoop-exception-in-thread-main-javalangunsatisfie.md'},\n",
       " {'id': '3a1fbd50a0',\n",
       "  'question': 'Java.io.IOException. Cannot run program “C:\\\\hadoop\\\\bin\\\\winutils.exe”. CreateProcess error=216, This version of 1% is not compatible with the version of Windows you are using.',\n",
       "  'sort_order': 44,\n",
       "  'content': \"To resolve the issue, follow these steps:\\n\\n1. Change the Hadoop version to 3.0.1.\\n2. Replace all the files in the local Hadoop `bin` folder with the files from this repository: [winutils/hadoop-3.0.1/bin at master · cdarlint/winutils](https://github.com/cdarlint/winutils/tree/master/hadoop-3.0.1/bin).\\n3. If this does not work, try other versions available in the repository.\\n\\nFor more information, refer to the following issue discussion: [This version of %1 is not compatible with the version of Windows you're running · Issue #20 · cdarlint/winutils](https://github.com/cdarlint/winutils/issues/20)\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/044_3a1fbd50a0_javaioioexception-cannot-run-program-chadoopbinwin.md'},\n",
       " {'id': 'd01f2fb06b',\n",
       "  'question': 'Dataproc: ERROR: (gcloud.dataproc.jobs.submit.pyspark) The required property [project] is not currently set. It can be set on a per-command basis by re-running your command with the [--project] flag.',\n",
       "  'sort_order': 45,\n",
       "  'content': 'Fix is to set the flag like the error states. Get your project ID from your dashboard and set it like so:\\n\\n```bash\\ngcloud dataproc jobs submit pyspark \\\\\\n  --cluster=my_cluster \\\\\\n  --region=us-central1 \\\\\\n  --project=my-dtc-project-1010101 \\\\\\n  gs://my-dtc-bucket-id/code/06_spark_sql.py \\\\\\n  -- \\n```\\n\\n<{IMAGE:image_id}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/045_d01f2fb06b_dataproc-error-gclouddataprocjobssubmitpyspark-the.md'},\n",
       " {'id': '61b2ad91aa',\n",
       "  'question': 'Run Local Cluster Spark in Windows 10 with CMD',\n",
       "  'sort_order': 46,\n",
       "  'content': '1. Go to `%SPARK_HOME%\\\\bin`\\n\\n2. Run the following command to start the master:\\n\\n   ```bash\\n   spark-class org.apache.spark.deploy.master.Master\\n   ```\\n   \\n   This will give you a URL of the form `spark://ip:port`.\\n\\n3. Run the following command to start the worker, replacing `spark://ip:port` with the URL obtained from the previous step:\\n\\n   ```bash\\n   spark-class org.apache.spark.deploy.worker.Worker spark://ip:port\\n   ```\\n\\n4. Create a new Jupyter notebook and set up the Spark session:\\n\\n   ```python\\n   spark = SparkSession.builder \\\\\\n       .master(\"spark://{ip}:7077\") \\\\\\n       .appName(\\'test\\') \\\\\\n       .getOrCreate()\\n   ```\\n\\n5. Check on the Spark UI to see the master, worker, and application running.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/046_61b2ad91aa_run-local-cluster-spark-in-windows-10-with-cmd.md'},\n",
       " {'id': 'fdef2dfcc4',\n",
       "  'question': \"lServiceException: 401 Anonymous caller does not have storage.objects.list access to the Google Cloud Storage bucket. Permission 'storage.objects.list' denied on resource (or it may not exist).\",\n",
       "  'sort_order': 47,\n",
       "  'content': 'This occurs because you are not logged in with Google Cloud SDK, or the project ID is not set. Follow these steps:\\n\\n1. Log in using Google Cloud SDK:\\n   \\n   ```bash\\n   gcloud auth login\\n   ```\\n   \\n   This will open a tab in the browser. Accept the terms, then close the tab if needed.\\n\\n2. Set the project ID:\\n   \\n   ```bash\\n   gcloud config set project <YOUR_PROJECT_ID>\\n   ```\\n\\n3. Upload the `pq` directory to a Google Cloud Storage (GCS) Bucket:\\n   \\n   ```bash\\n   gsutil -m cp -r pq/ <YOUR_URI_from_gsutil>/pq\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/047_fdef2dfcc4_lserviceexception-401-anonymous-caller-does-not-ha.md'},\n",
       " {'id': '39fc22b5cb',\n",
       "  'question': 'GCP: py4j.protocol.Py4JJavaError',\n",
       "  'sort_order': 48,\n",
       "  'content': \"When submitting a job, you might encounter a `py4j.protocol.Py4JJavaError` related to Java in the log panel within Dataproc. \\n\\nTo address this error, consider the following steps:\\n\\n1. **Cluster Versioning Control:**\\n   - If you've recently changed the versioning settings, ensure that the cluster configuration is compatible with your requirements. For example, switching from **Debian-Hadoop-Spark** to **Ubuntu 20.02-Hadoop3.3-Spark3.3** might resolve issues if you have a similar setup on your local machine.\\n\\n2. **Consistency with Local Environment:**\\n   - Aligning the cluster's OS version and software stack with your local environment can help reduce configuration issues.\\n\\nAlthough specific documentation may not be available, this approach has proven effective in some scenarios.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/048_39fc22b5cb_gcp-py4jprotocolpy4jjavaerror.md'},\n",
       " {'id': 'fa58733a98',\n",
       "  'question': 'Repartition the Dataframe to 6 partitions using df.repartition(6) - got 8 partitions instead',\n",
       "  'sort_order': 49,\n",
       "  'content': \"Use both `repartition` and `coalesce`, like so:\\n\\n```python\\ndf = df.repartition(6)\\n\\ndf = df.coalesce(6)\\n\\ndf.write.parquet('fhv/2019/10', mode='overwrite')\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/049_fa58733a98_repartition-the-dataframe-to-6-partitions-using-df.md'},\n",
       " {'id': '05dc43cda8',\n",
       "  'question': 'Jupyter Notebook or SparkUI not loading properly at localhost after port forwarding from VS code?',\n",
       "  'sort_order': 50,\n",
       "  'content': \"**Possible Solution:** Try to forward the port using SSH CLI instead of VS Code. \\n\\nRun the following command:\\n\\n```bash\\nssh -L <local port>:<VM host/ip>:<VM port> <ssh hostname>\\n```\\n\\n- `ssh hostname` is the name you specified in the `~/.ssh/config` file.\\n\\nFor example, in case of Jupyter Notebook, run:\\n\\n```bash\\nssh -L 8888:localhost:8888 gcp-vm\\n```\\n\\nfrom your local machine’s CLI.\\n\\n**Note:** If you logout from the session, the connection would break. While creating the Spark session, check the block's log because sometimes it fails to run at 4040 and switches to 4041.\\n\\nIf you are having trouble accessing localhost ports from a GCP VM, consider adding the forwarding instructions to the `.ssh/config` file as follows:\\n\\n```bash\\nHost <hostname>\\n  Hostname <external-gcp-ip>\\n  User xxxx\\n  IdentityFile yyyy\\n  LocalForward 8888 localhost:8888\\n  LocalForward 8080 localhost:8080\\n  LocalForward 5432 localhost:5432\\n  LocalForward 4040 localhost:4040\\n```\\n\\nThis should automatically forward all ports and will enable accessing localhost ports.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/050_05dc43cda8_jupyter-notebook-or-sparkui-not-loading-properly-a.md'},\n",
       " {'id': '72d6369d8a',\n",
       "  'question': 'Installing Java 11 on codespaces',\n",
       "  'sort_order': 51,\n",
       "  'content': \"1. Use the command below to check for available Java SDK versions:\\n   \\n   ```bash\\n   sdk list java\\n   ```\\n\\n2. Install the desired version, for example:\\n   \\n   ```bash\\n   sdk install java 11.0.22-amzn\\n   ```\\n\\n3. If prompted, press 'Y' to change the default Java version.\\n\\n4. Verify the installation by checking the Java version:\\n   \\n   ```bash\\n   java -version\\n   ```\\n   \\n5. If the version does not work correctly, set the default version with:\\n   \\n   ```bash\\n   sdk default java 11.0.22-amzn\\n   ```\\n   \\n   Adjust this command to match the version you have installed.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/051_72d6369d8a_installing-java-11-on-codespaces.md'},\n",
       " {'id': '0c9fc7e7d3',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_6ab99490.png'}],\n",
       "  'question': \"Error: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 470.0.\",\n",
       "  'sort_order': 52,\n",
       "  'content': 'Sometimes while creating a dataproc cluster on GCP, the following error is encountered.\\n\\n<{IMAGE:image_1}>\\n\\nSolutions:\\n\\n1. As mentioned [here](https://stackoverflow.com/a/59038704/22748533), sometimes there might not be enough resources in the given region to allocate the request. Usually, resources get freed up in a bit, and one can create a cluster.\\n\\n2. Changing the type of boot-disk from PD-Balanced to PD-Standard in Terraform helped solve the problem.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/052_0c9fc7e7d3_error-insufficient-ssd_total_gb-quota-requested-50.md'},\n",
       " {'id': '1da0437718',\n",
       "  'question': 'Homework: how to convert the time difference of two timestamps to hours',\n",
       "  'sort_order': 53,\n",
       "  'content': \"Pyspark converts the difference of two `TimestampType` values to Python's native `datetime.timedelta` object. The `timedelta` object stores the duration in terms of days, seconds, and microseconds. Each of these units must be manually converted into hours to express the total duration between the two timestamps using only hours.\\n\\nAnother method to achieve this is using the `datediff` SQL function. It requires the following parameters:\\n\\n- **Upper Date**: The closer date, e.g., `dropoff_datetime`.\\n- **Lower Date**: The farther date, e.g., `pickup_datetime`.\\n\\nThe result is returned in days, so you can multiply the result by 24 to get the duration in hours.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/053_1da0437718_homework-how-to-convert-the-time-difference-of-two.md'},\n",
       " {'id': '1913393ea4',\n",
       "  'question': 'PicklingError: Could not serialize object: IndexError: tuple index out of range',\n",
       "  'sort_order': 54,\n",
       "  'content': \"This version combination worked for resolving the issue:\\n\\n- **PySpark**: 3.3.2\\n- **Pandas**: 1.5.3\\n\\nIf you continue to encounter the error:\\n\\n```\\nPy4JJavaError: An error occurred while calling o180.showString. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 6.0 failed 1 times, most recent failure: Lost task 0.0 in stage 6.0 (TID 6) (host.docker.internal executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\\n```\\n\\nTry running the following before initializing `SparkSession`:\\n\\n```python\\nimport os\\nimport sys\\n\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\n```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/054_1913393ea4_picklingerror-could-not-serialize-object-indexerro.md'},\n",
       " {'id': 'cb1c023dc2',\n",
       "  'question': 'RuntimeError: Python in worker has different version 3.11 than that in driver 3.10, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.',\n",
       "  'sort_order': 55,\n",
       "  'content': \"To resolve the version mismatch error between the worker and driver Python versions in PySpark, set the environment variables `PYSPARK_PYTHON` and `PYSPARK_DRIVER_PYTHON` to the same executable.\\n\\n```python\\nimport os\\nimport sys\\n\\nos.environ['PYSPARK_PYTHON'] = sys.executable\\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\\n```\\n\\nFor further information on Dataproc Pricing, visit: [Dataproc Pricing](https://cloud.google.com/dataproc/pricing#on_gke_pricing)\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/055_cb1c023dc2_runtimeerror-python-in-worker-has-different-versio.md'},\n",
       " {'id': 'd707493842',\n",
       "  'question': 'Dataproc Qn: Is it essential to have a VM on GCP for running Dataproc and submitting jobs?',\n",
       "  'sort_order': 56,\n",
       "  'content': 'No, you can submit a job to Dataproc from your local computer by installing and configuring `gsutil`. For installation instructions, visit [https://cloud.google.com/storage/docs/gsutil_install](https://cloud.google.com/storage/docs/gsutil_install).\\n\\nYou can execute the following command from your local computer:\\n\\n```bash\\ngcloud dataproc jobs submit pyspark \\\\\\n  --cluster=de-zoomcamp-cluster \\\\\\n  --region=europe-west6 \\\\\\n  gs://dtc_data_lake_de-zoomcamp-nytaxi/code/06_spark_sql.py \\\\\\n  -- \\\\\\n  --input_green=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/green/2020/*/ \\\\\\n  --input_yellow=gs://dtc_data_lake_de-zoomcamp-nytaxi/pq/yellow/2020/*/ \\\\\\n  --output=gs://dtc_data_lake_de-zoomcamp-nytaxi/report-2020\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/056_d707493842_dataproc-qn-is-it-essential-to-have-a-vm-on-gcp-fo.md'},\n",
       " {'id': '0c116f6001',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_91f633ae.png'}],\n",
       "  'question': 'In module 5.3.1, trying to run `spark.createDataFrame(df_pandas).show()` returns error',\n",
       "  'sort_order': 57,\n",
       "  'content': \"```\\nAttributeError: 'DataFrame' object has no attribute 'iteritems'\\n```\\n\\nThis error occurs because a method inside PySpark refers to a package that has been deprecated ([Stack Overflow](https://stackoverflow.com/questions/76404811/attributeerror-dataframe-object-has-no-attribute-iteritems)).\\n\\n### Solutions\\n\\n- Refer to the code mentioned in the Stack Overflow link.\\n\\n  <{IMAGE:image_1}>\\n\\n- Another workaround is to create a conda environment to downgrade Python's version to 3.8 and Pandas to 1.5.3:\\n\\n  ```bash\\n  conda create -n pyspark_env python=3.8 pandas=1.5.3 jupyter\\n  \\n  conda activate pyspark_env\\n  ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/057_0c116f6001_in-module-531-trying-to-run-sparkcreatedataframedf.md'},\n",
       " {'id': 'e058b2432a',\n",
       "  'question': \"Cannot create a cluster: Insufficient 'SSD_TOTAL_GB' quota. Requested 500.0, available 250.0.\",\n",
       "  'sort_order': 58,\n",
       "  'content': 'The master and worker nodes are allocated a maximum of 250 GB of memory combined. In the configuration section, adhere to the following specifications:\\n\\n- **Master Node:**\\n  - Machine type: `n2-standard-2`\\n  - Primary disk size: 85 GB\\n\\n- **Worker Node:**\\n  - Number of worker nodes: 2\\n  - Machine type: `n2-standard-2`\\n  - Primary disk size: 80 GB\\n\\nYou can allocate up to 82.5 GB memory for worker nodes, keeping in mind that the total memory allocated across all nodes cannot exceed 250 GB.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/058_e058b2432a_cannot-create-a-cluster-insufficient-ssd_total_gb.md'},\n",
       " {'id': 'a5a8a6ac39',\n",
       "  'question': 'Setting JAVA_HOME with Homebrew on Apple Silicon',\n",
       "  'sort_order': 59,\n",
       "  'content': 'The MacOS setup instruction ([here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/05-batch/setup/macos.md#installing-java)) for setting the `JAVA_HOME` environment variable is for Intel-based Macs, which have a default install location at `/usr/local/`. If you have an Apple Silicon Mac, you need to set `JAVA_HOME` to `/opt/homebrew/`. Update your `.bashrc` or `.zshrc` file with the following:\\n\\n```bash\\nexport JAVA_HOME=\"/opt/homebrew/opt/openjdk/bin\"\\nexport PATH=\"$JAVA_HOME:$PATH\"\\n```\\n\\nConfirm that your path was correctly set by running the command:\\n\\n```bash\\nwhich java\\n```\\n\\nYou should expect to see the output:\\n\\n```\\n/opt/homebrew/opt/openjdk/bin/java\\n```\\n\\nCheck the Java version with the following command:\\n\\n```bash\\njava -version\\n```\\n\\nReference: [https://docs.brew.sh/Installation](https://docs.brew.sh/Installation)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/059_a5a8a6ac39_setting-java_home-with-homebrew-on-apple-silicon.md'},\n",
       " {'id': 'a25406395b',\n",
       "  'question': \"Subnetwork 'default' does not support Private Google Access which is required for Dataproc clusters when 'internal_ip_only' is set to 'true'. Enable Private Google Access on subnetwork 'default' or set 'internal_ip_only' to 'false'.\",\n",
       "  'sort_order': 60,\n",
       "  'content': 'To resolve this issue, follow these steps:\\n\\n1. Search for VPC in Google Cloud Console.\\n2. Navigate to the \"SUBNETS IN CURRENT PROJECT\" tab.\\n3. Locate the region/location where your Dataproc will be located and click on it.\\n4. Click the edit button and toggle on \"Private Google Access.\"\\n5. Save changes.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-5/060_a25406395b_subnetwork-default-does-not-support-private-google.md'},\n",
       " {'id': '5b1d465332',\n",
       "  'question': 'Spark: Is working, however, nothing appears in the Spark UI (e.g., .show())?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"Since we used multiple notebooks during the course, it's possible that there are more than one Spark session active. It’s highly likely that you are observing the incorrect one. Follow these steps to troubleshoot:\\n\\n- Spark uses port 4040 by default, but if more than one session is active, it will try to use the next available port (e.g., 4041).\\n\\n- Ensure you're viewing the correct Spark Web UI for the application where your jobs are running.\\n\\n- Verify the current application session address:\\n  \\n  ```python\\n  # Using the following command in your session\\n  spark.sparkContext.uiWebUrl\\n  ```\\n  \\n  Expected output might look like:\\n  \\n  ```\\n  http://your.application.session.address.internal:4041\\n  ```\\n  \\n  Indicating port 4041.\\n\\n- If using a VM, make sure you forward the identified port to access the web UI on `localhost:<port>`.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/001_5b1d465332_spark-is-working-however-nothing-appears-in-the-sp.md'},\n",
       " {'id': '94ccf8c158',\n",
       "  'question': 'Docker: Could not start docker image \"control-center\" from the docker-compose.yaml file.',\n",
       "  'sort_order': 2,\n",
       "  'content': 'Check Docker Compose File:\\n\\nEnsure that your `docker-compose.yaml` file is correctly configured with the necessary details for the \"control-center\" service. Check the service name, image name, ports, volumes, environment variables, and any other configurations required for the container to start.\\n\\nOn Mac OSX 12.2.1 (Monterey), if you encounter this issue, try the following:\\n\\n- Open Docker Desktop and check for any images still running from previous sessions.\\n- If there are images still running and they do not appear with the `docker ps` command, they may need to be deleted directly from Docker Desktop.\\n- Once those images are removed, try starting the Kafka environment again.\\n\\nThis approach resolved the issue on Mac OSX 12.2.1 for a similar setup.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/002_94ccf8c158_docker-could-not-start-docker-image-control-center.md'},\n",
       " {'id': 'a1ea959c7e',\n",
       "  'question': 'Module “kafka” not found when trying to run producer.py',\n",
       "  'sort_order': 3,\n",
       "  'content': 'To resolve the \"Module \\'kafka\\' not found\" error when running `producer.py`, you can create a virtual environment and install the required packages. Follow these steps:\\n\\n1. **Create a Virtual Environment**\\n   \\n   Run the following command to create a virtual environment:\\n   \\n   ```bash\\n   python -m venv env\\n   ```\\n\\n2. **Activate the Virtual Environment**\\n\\n   - On macOS and Linux:\\n     \\n     ```bash\\n     source env/bin/activate\\n     ```\\n   \\n   - On Windows:\\n     \\n     ```bash\\n     env\\\\Scripts\\\\activate\\n     ```\\n\\n3. **Install Required Packages**\\n   \\n   Install the packages listed in `requirements.txt`:\\n   \\n   ```bash\\n   pip install -r ../requirements.txt\\n   ```\\n\\n4. **Deactivate the Virtual Environment**\\n   \\n   When you\\'re done, deactivate the virtual environment:\\n   \\n   ```bash\\n   deactivate\\n   ```\\n\\n**Note:** Ensure that Docker images are running before executing the Python file. The virtual environment is meant for running the Python files locally.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/003_a1ea959c7e_module-kafka-not-found-when-trying-to-run-producer.md'},\n",
       " {'id': '1cc0ab1fea',\n",
       "  'question': 'Error: Importing cimpl DLL when running Avro examples',\n",
       "  'sort_order': 4,\n",
       "  'content': '```\\nImportError: DLL load failed while importing cimpl: The specified module could not be found\\n```\\n\\n### Steps to Resolve:\\n\\n1. **Verify Python Version:**\\n   \\n   Ensure you are using a compatible version of Python with the Avro library. Check the Python version and compatibility requirements specified by the Avro library documentation.\\n\\n2. **Load Required DLL:**\\n   \\n   You may need to load `librdkafka-5d2e2910.dll` in your code before importing Avro. Add the following:\\n   \\n   ```python\\n   from ctypes import CDLL\\n\\n   CDLL(\"C:\\\\\\\\Users\\\\\\\\YOUR_USER_NAME\\\\\\\\anaconda3\\\\\\\\envs\\\\\\\\dtcde\\\\\\\\Lib\\\\\\\\site-packages\\\\\\\\confluent_kafka.libs\\\\\\\\librdkafka-5d2e2910.dll\")\\n   ```\\n   \\n   Note that this error may occur depending on the OS and Python version installed.\\n\\n3. **Alternative Solution:**\\n\\n   If you encounter `ImportError: DLL load failed while importing cimpl`, you can try the following solution in PowerShell:\\n\\n   ```bash\\n   $env:CONDA_DLL_SEARCH_MODIFICATION_ENABLE=1\\n   ```\\n\\n   This sets the DLL manually in the Conda environment.\\n\\n### Source:\\n\\n[Confluent-Kafka Python Issue #1186](https://githubhot.com/repo/confluentinc/confluent-kafka-python/issues/1186?page=2)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/004_1cc0ab1fea_error-importing-cimpl-dll-when-running-avro-exampl.md'},\n",
       " {'id': '0a441d9976',\n",
       "  'question': \"ModuleNotFoundError: No module named 'avro'\",\n",
       "  'sort_order': 5,\n",
       "  'content': '### Solution\\n\\nTo resolve the error, install the Avro module using the following command:\\n\\n```bash\\npip install confluent-kafka[avro]\\n```\\n\\nNote: This issue may occur because Conda does not include the Avro module when installing `confluent-kafka` via pip.\\n\\n### Additional Resources\\n\\nFor more information on Anaconda and `confluent-kafka` issues, visit the following links:\\n\\n- [GitHub Issue 590](https://github.com/confluentinc/confluent-kafka-python/issues/590)\\n- [GitHub Issue 1221](https://github.com/confluentinc/confluent-kafka-python/issues/1221)\\n- [StackOverflow Discussion](https://stackoverflow.com/questions/69085157/cannot-import-producer-from-confluent-kafka)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/005_0a441d9976_modulenotfounderror-no-module-named-avro.md'},\n",
       " {'id': '44cc6eaa10',\n",
       "  'question': 'Error while running python3 stream.py worker',\n",
       "  'sort_order': 6,\n",
       "  'content': 'If you get an error while running the command `python3 stream.py worker`, follow these steps:\\n\\n1. Uninstall the current `kafka-python` package:\\n   \\n   ```bash\\n   pip uninstall kafka-python\\n   ```\\n\\n2. Install the specific version of `kafka-python`:\\n\\n   ```bash\\n   pip install kafka-python==1.4.6\\n   ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/006_44cc6eaa10_error-while-running-python3-streampy-worker.md'},\n",
       " {'id': 'f2a0d0c3f0',\n",
       "  'question': 'What is the use of Redpanda?',\n",
       "  'sort_order': 7,\n",
       "  'content': 'Redpanda is built on top of the Raft consensus algorithm and is designed as a high-performance, low-latency alternative to Kafka. It uses a log-centric architecture similar to Kafka but with different underlying principles.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/007_f2a0d0c3f0_what-is-the-use-of-redpanda.md'},\n",
       " {'id': 'b943770904',\n",
       "  'question': 'Negsignal:SIGKILL while converting data files to parquet format',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Got this error because the Docker container memory was exhausted. The data file was up to 800MB but my Docker container does not have enough memory to handle that.\\n\\n**Solution:**\\n\\n- Load the file in chunks with Pandas.\\n- Create multiple parquet files for each data file being processed.\\n\\nThis approach worked smoothly and resolved the issue.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/008_b943770904_negsignalsigkill-while-converting-data-files-to-pa.md'},\n",
       " {'id': 'dcdd7eda4d',\n",
       "  'question': 'resources/rides.csv is missing',\n",
       "  'sort_order': 9,\n",
       "  'content': 'Copy the file found in the Java example: `data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv`',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/009_dcdd7eda4d_resourcesridescsv-is-missing.md'},\n",
       " {'id': '348f696873',\n",
       "  'question': 'Kafka: Python videos have low audio and are hard to follow up',\n",
       "  'sort_order': 10,\n",
       "  'content': 'To improve the audio quality:\\n\\n- **Download the videos** and use VLC media player. You can set the audio to 200% of the original volume for better sound quality.\\n- Alternatively, **use auto-generated captions** directly on YouTube for better clarity.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/010_348f696873_kafka-python-videos-have-low-audio-and-are-hard-to.md'},\n",
       " {'id': '759b5d80ec',\n",
       "  'question': 'Kafka Python Videos: Rides.csv',\n",
       "  'sort_order': 11,\n",
       "  'content': 'There is no clear explanation of the `rides.csv` data that the `producer.py` Python programs use. You can find it here: [Rides CSV File](https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/011_759b5d80ec_kafka-python-videos-ridescsv.md'},\n",
       " {'id': 'ea9c96ab72',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'sort_order': 12,\n",
       "  'content': 'If you encounter this error, it is likely that your Kafka broker Docker container is not running.\\n\\n- Use the following command to check the running containers:\\n\\n  ```bash\\n  docker ps\\n  ```\\n\\n- Navigate to the folder containing your Docker Compose YAML file and execute the following command to start all instances:\\n\\n  ```bash\\n  docker-compose up -d\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/012_ea9c96ab72_kafkaerrorsnobrokersavailable-nobrokersavailable.md'},\n",
       " {'id': '443b9afd04',\n",
       "  'question': 'Kafka homework Q3: There are options that support the scaling concept more than the others.',\n",
       "  'sort_order': 13,\n",
       "  'content': 'Focus on the horizontal scaling option.\\n\\nThink of scaling in terms of scaling from the consumer end, or consuming messages via horizontal scaling.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/013_443b9afd04_kafka-homework-q3-there-are-options-that-support-t.md'},\n",
       " {'id': '664833c280',\n",
       "  'question': \"Docker: How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\",\n",
       "  'sort_order': 14,\n",
       "  'content': 'If you get this error, it means you have not built your Spark and Jupyter images. These images aren’t readily available on DockerHub.\\n\\nTo resolve this:\\n\\n- In the Spark folder, run the following command from a bash CLI to build all images before running docker compose:\\n  \\n  ```bash\\n  ./build.sh\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/014_664833c280_docker-how-to-fix-docker-compose-error-error-respo.md'},\n",
       " {'id': 'e821d8fe04',\n",
       "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
       "  'sort_order': 15,\n",
       "  'content': 'Run this command in the terminal in the same directory (`/docker/spark`):\\n\\n```bash\\nchmod +x build.sh\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/015_e821d8fe04_python-kafka-buildsh-permission-denied-error.md'},\n",
       " {'id': 'b2c1c1d0ec',\n",
       "  'question': 'Python Kafka: ‘KafkaTimeoutError: Failed to update metadata after 60.0 secs.’ when running stream-example/producer.py',\n",
       "  'sort_order': 16,\n",
       "  'content': 'Restarting all services worked for me:\\n\\n```bash\\ndocker-compose down\\n\\ndocker-compose up\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/016_b2c1c1d0ec_python-kafka-kafkatimeouterror-failed-to-update-me.md'},\n",
       " {'id': '477104b863',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.',\n",
       "  'sort_order': 17,\n",
       "  'content': 'While following [tutorial 13.2](https://www.youtube.com/watch?v=5hRJ8-6Fpyk&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=79), when running `./spark-submit.sh streaming.py`, encountered the following error:\\n\\n```\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n```\\n\\n**Solution:**\\n\\n1. **Downgrade PySpark:**\\n   - Downgrade your local PySpark to 3.3.1 (ensure it matches the version used in Dockerfile).\\n   - The mismatch of PySpark versions can be a cause of the failed connection.\\n   - Check the logs of `spark-master` in the Docker container for confirmation.\\n\\n2. **Check Spark Version:**\\n   - Run `pyspark --version` and `spark-submit --version` to check your local Spark version.\\n   - Adjust the `SPARK_VERSION` variable in `build.sh` to match your current Spark version.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/017_477104b863_python-kafka-spark-submitsh-streamingpy-error-stan.md'},\n",
       " {'id': 'e7015b4263',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       "  'sort_order': 18,\n",
       "  'content': \"- Start a new terminal.\\n\\n- Run the following command to list running containers:\\n\\n  ```bash\\n  docker ps\\n  ```\\n\\n- Copy the `CONTAINER ID` of the `spark-master` container.\\n\\n- Execute the following command to access the container's shell:\\n\\n  ```bash\\n  docker exec -it <spark_master_container_id> bash\\n  ```\\n\\n- Run this command to view the Spark master logs:\\n\\n  ```bash\\n  cat logs/spark-master.out\\n  ```\\n\\n- Check the log for the timestamp when the error occurred.\\n\\n- Search the error message online for further troubleshooting.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/018_e7015b4263_python-kafka-spark-submitsh-streamingpy-how-to-che.md'},\n",
       " {'id': '0e8bce921a',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.',\n",
       "  'sort_order': 19,\n",
       "  'content': \"Make sure your Java version is 11 or 8.\\n\\n- Check your version by:\\n\\n  ```bash\\n  java --version\\n  ```\\n\\n- Check all your installed Java versions by:\\n\\n  ```bash\\n  /usr/libexec/java_home -V\\n  ```\\n\\n- If you already have Java 11 but it's not set as the default, select the specific version by:\\n\\n  ```bash\\n  export JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n  ```\\n\\n  (or another version of 11)\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/019_0e8bce921a_python-kafka-spark-submitsh-streamingpy-error-py4j.md'},\n",
       " {'id': 'd01cbfa9cb',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'sort_order': 20,\n",
       "  'content': 'In my setup, all of the dependencies listed in `build.gradle` were not installed in `<project_name>-1.0-SNAPSHOT.jar`.\\n\\nSolution:\\n\\n1. In the `build.gradle` file, add the following at the end:\\n   \\n   ```groovy\\n   shadowJar {\\n       archiveBaseName = \"java-kafka-rides\"\\n       archiveClassifier = \\'\\'\\n   }\\n   ```\\n\\n2. In the command line, run:\\n   \\n   ```bash\\n   gradle shadowjar\\n   ```\\n\\n3. Execute the script from `java-kafka-rides-1.0-SNAPSHOT.jar` created by the shadowjar.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/020_d01cbfa9cb_java-kafka-project_name-10-snapshotjar-errors-pack.md'},\n",
       " {'id': '2763850d3e',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'sort_order': 21,\n",
       "  'content': 'To install the necessary dependencies for running `producer.py` in the `avro_example` directory, use the following commands:\\n\\n- Install `confluent-kafka`:\\n  - Using pip:\\n  ```bash\\n  pip install confluent-kafka\\n  ```\\n  - Using conda:\\n  ```bash\\n  conda install conda-forge::python-confluent-kafka\\n  ```\\n\\n- Install `fastavro`:\\n  ```bash\\n  pip install fastavro\\n  ```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/021_2763850d3e_python-kafka-installing-dependencies-for-python3-0.md'},\n",
       " {'id': '2d274b7acd',\n",
       "  'question': 'Can I install the Faust Library for Module 6 Python Version due to dependency conflicts?',\n",
       "  'sort_order': 22,\n",
       "  'content': 'The Faust repository and library is no longer maintained - [GitHub Repository](https://github.com/robinhood/faust)\\n\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here: [YouTube Playlist](https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80). Follow the RedPanda Python version here: [RedPanda Example](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example).\\n\\n**Note:** I highly recommend watching the Java videos to understand the concept of streaming, but you can skip the coding parts. All will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/022_2d274b7acd_can-i-install-the-faust-library-for-module-6-pytho.md'},\n",
       " {'id': '5ca6890c1a',\n",
       "  'question': 'Java Kafka: How to run producer/consumer/kstreams/etc in terminal',\n",
       "  'sort_order': 23,\n",
       "  'content': 'In the project directory, run:\\n\\n```bash\\njava -cp build/libs/<jar_name>-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/023_5ca6890c1a_java-kafka-how-to-run-producerconsumerkstreamsetc.md'},\n",
       " {'id': 'cd8a62fc55',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'sort_order': 24,\n",
       "  'content': 'For example, when running `JsonConsumer.java`, you might see:\\n\\n```\\nConsuming form kafka started\\n\\nRESULTS:::0\\n\\nRESULTS:::0\\n\\nRESULTS:::0\\n```\\n\\nOr when running `JsonProducer.java`, you might encounter:\\n\\n```\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\n```\\n\\n**Solution:**\\n\\n1. Ensure the `StreamsConfig.BOOTSTRAP_SERVERS_CONFIG` in the scripts located at `src/main/java/org/example/` (e.g., `JsonConsumer.java`, `JsonProducer.java`) is pointing to the correct server URL (e.g., `europe-west3` vs `europe-west2`).\\n\\n2. Verify that the cluster key and secrets are updated in `src/main/java/org/example/Secrets.java` (`KAFKA_CLUSTER_KEY` and `KAFKA_CLUSTER_SECRET`).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/024_cd8a62fc55_java-kafka-when-running-the-producerconsumeretc-ja.md'},\n",
       " {'id': 'd039adfb76',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_07f863ec.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_df52218e.png'},\n",
       "   {'description': 'image #3',\n",
       "    'id': 'image_3',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_deaf91b1.png'},\n",
       "   {'description': 'image #4',\n",
       "    'id': 'image_4',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_8584c5ce.png'}],\n",
       "  'question': 'Java Kafka: Tests are not picked up in VSCode',\n",
       "  'sort_order': 25,\n",
       "  'content': \"In VS Code, you might expect to see a triangle icon next to each test method in your Java files. If you don't see it, here are the steps to fix the issue:\\n\\n1. Open **Explorer** (first icon on the left navigation bar).\\n2. Navigate to **JAVA PROJECTS** (bottom collapsable section).\\n3. Click the icon in the rightmost position next to **JAVA PROJECTS** to open the options.\\n   \\n   <{IMAGE:image_1}>\\n\\n   <{IMAGE:image_2}>\\n   \\n   <{IMAGE:image_3}>\\n\\n4. Select **Clean Workspace**.\\n5. Confirm by clicking **Reload and Delete**.\\n\\nFollowing these steps should restore the triangle icons you expect to see next to each test, similar to those visible in Python tests.\\n\\nExample:\\n\\n<{IMAGE:image_4}>\\n\\nAdditionally, you can add classes and packages in this window instead of creating files directly in the project directory.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/025_d039adfb76_java-kafka-tests-are-not-picked-up-in-vscode.md'},\n",
       " {'id': '30fbb4f5b8',\n",
       "  'question': 'Confluent Kafka: Where can I find schema registry URL?',\n",
       "  'sort_order': 26,\n",
       "  'content': 'In [Confluent Cloud](https://confluent.cloud/):\\n\\n- Navigate to your Environment (e.g., default or a custom name).\\n- Use the right navigation bar to find \"Stream Governance API.\"\\n- The URL can be found under \"Endpoint.\"\\n- Create credentials from the Credentials section below it.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/026_30fbb4f5b8_confluent-kafka-where-can-i-find-schema-registry-u.md'},\n",
       " {'id': '663c1e915e',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'sort_order': 27,\n",
       "  'content': 'You can check the version of your local Spark using:\\n\\n```bash\\nspark-submit --version\\n```\\n\\nIn the `build.sh` file of the Python folder, ensure that `SPARK_VERSION` matches your local version. Similarly, ensure the PySpark you installed via pip also matches this version.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/027_663c1e915e_how-do-i-check-compatibility-of-local-and-containe.md'},\n",
       " {'id': '7c3cf3b8fc',\n",
       "  'question': 'How to fix the error \"ModuleNotFoundError: No module named \\'kafka.vendor.six.moves\\'?\"',\n",
       "  'sort_order': 28,\n",
       "  'content': 'According to [GitHub](https://github.com/dpkp/kafka-python/):\\n\\n\"DUE TO ISSUES WITH RELEASES, IT IS SUGGESTED TO USE [GitHub](https://github.com/wbarnha/kafka-python-ng) FOR THE TIME BEING.\"\\n\\nUse the following command to install:\\n\\n```bash\\npip install kafka-python-ng\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/028_7c3cf3b8fc_how-to-fix-the-error-modulenotfounderror-no-module.md'},\n",
       " {'id': '8fe89183d7',\n",
       "  'question': 'How to fix “connection failed: connection to server at \"127.0.0.1\", port 5432 failed” error when setting up Postgres connection in pgAdmin?',\n",
       "  'sort_order': 29,\n",
       "  'content': \"Instead of using “localhost” as the host name/address, try “postgres”, or “host.docker.internal” instead.\\n\\nAlternative Solution:\\n\\n- If you have installed Postgres locally and disabled persist data on the Postgres container in Docker (i.e., volume: removed), use a Postgres port other than 5432, such as 5433.\\n- For the pgAdmin host name/address, if 'localhost', 'postgres', or 'host.docker.internal' are not working, you can use your own IPv4 Address.\\n\\n  To find your IPv4 Address on Windows OS:\\n  \\n  1. Open Command Prompt.\\n  2. Run the command:\\n     \\n     ```bash\\n     ipconfig\\n     ```\\n  \\n  3. Look under Wireless LAN adapter WiFi 2 for the IPv4 Address. For example:\\n\\n     ```text\\n     IPv4 Address. . . . . . . . . . . : 192.168.0.148\\n     ```\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/module-6/029_8fe89183d7_how-to-fix-connection-failed-connection-to-server.md'},\n",
       " {'id': '56d8f7ae9a',\n",
       "  'question': 'Why is my table not being created in PostgreSQL when I submit a job?',\n",
       "  'sort_order': 1,\n",
       "  'content': \"There could be a few reasons for this issue:\\n\\n- **Race Conditions**: If you're running multiple processes in parallel.\\n\\n- **Database Connection Issues**: The job might not be connecting to the correct PostgreSQL database, or there could be authentication or permission issues preventing table creation.\\n\\n- **Missing Table Creation Logic**: The code responsible for creating the table might not be properly included or executed in the job submission process.\\n\\nAs a best practice, it's generally recommended to pre-create tables in PostgreSQL to avoid runtime errors. This ensures the database schema is properly set up before any jobs are executed.\\n\\nExtra: Use `CREATE TABLE IF NOT EXISTS` in your code. This will prevent errors if the table already exists and ensure smooth job execution.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/001_56d8f7ae9a_why-is-my-table-not-being-created-in-postgresql-wh.md'},\n",
       " {'id': 'ab1b812541',\n",
       "  'question': 'How is my capstone project going to be evaluated?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'Each submitted project will be evaluated by three randomly assigned students who have also submitted the project.\\n\\nYou will also be responsible for grading projects from three fellow students yourself. Please note that not complying with this rule will result in failing to achieve the Certificate at the end of the course.\\n\\nThe final grade you receive will be the median score of the grades from peer reviewers.\\n\\nThe peer review criteria for evaluating or being evaluated must follow the guidelines defined [here](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/week_7_project#peer-review-criteria).',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/002_ab1b812541_how-is-my-capstone-project-going-to-be-evaluated.md'},\n",
       " {'id': '5a491cd847',\n",
       "  'question': 'Can I collaborate with others on the capstone project?',\n",
       "  'sort_order': 3,\n",
       "  'content': 'Collaboration is not allowed for the capstone submission. However, you can discuss ideas and get feedback from peers in the forums or Slack channels.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/003_5a491cd847_can-i-collaborate-with-others-on-the-capstone-proj.md'},\n",
       " {'id': '5c5321c295',\n",
       "  'question': 'Project 1 & Project 2',\n",
       "  'sort_order': 4,\n",
       "  'content': 'There is only ONE project for this Zoomcamp. You do not need to submit or create two projects.\\n\\nThere are simply TWO chances to pass the course. You can use the Second Attempt if you:\\n\\n- Fail the first attempt\\n- Do not have the time due to other engagements such as holidays or sickness to enter your project into the first attempt.\\n\\n**Project Evaluation - Reproducibility**\\n\\nEven with thorough documentation, ensuring that a peer reviewer can follow your steps can be challenging. Here’s how this criterion will be evaluated:\\n\\n> \"Ideally yes, you should try to re-run everything. But I understand that not everyone has time to do it, so if you check the code by looking at it and try to spot errors, places with missing instructions, and so on - then it\\'s already great.\"\\n\\n**Certificates: How do I get it?**\\n\\nSee the `certificate.mdx` file.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/004_5c5321c295_project-1-project-2.md'},\n",
       " {'id': '2220285e58',\n",
       "  'question': 'Does anyone know nice and relatively large datasets?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'See a list of datasets here: [GitHub Datasets](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/projects/datasets.md)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/005_2220285e58_does-anyone-know-nice-and-relatively-large-dataset.md'},\n",
       " {'id': 'a3776dc060',\n",
       "  'question': 'How to run Python as a startup script?',\n",
       "  'sort_order': 6,\n",
       "  'content': 'You need to redefine the Python environment variable to that of your user account.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/006_a3776dc060_how-to-run-python-as-a-startup-script.md'},\n",
       " {'id': 'ce77e05d24',\n",
       "  'question': 'Spark Streaming: How do I read from multiple topics in the same Spark Session',\n",
       "  'sort_order': 7,\n",
       "  'content': 'To read from multiple topics in the same Spark session, follow these steps:\\n\\n1. **Initiate a Spark Session:**\\n   \\n   ```python\\n   spark = (SparkSession\\n       .builder\\n       .appName(app_name)\\n       .master(master=master)\\n       .getOrCreate())\\n   \\n   spark.streams.resetTerminated()\\n   ```\\n\\n2. **Read Streams from Multiple Topics:**\\n   \\n   ```python\\n   query1 = spark\\n       .readStream\\n       ...\\n       ...\\n       .load()\\n   \\n   query2 = spark\\n       .readStream\\n       ...\\n       ...\\n       .load()\\n   \\n   query3 = spark\\n       .readStream\\n       ...\\n       ...\\n       .load()\\n   ```\\n\\n3. **Start the Queries:**\\n   \\n   ```python\\n   query1.start()\\n   query2.start()\\n   query3.start()\\n   ```\\n\\n4. **Await Termination:**\\n   \\n   ```python\\n   spark.streams.awaitAnyTermination()  # Waits for any one of the queries to receive a kill signal or error failure. This is asynchronous.\\n   ```\\n\\n   Note: `query3.start().awaitTermination()` is a blocking call. It works well when we are reading only from one topic.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/007_ce77e05d24_spark-streaming-how-do-i-read-from-multiple-topics.md'},\n",
       " {'id': '1c7bfa9357',\n",
       "  'question': 'Data Transformation from Databricks to Azure SQL DB',\n",
       "  'sort_order': 8,\n",
       "  'content': 'Transformed data can be moved into Azure Blob Storage and then it can be moved into Azure SQL DB, instead of moving directly from Databricks to Azure SQL DB.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/008_1c7bfa9357_data-transformation-from-databricks-to-azure-sql-d.md'},\n",
       " {'id': 'c81e613d5a',\n",
       "  'question': 'Orchestrating dbt with Airflow',\n",
       "  'sort_order': 9,\n",
       "  'content': 'The trial dbt account provides access to the dbt API. A job will still need to be added manually. Airflow can run the job using a Python operator that calls the API. You will need to provide an API key, job ID, etc., and be careful not to commit this information to GitHub.\\n\\n- Detailed explanation: [dbt and Airflow Spiritual Alignment](https://docs.getdbt.com/blog/dbt-airflow-spiritual-alignment)\\n- Source code example: [GitHub dbt Cloud Example](https://github.com/sungchun12/airflow-toolkit/blob/95d40ac76122de337e1b1cdc8eed35ba1c3051ed/dags/examples/dbt_cloud_example.py)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/009_c81e613d5a_orchestrating-dbt-with-airflow.md'},\n",
       " {'id': 'ec1001476f',\n",
       "  'question': 'Orchestrating DataProc with Airflow',\n",
       "  'sort_order': 10,\n",
       "  'content': 'For orchestrating DataProc with Airflow, you can refer to the following documentation:\\n\\n- [Airflow DataProc Operators - API Docs](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html)\\n- [Airflow DataProc Operators - Module Docs](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/airflow/providers/google/cloud/operators/dataproc.html)\\n\\n### Roles for Service Account\\n\\nEnsure that you assign the following roles to your service account:\\n\\n- **DataProc Administrator**\\n- **Service Account User**\\n  \\n  For more details, see the explanation on [Stack Overflow](https://stackoverflow.com/questions/63941429/user-not-authorized-to-act-as-service-account-when-using-workload-identity).\\n\\n### Operators to Use\\n\\n- `DataprocSubmitPySparkJobOperator`\\n- `DataprocDeleteClusterOperator`\\n- `DataprocCreateClusterOperator`\\n\\n### Important Note\\n\\nWhen using `DataprocSubmitPySparkJobOperator`, make sure to add the BigQuery Connector, as DataProc does not include it by default:\\n\\n```python\\n  dataproc_jars = [\"gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-0.24.0.jar\"]\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/010_ec1001476f_orchestrating-dataproc-with-airflow.md'},\n",
       " {'id': '6c9785b291',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_981381c8.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_ebc771b3.png'}],\n",
       "  'question': 'Orchestrating dbt cloud with Mage',\n",
       "  'sort_order': 11,\n",
       "  'content': 'You can trigger your dbt job in a Mage pipeline. Follow these steps:\\n\\n1. Retrieve your dbt cloud API key from **Settings > API Tokens > Personal Tokens**. Add this key safely to your `.env` file. For example:\\n\\n   ```bash\\n   dbt_api_trigger=dbt_**\\n   ```\\n\\n2. Navigate to the job page in dbt cloud and find the API trigger link.\\n\\n3. Create a custom Mage Python block with a simple HTTP request, as shown in [this example](https://github.com/Nogromi/ukraine-vaccinations/blob/master/2_mage/vaccination/custom/trigger_dbt_cloud.py).\\n\\n   <{IMAGE:image_1}>\\n\\n   <{IMAGE:image_2}>\\n\\n4. Use the following script to trigger the dbt job:\\n\\n   ```python\\n   from dotenv import load_dotenv\\n   from pathlib import Path\\n\\n   dotenv_path = Path(\\'/home/src/.env\\')\\n   load_dotenv(dotenv_path=dotenv_path)\\n\\n   dbt_api_trigger = os.getenv(\\'dbt_api_trigger\\')\\n   \\n   url = f\"https://cloud.getdbt.com/api/v2/accounts/{dbt_account_id}/jobs/<job_id>/run/\"\\n\\n   headers = {\\n       \"Authorization\": f\"Token {dbt_api_trigger}\",\\n       \"Content-Type\": \"application/json\"\\n   }\\n\\n   body = {\\n       \"cause\": \"Triggered via API\"\\n   }\\n\\n   response = requests.post(url, headers=headers, json=body)\\n   ```\\n\\n   Voilà! You\\'ve triggered a dbt job from your Mage pipeline.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/011_6c9785b291_orchestrating-dbt-cloud-with-mage.md'},\n",
       " {'id': 'ec5e405fa6',\n",
       "  'question': 'Key Vault in Azure cloud stack',\n",
       "  'sort_order': 12,\n",
       "  'content': 'The Key Vault in Azure Cloud is used to store credentials, passwords, or secrets for different technologies used within Azure. For example, if you do not want to expose the password of an SQL database, you can save the password under a given name and use it in other Azure services.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/012_ec5e405fa6_key-vault-in-azure-cloud-stack.md'},\n",
       " {'id': 'b3bb998ae2',\n",
       "  'question': 'How to connect Pyspark with BigQuery?',\n",
       "  'sort_order': 13,\n",
       "  'content': 'To connect Pyspark with BigQuery, include the following line in the Pyspark configuration:\\n\\n```python\\n# Example initialization of SparkSession variable\\n\\nspark = (SparkSession.builder\\n\\n    .master(\"...\")\\n    .appName(\"...\")\\n    \\n    # Add the following configuration\\n    .config(\"spark.jars.packages\", \"com.google.cloud.spark:spark-3.5-bigquery:0.37.0\")\\n)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/013_b3bb998ae2_how-to-connect-pyspark-with-bigquery.md'},\n",
       " {'id': 'e839b64165',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_bd4861e1.png'}],\n",
       "  'question': 'How to run a dbt-core project as an Airflow Task Group on Google Cloud Composer using a service account JSON key',\n",
       "  'sort_order': 14,\n",
       "  'content': '1. Install the [astronomer-cosmos](https://github.com/astronomer/astronomer-cosmos) package as a dependency. Refer to the installation guide [here](https://cloud.google.com/composer/docs/composer-2/install-python-dependencies#install_custom_packages_in_a_environment) and see a Terraform [example](https://github.com/wndrlxx/ca-trademarks-data-pipeline/blob/4e6a0e757495a99e01ff6c8b981a23d6dc421046/terraform/main.tf#L100).\\n\\n2. Create a new folder, `dbt/`, inside the `dags/` folder of your Composer GCP bucket and copy your dbt-core project there. See the [example](https://github.com/wndrlxx/ca-trademarks-data-pipeline/tree/4e6a0e757495a99e01ff6c8b981a23d6dc421046/dags/dbt/ca_trademarks_dp).\\n\\n3. Ensure your `profiles.yml` is configured to authenticate with a service account key. Refer to the BigQuery [example](https://docs.getdbt.com/docs/core/connect-data-platform/bigquery-setup#service-account-file).\\n\\n4. Create a new DAG using the `DbtTaskGroup` class. Use a `ProfileConfig` specifying a `profiles_yml_filepath` that points to the location of your JSON key file. See this [example](https://github.com/wndrlxx/ca-trademarks-data-pipeline/blob/4e6a0e757495a99e01ff6c8b981a23d6dc421046/dags/6_dbt_cosmos_task_group.py#L47).\\n\\nYour dbt lineage graph should now appear as tasks inside a task group like this:\\n\\n<{IMAGE:image_1}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/014_e839b64165_how-to-run-a-dbt-core-project-as-an-airflow-task-g.md'},\n",
       " {'id': '13c4d27a91',\n",
       "  'question': 'How can I run UV in Kestra without installing it on every flow execution?',\n",
       "  'sort_order': 15,\n",
       "  'content': \"To avoid reinstalling `uv` on each flow run, you can create a custom Docker image based on the official Kestra image with `uv` pre-installed. Here's how:\\n\\n1. **Create a Dockerfile (e.g., Dockerfile) with the following content:**\\n\\n   ```dockerfile\\n   # Use the official Kestra image as a base\\n   FROM kestra/kestra\\n   \\n   # Install uv\\n   RUN pip install uv\\n   ```\\n\\n2. **Update your `docker-compose.yml` to build this custom image instead of pulling the default one:**\\n\\n   ```yaml\\n   services:\\n     kestra:\\n       build:\\n         context: .\\n         dockerfile: Dockerfile\\n       image: custom-kestra\\n   ```\\n\\nThis approach ensures that `uv` is available in the container at runtime without requiring installation during each flow execution.\",\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/015_13c4d27a91_how-can-i-run-uv-in-kestra-without-installing-it-o.md'},\n",
       " {'id': '7ece5b3182',\n",
       "  'question': 'Is it possible to create external tables in BigQuery using URLs, such as those from the NY Taxi data website?',\n",
       "  'sort_order': 16,\n",
       "  'content': 'Not really, only Bigtable, Cloud Storage, and Google Drive are supported data stores.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/016_7ece5b3182_is-it-possible-to-create-external-tables-in-bigque.md'},\n",
       " {'id': 'c0c68e1ee8',\n",
       "  'question': 'Is it ok to use NY_Taxi data for the project?',\n",
       "  'sort_order': 17,\n",
       "  'content': 'No.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/017_c0c68e1ee8_is-it-ok-to-use-ny_taxi-data-for-the-project.md'},\n",
       " {'id': 'df38a5d809',\n",
       "  'question': 'How to use dbt-core with Athena?',\n",
       "  'sort_order': 18,\n",
       "  'content': 'If you don’t have access to dbt Cloud, which is natively supported by AWS, you can use the community-built [dbt-Athena Adapter](https://dbt-athena.github.io/) for dbt-Core. Here are some references:\\n\\n- [AWS Blog](https://aws.amazon.com/blogs/big-data/from-data-lakes-to-insights-dbt-adapter-for-amazon-athena-now-supported-in-dbt-cloud/)\\n- [YouTube Tutorial](https://youtu.be/JEizJAaaBkg?si=niTYdWoeiyC_w3h7)\\n- [dbt Guides: Athena](https://docs.getdbt.com/guides/athena?step=1)\\n- [dbt Athena Setup Documentation](https://docs.getdbt.com/docs/core/connect-data-platform/athena-setup)\\n\\n### Key Features:\\n\\n- Enables dbt to work with AWS Athena using dbt Core\\n- Allows data transformation using `CREATE TABLE AS` or `CREATE VIEW` SQL queries\\n\\n### Not Yet Supported Features:\\n\\n- Python models\\n- Persisting documentation for views\\n\\nThis adapter can be a valuable resource for those who need to work with Athena using dbt Core.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/project/018_df38a5d809_how-to-use-dbt-core-with-athena.md'},\n",
       " {'id': '70072fcf7a',\n",
       "  'question': 'Solving dbt-Athena library conflicts',\n",
       "  'sort_order': 1,\n",
       "  'content': 'When working on a dbt-Athena project, do not install `dbt-athena-adapter`. Instead, always use the `dbt-athena-community` package, ensuring it matches the version of `dbt-core` to avoid compatibility conflicts.\\n\\n### Best Practice\\n\\n- **Always pin the versions of `dbt-core` and `dbt-athena-community` to the same version.**\\n  \\n  ```\\n  Example: dbt-core~=1.9.3 dbt-athena-community~=1.9.3\\n  ```\\n\\n### Why?\\n\\n- `dbt-athena-adapter` is outdated and no longer maintained.\\n- `dbt-athena-community` is the actively maintained package and is compatible with the latest versions of `dbt-core`.\\n\\n### Steps to Avoid Conflicts\\n\\n1. **Check the compatibility matrix** in the [dbt-athena-community](https://github.com/dbt-labs/dbt-adapters/tree/main/dbt-athena-community) GitHub repository.\\n2. **Update `requirements.txt`** to use the latest compatible versions of `dbt-core` and `dbt-athena-community`.\\n3. **Avoid mixing** `dbt-athena-adapter` with `dbt-athena-community` in the same environment.\\n\\nBy following this practice, you can avoid the conflicts we faced previously and ensure a smooth development experience.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/001_70072fcf7a_solving-dbt-athena-library-conflicts.md'},\n",
       " {'id': 'd7abe28e77',\n",
       "  'question': 'Which set-up should I use for my dlt homework?',\n",
       "  'sort_order': 2,\n",
       "  'content': 'Technically you can use any code editor or Jupyter Notebook, as long as you can run dbt and answer the homework questions. A lot of code is provided by the instructor on the homework page to give you a head start in the right direction: [dlt Homework Instructions](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/cohorts/2025/workshops/dlt/dlt_homework.md).\\n\\nThe most practical way is to use the provided Colabs Jupyter notebook called ‘dlt - Homework.ipynb’ which you can find here: [Colab Notebook](https://colab.research.google.com/drive/1plqdl33K_HkVx0E0nGJrrkEUssStQsW7#scrollTo=BtsSxtFfXQs3) since all of the provided code is applicable in the Colabs set-up.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/002_d7abe28e77_which-set-up-should-i-use-for-my-dlt-homework.md'},\n",
       " {'id': '122d2b0aed',\n",
       "  'question': 'How do I install the necessary dependencies to run the code?',\n",
       "  'sort_order': 3,\n",
       "  'content': 'To run the provided code, ensure that the `dlt[duckdb]` package is installed. You can do this by executing the following installation command in a Jupyter notebook:\\n\\n```bash\\n!pip install dlt[duckdb]\\n```\\n\\nIf you’re installing it locally, make sure to also have `duckdb` installed before the `duckdb` package is loaded:\\n\\n```zsh\\npip install \"dlt[duckdb]\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/003_122d2b0aed_how-do-i-install-the-necessary-dependencies-to-run.md'},\n",
       " {'id': 'ee564fdf82',\n",
       "  'question': 'Other packages needed but not listed',\n",
       "  'sort_order': 4,\n",
       "  'content': 'If you are running Jupyter Notebook on a fresh new Codespace or in a local machine with a new virtual environment, you will need these packages to run the starter Jupyter Notebook offered by the teacher. Execute this command to install all the necessary dependencies:\\n\\n```bash\\npip install duckdb pandas numpy pyarrow\\n```\\n\\nOr save it into a `requirements.txt` file:\\n\\n```\\ndlt[duckdb]\\nduckdb\\npandas\\nnumpy\\npyarrow  # Optional, needed for Parquet support\\n```\\n\\nThen run:\\n\\n```bash\\npip install -r requirements.txt\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/004_ee564fdf82_other-packages-needed-but-not-listed.md'},\n",
       " {'id': '60b8576833',\n",
       "  'question': 'How can I use DuckDB In-Memory database with dlt?',\n",
       "  'sort_order': 5,\n",
       "  'content': 'Alternatively, you can switch to in-file storage with:',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/005_60b8576833_how-can-i-use-duckdb-in-memory-database-with-dlt.md'},\n",
       " {'id': '5b5d82a924',\n",
       "  'images': [{'description': 'image #1',\n",
       "    'id': 'image_1',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_4c7b9e6b.png'},\n",
       "   {'description': 'image #2',\n",
       "    'id': 'image_2',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_b4d5ed3c.png'},\n",
       "   {'description': 'image #3',\n",
       "    'id': 'image_3',\n",
       "    'path': 'images/data-engineering-zoomcamp/image_2ba9606d.png'}],\n",
       "  'question': 'Homework: dlt Exercise 3 - Merge a generator concerns',\n",
       "  'sort_order': 6,\n",
       "  'content': 'After loading, you should have a total of 8 records, and ID 3 should have age 33.\\n\\n**Question:** Calculate the sum of ages of all the people loaded as described above.\\n\\n- The sum of all eight records\\' respective ages is too big to be in the choices.\\n- You need to first filter out the people whose occupation is equal to `None` in order to get an answer that is close to or present in the given choices. \\n\\n---\\n\\n### Issue:\\n\\nI\\'m having an issue with the DLT workshop notebook, specifically in the \\'Load to Parquet file\\' section. No matter what I change the file path to, it\\'s still saving the DLT files directly to my C drive.\\n\\n### Solution:\\n\\nUse a raw string and keep the `file:///` at the start of your file path.\\n\\n```python\\n# Set the bucket_url. We can also use a local folder\\nos.environ[\\'DESTINATION__FILESYSTEM__BUCKET_URL\\'] = r\\'file:///content/.dlt/my_folder\\'\\n\\nurl = \"https://storage.googleapis.com/dtc_zoomcamp_api/yellow_tripdata_2009-06.jsonl\"\\n\\n# Define your pipeline\\npipeline = dlt.pipeline(\\n    pipeline_name=\\'my_pipeline\\',\\n    destination=\\'filesystem\\',\\n    dataset_name=\\'mydata\\'\\n)\\n\\n# Run the pipeline with the generator we created earlier.\\nload_info = pipeline.run(stream_download_jsonl(url), table_name=\"users\", loader_file_format=\"parquet\")\\n\\nprint(load_info)\\n\\n# Get a list of all Parquet files in the specified folder\\nparquet_files = glob.glob(\\'/content/.dlt/my_folder/mydata/users/*.parquet\\')\\n\\n# Show Parquet files\\nfor file in parquet_files:\\n    print(file)\\n```\\n\\n<{IMAGE:image_1}>\\n\\n<{IMAGE:image_2}>\\n\\n<{IMAGE:image_3}>',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/006_5b5d82a924_homework-dlt-exercise-3-merge-a-generator-concerns.md'},\n",
       " {'id': '27fa940238',\n",
       "  'question': 'Problem with importing the dlt or dlt.sources module',\n",
       "  'sort_order': 7,\n",
       "  'content': 'Make sure you don’t have a `dlt.py` file saved in the same directory as your working file.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/007_27fa940238_problem-with-importing-the-dlt-or-dltsources-modul.md'},\n",
       " {'id': '4ecc97beb5',\n",
       "  'question': 'How to set credentials in Google Colab notebook to connect to BigQuery',\n",
       "  'sort_order': 8,\n",
       "  'content': 'In the secrets sidebar, create a secret `BIGQUERY_CREDENTIALS` with the value being your Google Cloud service account key. Then load it with:\\n\\n```python\\nimport os\\nfrom google.colab import userdata\\n\\nos.environ[\"DESTINATION__BIGQUERY__CREDENTIALS\"] = userdata.get(\\'BIGQUERY_CREDENTIALS\\')\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/008_4ecc97beb5_how-to-set-credentials-in-google-colab-notebook-to.md'},\n",
       " {'id': 'd2add610e3',\n",
       "  'question': 'How do I set up credentials to run dlt in my environment (not Google Colab)?',\n",
       "  'sort_order': 9,\n",
       "  'content': 'You can set up credentials for `dlt` in several ways. Here are the two most common methods:\\n\\n### Environment Variables (Easiest)\\n\\nSet credentials via environment variables. For example, to configure Google Cloud credentials. This method avoids hardcoding secrets in your code and works seamlessly with most environments.\\n\\n### Configuration Files (Recommended for Local Use)\\n\\n- Use `.dlt/secrets.toml` for sensitive credentials and `.dlt/config.toml` for non-sensitive configurations.\\n- Example for Google Cloud in `secrets.toml`:\\n\\n```toml\\n[google_cloud]\\nservice_account_key = \"YOUR_SERVICE_ACCOUNT_KEY\"\\n```\\n\\n- Place these files in the `.dlt` folder of your project.\\n\\n### Additional Notes:\\n\\n- Never commit `secrets.toml` to version control (add it to `.gitignore`).\\n- Credentials can also be loaded via vaults, AWS Parameter Store, or custom setups.\\n\\nFor additional methods and detailed information, refer to the [official dlt documentation](https://dlthub.com/docs/general-usage/credentials/)',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/009_d2add610e3_how-do-i-set-up-credentials-to-run-dlt-in-my-envir.md'},\n",
       " {'id': '1075e9e2b6',\n",
       "  'question': 'Make DLT comply with the XDG Base Dir Specification',\n",
       "  'sort_order': 10,\n",
       "  'content': 'You can set the environment variable in your shell init script:\\n\\nFor Bash or ZSH:\\n\\n```bash\\nexport DLT_DATA_DIR=$XDG_DATA_HOME/dlt\\n```\\n\\nFor Fish (in `config.fish`):\\n\\n```bash\\nset -x DLT_DATA_DIR \"$XDG_DATA_HOME/dlt\"\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/010_1075e9e2b6_make-dlt-comply-with-the-xdg-base-dir-specificatio.md'},\n",
       " {'id': 'f48b484cd4',\n",
       "  'question': 'Embedding dlt into Apache Airflow',\n",
       "  'sort_order': 11,\n",
       "  'content': 'To integrate a `dlt` pipeline into Apache Airflow, follow this example:\\n\\n```python\\nfrom airflow import DAG\\nfrom airflow.operators.python import PythonOperator\\nfrom datetime import datetime, timedelta\\nimport dlt\\nfrom my_dlt_pipeline import load_data  # Import your dlt pipeline function\\n\\ndefault_args = {\\n    \"owner\": \"airflow\",\\n    \"depends_on_past\": False,\\n    \"start_date\": datetime(2024, 2, 16),\\n    \"retries\": 1,\\n    \"retry_delay\": timedelta(minutes=5),\\n}\\n\\ndef run_dlt_pipeline():\\n    pipeline = dlt.pipeline(\\n        pipeline_name=\"my_pipeline\",\\n        destination=\"duckdb\",  # Change this based on your database\\n        dataset_name=\"my_dataset\"\\n    )\\n    info = pipeline.run(load_data())\\n    print(info)  # Logs for debugging\\n\\nwith DAG(\\n    \"dlt_airflow_pipeline\",\\n    default_args=default_args,\\n    schedule_interval=\"@daily\",\\n    catchup=False,\\n) as dag:\\n    run_dlt_task = PythonOperator(\\n        task_id=\"run_dlt_pipeline\",\\n        python_callable=run_dlt_pipeline,\\n    )\\n    run_dlt_task\\n```\\n\\nEnsure to replace `\"duckdb\"` with your actual database name and adjust the `load_data` function according to your specific `dlt` pipeline.',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/011_f48b484cd4_embedding-dlt-into-apache-airflow.md'},\n",
       " {'id': 'eaae17dfea',\n",
       "  'question': 'Embedding dlt into Kestra',\n",
       "  'sort_order': 12,\n",
       "  'content': '```yaml\\nid: dlt_ingestion\\n\\nnamespace: my.dlt\\n\\ndescription: \"Run dlt pipeline with Kestra\"\\n\\ntasks:\\n\\n- id: run_dlt\\n\\n  type: io.kestra.plugin.scripts.python.Commands\\n\\n  commands:\\n\\n  - |\\n\\n    import dlt\\n\\n    from my_dlt_pipeline import load_data  # Import your dlt function\\n\\n    pipeline = dlt.pipeline(\\n\\n      pipeline_name=\"kestra_pipeline\",\\n\\n      destination=\"duckdb\",\\n\\n      dataset_name=\"kestra_dataset\"\\n\\n    )\\n\\n    info = pipeline.run(load_data())\\n\\n    print(info)\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/012_eaae17dfea_embedding-dlt-into-kestra.md'},\n",
       " {'id': '7be69bdcb4',\n",
       "  'question': 'Loading Dlt Exports from GCS Filesystems',\n",
       "  'sort_order': 13,\n",
       "  'content': 'When using the filesystem destination, you may have issues reading the files exported because DLT will by default compress the files. If you are using `loader_file_format=\"parquet\"` then BigQuery should cope with this compression OK. If you want to use JSONL or CSV format, however, you may need to disable file compression to avoid issues with reading the files directly in BigQuery. To do this, set the following config:\\n\\n```bash\\n[normalize.data_writer]\\ndisable_compression = true\\n```\\n\\nThere is further information at [DLTHub Docs](https://dlthub.com/docs/dlt-ecosystem/destinations/filesystem#file-compression).\\n\\n**Warning:**\\n```yaml\\nTest \\'test.taxi_rides_ny.relationships_stg_yellow_tripdata_dropoff_locationid__locationid__ref_taxi_zone_lookup_csv_.085c4830e7\\' (models/staging/schema.yml) depends on a node named \\'taxi_zone_lookup.csv\\' in package \\'\\' which was not found\\n```\\n\\n**Solution:** This warning indicates that dbt is trying to reference a model or source named `taxi_zone_lookup.csv`, but it cannot find it. We might have a typo in our `ref()` function.\\n\\n**Tests:**\\n\\n- **Name:** relationships_stg_yellow_tripdata_dropoff_locationid\\n  \\n  **Description:** Ensure `dropoff_location_id` exists in `taxi_zone_lookup.csv`\\n\\n  **Relationships:**\\n  - **To:** `ref(\\'taxi_zone_lookup.csv\\')`  # ❌ Wrong reference\\n\\n  - **Field:** `locationid`\\n\\n  - **To:** `ref(\\'taxi_zone_lookup\\')`  # ✅ Correct reference\\n\\n**Pandas and Spark Version Mismatch:**\\n\\nWhen running `df_spark = spark.createDataFrame(df_pandas)`, an error indicating a version mismatch between Pandas and Spark was encountered. To resolve this, either:\\n\\n1. Downgrade Pandas to a version below 2.\\n2. Upgrade Spark to version 3.5.5.\\n\\nI chose to upgrade Spark to 3.5.5, and it worked.\\n\\n**Avoiding Backpressure in Flink:**\\n\\n**What’s Backpressure?**\\n\\n- It occurs when Flink processes data slower than Kafka produces it.\\n- This leads to increased memory usage and can slow down or crash the job.\\n\\n**How to Fix It?**\\n\\n- Adjust Kafka’s consumer parallelism to match the producer rate.\\n- Increase partitions in Kafka to allow more parallel processing.\\n- Monitor Flink metrics to detect backpressure.\\n\\n```python\\nenv.set_parallelism(4)  # Adjust parallelism to avoid bottlenecks\\n```',\n",
       "  'filename': 'faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/013_7be69bdcb4_loading-dlt-exports-from-gcs-filesystems.md'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de_dtc_faq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f2092c3-762b-4815-8ee6-357b898f42ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "embedding_model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "099ec60f-2ac9-4c19-bfe4-9be1ac0e74ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "record = de_dtc_faq[2]\n",
    "text = record['question'] + ' ' + record['content']\n",
    "v_doc = embedding_model.encode(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f358f544-1b37-4d0f-ab23-2636e3553f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'I just found out about the course. Can I enroll now?'\n",
    "v_query = embedding_model.encode(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1078b21-8e71-4580-a820-21c75dbabcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = v_query.dot(v_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "218882b8-6549-48f3-a752-583ffa87e9de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6910df8a5ba94401a94ad0354a7e734c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/449 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "faq_embeddings = []\n",
    "\n",
    "for d in tqdm(de_dtc_faq):\n",
    "    text = d['question'] + ' ' + d['content']\n",
    "    v = embedding_model.encode(text)\n",
    "    faq_embeddings.append(v)\n",
    "\n",
    "faq_embeddings = np.array(faq_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b6aad63-4a65-40da-b318-abdb7f4322d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x169e50590>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import VectorSearch\n",
    "\n",
    "faq_vindex = VectorSearch()\n",
    "faq_vindex.fit(faq_embeddings, de_dtc_faq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f776f874-b5fd-4a88-ba7f-911dfc9fafa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "q = embedding_model.encode(query)\n",
    "results = faq_vindex.search(q)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "44d7f652-d96c-4641-893b-f27be478a554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01c3a45e42b45a29fff95aad979dbe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<minsearch.vector.VectorSearch at 0x15db9a180>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evidently_embeddings = []\n",
    "\n",
    "for d in tqdm(evidently_chunks):\n",
    "    v = embedding_model.encode(d['chunk'])\n",
    "    evidently_embeddings.append(v)\n",
    "\n",
    "evidently_embeddings = np.array(evidently_embeddings)\n",
    "\n",
    "evidently_vindex = VectorSearch()\n",
    "evidently_vindex.fit(evidently_embeddings, evidently_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c7ab70fa-6a46-4081-bcbc-585a5f23cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Can I join the course now?'\n",
    "\n",
    "text_results = faq_index.search(query, num_results=5)\n",
    "\n",
    "q = embedding_model.encode(query)\n",
    "vector_results = faq_vindex.search(q, num_results=5)\n",
    "\n",
    "final_results = text_results + vector_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b6d853d-f463-48f5-85cb-c2e7c1c336b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_search(query):\n",
    "    return faq_index.search(query, num_results=5)\n",
    "\n",
    "def vector_search(query):\n",
    "    q = embedding_model.encode(query)\n",
    "    return faq_vindex.search(q, num_results=5)\n",
    "\n",
    "def hybrid_search(query):\n",
    "    text_results = text_search(query)\n",
    "    vector_results = vector_search(query)\n",
    "    \n",
    "    # Combine and deduplicate results\n",
    "    seen_ids = set()\n",
    "    combined_results = []\n",
    "\n",
    "    for result in text_results + vector_results:\n",
    "        if result['filename'] not in seen_ids:\n",
    "            seen_ids.add(result['filename'])\n",
    "            combined_results.append(result)\n",
    "    \n",
    "    return combined_results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34030b90-093e-439c-bf8e-e1dedd5b0205",
   "metadata": {},
   "source": [
    "### Day 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "675747dc-9fec-4b84-aac7-57cbcb2ec001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It depends on the specific course and its enrollment policies. Many online courses allow for rolling admissions or ongoing enrollment, while others may have specific start dates or deadlines. Check the course website or contact the course administrator for more details.\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "openai_client = openai.OpenAI()\n",
    "\n",
    "user_prompt = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c71841a-06b6-4f24-b622-33ab0a60dbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"text_search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b08dc700-ed80-43df-9ad2-0d3d3da379d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\"\"\"\n",
    "\n",
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b80b147-4d55-438b-8492-4404e341a7fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"Can I join the course now?\"}', call_id='call_D8RfAyCQzTMiN2d9TK0FE07l', name='text_search', type='function_call', id='fc_028c19f23b60d6ac00694cae638c9881a184172d45f01275b5', status='completed')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9b275d2-354c-40ba-a2e6-fbd09941e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "call = response.output[0]\n",
    "\n",
    "arguments = json.loads(call.arguments)\n",
    "result = text_search(**arguments)\n",
    "\n",
    "call_output = {\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(result),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "001ec4c6-dc15-4856-a14f-970189364e28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even after the start date. You are eligible to submit homework assignments, but keep in mind that there will be deadlines for turning in homework and the final projects. It’s advisable not to leave everything until the last minute.\n",
      "\n",
      "If you're interested in joining, make sure to register and check the course details to stay updated.\n"
     ]
    }
   ],
   "source": [
    "chat_messages.append(call)\n",
    "chat_messages.append(call_output)\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[text_search_tool]\n",
    ")\n",
    "\n",
    "print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "aa2acb34-54fa-4bf0-96e1-1d5ff4deb081",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.\n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0deb7f11-5acd-47fb-9b14-756f3dd96000",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course. \n",
    "\n",
    "Always search for relevant information before answering. \n",
    "If the first search doesn't give you enough information, try different search terms.\n",
    "\n",
    "Make multiple searches if needed to provide comprehensive answers.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8c407b7a-8444-4134-8d60-9cd07ef90c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Any\n",
    "\n",
    "def text_search(query: str) -> List[Any]:\n",
    "    \"\"\"\n",
    "    Perform a text-based search on the FAQ index.\n",
    "\n",
    "    Args:\n",
    "        query (str): The search query string.\n",
    "\n",
    "    Returns:\n",
    "        List[Any]: A list of up to 5 search results returned by the FAQ index.\n",
    "    \"\"\"\n",
    "    return faq_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f0dc093c-54a8-45db-a97f-6d47783ba6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    name=\"faq_agent\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "82b3727e-efad-477e-93b5-2839209b7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I just discovered the course, can I join now?\"\n",
    "\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "633d0b73-d769-4440-b818-d7119ef5aefe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"Yes, you can still join the course even after it has started. Although you may not be officially registered, you are still eligible to submit homework assignments. However, keep in mind that there are deadlines for submitting homework and final projects, so it’s advisable not to postpone your work until the last minute. \\n\\nIf you're interested, the next cohort of the course will start on January 13th, 2025, and you can register for it [here](https://airtable.com/shr6oVXeQvSI5HuWD).\")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ca1052d1-ef22-4c56-877e-949143f8512a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ModelRequest(parts=[UserPromptPart(content='I just discovered the course, can I join now?', timestamp=datetime.datetime(2025, 12, 25, 3, 24, 43, 271819, tzinfo=datetime.timezone.utc))], timestamp=datetime.datetime(2025, 12, 25, 3, 24, 43, 272070, tzinfo=datetime.timezone.utc), instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\", run_id='523fc8aa-2b25-4020-a0b9-89a484022f4d'),\n",
       " ModelResponse(parts=[ToolCallPart(tool_name='text_search', args='{\"query\":\"can I join the course now?\"}', tool_call_id='call_BB6Q42vBGhMlNEE9XsVJzq1H')], usage=RequestUsage(input_tokens=146, output_tokens=20, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 12, 25, 3, 24, 45, 928702, tzinfo=datetime.timezone.utc), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'tool_calls', 'timestamp': datetime.datetime(2025, 12, 25, 3, 24, 44, tzinfo=TzInfo(0))}, provider_response_id='chatcmpl-CqWFwkBQTIcMbWfz5hIIgd7HT4OjI', finish_reason='tool_call', run_id='523fc8aa-2b25-4020-a0b9-89a484022f4d'),\n",
       " ModelRequest(parts=[ToolReturnPart(tool_name='text_search', content=[{'id': '3f1424af17', 'question': 'Course: Can I still join the course after the start date?', 'sort_order': 3, 'content': \"Yes, even if you don't register, you're still eligible to submit the homework.\\n\\nBe aware, however, that there will be deadlines for turning in homeworks and the final projects. So don't leave everything for the last minute.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/003_3f1424af17_course-can-i-still-join-the-course-after-the-start.md'}, {'id': '9e508f2212', 'question': 'Course: When does the course start?', 'sort_order': 1, 'content': \"The next cohort starts January 13th, 2025. More info at [DTC](https://datatalks.club/blog/guide-to-free-online-courses-at-datatalks-club.html).\\n\\n- Register before the course starts using this [link](https://airtable.com/shr6oVXeQvSI5HuWD).\\n- Join the [course Telegram channel with announcements](https://t.me/dezoomcamp).\\n- Don’t forget to register in DataTalks.Club's Slack and join the channel.\", 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/001_9e508f2212_course-when-does-the-course-start.md'}, {'id': '068529125b', 'question': 'Course - Can I follow the course after it finishes?', 'sort_order': 8, 'content': 'Yes, we will keep all the materials available, so you can follow the course at your own pace after it finishes.\\n\\nYou can also continue reviewing the homeworks and prepare for the next cohort. You can also start working on your final capstone project.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/008_068529125b_course-can-i-follow-the-course-after-it-finishes.md'}, {'id': '33fc260cd8', 'question': 'Course: What can I do before the course starts?', 'sort_order': 5, 'content': 'Start by installing and setting up all the dependencies and requirements:\\n\\n- Google Cloud account\\n- Google Cloud SDK\\n- Python 3 (installed with Anaconda)\\n- Terraform\\n- Git\\n\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/005_33fc260cd8_course-what-can-i-do-before-the-course-starts.md'}, {'id': 'c207b8614e', 'question': 'Course: Can I get support if I take the course in the self-paced mode?', 'sort_order': 9, 'content': 'Yes, the Slack channel remains open and you can ask questions there. However, always search the channel first and check the FAQ, as most likely your questions are already answered here.\\n\\nYou can also tag the bot `@ZoomcampQABot` to help you conduct the search, but don’t rely on its answers 100%.', 'filename': 'faq-main/_questions/data-engineering-zoomcamp/general/009_c207b8614e_course-can-i-get-support-if-i-take-the-course-in-t.md'}], tool_call_id='call_BB6Q42vBGhMlNEE9XsVJzq1H', timestamp=datetime.datetime(2025, 12, 25, 3, 24, 46, 78134, tzinfo=datetime.timezone.utc))], timestamp=datetime.datetime(2025, 12, 25, 3, 24, 46, 78499, tzinfo=datetime.timezone.utc), instructions=\"You are a helpful assistant for a course. \\n\\nAlways search for relevant information before answering. \\nIf the first search doesn't give you enough information, try different search terms.\\n\\nMake multiple searches if needed to provide comprehensive answers.\", run_id='523fc8aa-2b25-4020-a0b9-89a484022f4d'),\n",
       " ModelResponse(parts=[TextPart(content=\"Yes, you can still join the course even after it has started. Although you may not be officially registered, you are still eligible to submit homework assignments. However, keep in mind that there are deadlines for submitting homework and final projects, so it’s advisable not to postpone your work until the last minute. \\n\\nIf you're interested, the next cohort of the course will start on January 13th, 2025, and you can register for it [here](https://airtable.com/shr6oVXeQvSI5HuWD).\")], usage=RequestUsage(input_tokens=841, output_tokens=112, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}), model_name='gpt-4o-mini-2024-07-18', timestamp=datetime.datetime(2025, 12, 25, 3, 24, 51, 344356, tzinfo=datetime.timezone.utc), provider_name='openai', provider_url='https://api.openai.com/v1/', provider_details={'finish_reason': 'stop', 'timestamp': datetime.datetime(2025, 12, 25, 3, 24, 47, tzinfo=TzInfo(0))}, provider_response_id='chatcmpl-CqWFz3iNaPfENGlQ5bvVgwtG9XTwU', finish_reason='stop', run_id='523fc8aa-2b25-4020-a0b9-89a484022f4d')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.new_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a98fd41-775c-4f86-b624-3ea3f6710dfc",
   "metadata": {},
   "source": [
    "### Day 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e91e5aa-99c7-4f99-83f9-c0cc919f1146",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I install Kafka in Python?\"\n",
    "result = await agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "399c43cc-913c-4cd2-854e-1877a5798597",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentRunResult(output=\"To install Kafka for Python, you'll want to install specific libraries that enable you to interact with Kafka. Here are the recommended steps:\\n\\n1. **Install `confluent-kafka`:**\\n   You can use either `pip` or `conda` to install this library, which is a high-performance Kafka client for Python.\\n\\n   - Using pip:\\n   ```bash\\n   pip install confluent-kafka\\n   ```\\n\\n   - Using conda:\\n   ```bash\\n   conda install conda-forge::python-confluent-kafka\\n   ```\\n\\n2. **(Optional) Install `fastavro`:**\\n   If you're working with Avro data formats or want to optimize serialization/deserialization, you can also install `fastavro`.\\n\\n   ```bash\\n   pip install fastavro\\n   ```\\n\\n3. **Alternative Kafka client:** \\n   If you encounter issues with the `kafka-python` library, it's suggested to install `kafka-python-ng`:\\n\\n   ```bash\\n   pip install kafka-python-ng\\n   ```\\n\\nMake sure to choose the library that best suits your project needs!\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e54a35c-42d7-45fe-a56f-84de839d1ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai.messages import ModelMessagesTypeAdapter\n",
    "\n",
    "\n",
    "def log_entry(agent, messages, source=\"user\"):\n",
    "    tools = []\n",
    "\n",
    "    for ts in agent.toolsets:\n",
    "        tools.extend(ts.tools.keys())\n",
    "\n",
    "    dict_messages = ModelMessagesTypeAdapter.dump_python(messages)\n",
    "\n",
    "    return {\n",
    "        \"agent_name\": agent.name,\n",
    "        \"system_prompt\": agent._instructions,\n",
    "        \"provider\": agent.model.system,\n",
    "        \"model\": agent.model.model_name,\n",
    "        \"tools\": tools,\n",
    "        \"messages\": dict_messages,\n",
    "        \"source\": source\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "419cbb2f-7a78-456a-8b57-16fbc6d17738",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import secrets\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "LOG_DIR = Path('logs')\n",
    "LOG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "def serializer(obj):\n",
    "    if isinstance(obj, datetime):\n",
    "        return obj.isoformat()\n",
    "    raise TypeError(f\"Type {type(obj)} not serializable\")\n",
    "\n",
    "\n",
    "def log_interaction_to_file(agent, messages, source='user'):\n",
    "    entry = log_entry(agent, messages, source)\n",
    "\n",
    "    ts = entry['messages'][-1]['timestamp']\n",
    "    ts_str = ts.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    rand_hex = secrets.token_hex(3)\n",
    "\n",
    "    filename = f\"{agent.name}_{ts_str}_{rand_hex}.json\"\n",
    "    filepath = LOG_DIR / filename\n",
    "\n",
    "    with filepath.open(\"w\", encoding=\"utf-8\") as f_out:\n",
    "        json.dump(entry, f_out, indent=2, default=serializer)\n",
    "\n",
    "    return filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac1ad2c-31a2-4055-b4b7-b81031d56d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " what do I need to do for the certificate?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To obtain a certificate for the course, you need to complete the following requirements:\n",
      "\n",
      "1. **Peer-Reviewed Capstone Projects**: You must finish the peer-reviewed capstone projects on time. Completing the homeworks is not mandatory if you join late, as long as you submit the projects within the required timeframe.\n",
      "\n",
      "2. **Course Format**: You need to complete the course with a “live” cohort. Certificates are not awarded for completing the course in self-paced mode, since peer reviews can only occur while the course is actively running.\n",
      "\n",
      "3. **Certificate Generation**: After the course grading is completed, there will be an announcement on Telegram and in the course channel about:\n",
      "   - Checking that your full name is displayed correctly on the Certificate.\n",
      "   - Instructions on how to generate your certificate document yourself. You will find the certificate in your course profile.\n",
      "\n",
      "   Make sure you're logged in to access your course profile. The link for the 2025 edition is [https://courses.datatalks.club/de-zoomcamp-2025/enrollment](https://courses.datatalks.club/de-zoomcamp-2025/enrollment). Be sure to refer to your specific edition's link if you're in a different year.\n",
      "\n",
      "For more detailed information, you can read the official guidelines on generating certificates [here](https://github.com/DataTalksClub/data-engineering-zoomcamp/blob/main/certificates.md).\n",
      "\n",
      "References:\n",
      "- [Certificate: Do I need to do the homeworks to get the certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/014_3774a79c13_certificate-do-i-need-to-do-the-homeworks-to-get-t.md)\n",
      "- [Certificate - Can I follow the course in a self-paced mode and get a certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/015_900f60fd25_certificate-can-i-follow-the-course-in-a-self-pace.md)\n",
      "- [How do I get my certificate?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/046_6314bc3029_how-do-i-get-my-certificate.md)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/faq_agent_v2_20251225_032939_e451a8.json')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = input()\n",
    "result = await agent.run(user_prompt=question)\n",
    "print(result.output)\n",
    "log_interaction_to_file(agent, result.new_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2fcb74aa-cf3c-4ba4-82ce-c40175eb8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant for a course.  \n",
    "\n",
    "Use the search tool to find relevant information from the course materials before answering questions.  \n",
    "\n",
    "If you can find specific information through search, use it to provide accurate answers.\n",
    "\n",
    "Always include references by citing the filename of the source material you used.  \n",
    "When citing the reference, replace \"faq-main\" by the full path to the GitHub repository: \"https://github.com/DataTalksClub/faq/blob/main/\"\n",
    "Format: [LINK TITLE](FULL_GITHUB_LINK)\n",
    "\n",
    "If the search doesn't return relevant results, let the user know and provide general guidance.  \n",
    "\"\"\".strip()\n",
    "\n",
    "# Create another version of agent, let's call it faq_agent_v2\n",
    "agent = Agent(\n",
    "    name=\"faq_agent_v2\",\n",
    "    instructions=system_prompt,\n",
    "    tools=[text_search],\n",
    "    model='gpt-4o-mini'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7af2fdfe-3034-4588-9077-41fc93d5ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_prompt = \"\"\"\n",
    "Use this checklist to evaluate the quality of an AI agent's answer (<ANSWER>) to a user question (<QUESTION>).\n",
    "We also include the entire log (<LOG>) for analysis.\n",
    "\n",
    "For each item, check if the condition is met. \n",
    "\n",
    "Checklist:\n",
    "\n",
    "- instructions_follow: The agent followed the user's instructions (in <INSTRUCTIONS>)\n",
    "- instructions_avoid: The agent avoided doing things it was told not to do  \n",
    "- answer_relevant: The response directly addresses the user's question  \n",
    "- answer_clear: The answer is clear and correct  \n",
    "- answer_citations: The response includes proper citations or sources when required  \n",
    "- completeness: The response is complete and covers all key aspects of the request\n",
    "- tool_call_search: Is the search tool invoked? \n",
    "\n",
    "Output true/false for each check and provide a short explanation for your judgment.\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f6c87116-5c59-4692-b3fe-fcb96d93f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class EvaluationCheck(BaseModel):\n",
    "    check_name: str\n",
    "    justification: str\n",
    "    check_pass: bool\n",
    "\n",
    "class EvaluationChecklist(BaseModel):\n",
    "    checklist: list[EvaluationCheck]\n",
    "    summary: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ea5da3f-ddd8-4d7c-9d98-878eb5e80f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_agent = Agent(\n",
    "    name='eval_agent',\n",
    "    model='gpt-5-nano',\n",
    "    instructions=evaluation_prompt,\n",
    "    output_type=EvaluationChecklist\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "06e17fa5-4ab0-403b-81ce-2bbb6f62d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt_format = \"\"\"\n",
    "<INSTRUCTIONS>{instructions}</INSTRUCTIONS>\n",
    "<QUESTION>{question}</QUESTION>\n",
    "<ANSWER>{answer}</ANSWER>\n",
    "<LOG>{log}</LOG>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0a123163-0b5d-415c-97e1-b898c4e1ef39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_log_file(log_file):\n",
    "    with open(log_file, 'r') as f_in:\n",
    "        log_data = json.load(f_in)\n",
    "        log_data['log_file'] = log_file\n",
    "        return log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "895f4a41-f293-436f-8cea-3816de7d0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_record = load_log_file('./logs/faq_agent_v2_20251225_032927_164b47.json')\n",
    "\n",
    "instructions = log_record['system_prompt']\n",
    "question = log_record['messages'][0]['parts'][0]['content']\n",
    "answer = log_record['messages'][-1]['parts'][0]['content']\n",
    "log = json.dumps(log_record['messages'])\n",
    "\n",
    "user_prompt = user_prompt_format.format(\n",
    "    instructions=instructions,\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    log=log\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6a696530-18e9-453e-8e67-eec04afa94e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The answer is accurate, well-cited, and complete. It directly addresses joining late and receiving a certificate, includes required citation format, and points the user to relevant FAQ resources.\n",
      "check_name='instructions_follow' justification='The assistant followed the course instructions to search for relevant information and cite sources, and provided the required citations.' check_pass=True\n",
      "check_name='instructions_avoid' justification='No disallowed content or actions detected; answered within scope.' check_pass=True\n",
      "check_name='answer_relevant' justification='Response directly answers whether one can join late and get a certificate and under what conditions.' check_pass=True\n",
      "check_name='answer_clear' justification='The explanation is clear: you can join late if you complete peer-reviewed capstone on time; no homework requirement; includes steps to check announcements and where to read more.' check_pass=True\n",
      "check_name='answer_citations' justification='Citations are present with proper links and titles. They reference the correct files in the repository, formatted as [TITLE](LINK).' check_pass=True\n",
      "check_name='completeness' justification='Covers the main question and provides additional resources for how to get the certificate and related conditions; mentions self-paced mode usage in the linked resources list.' check_pass=True\n",
      "check_name='tool_call_search' justification='Search tool was used to locate relevant FAQ entries and the answer references those sources; indicated by the included links.' check_pass=True\n"
     ]
    }
   ],
   "source": [
    "result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "\n",
    "checklist = result.output\n",
    "print(checklist.summary)\n",
    "\n",
    "for check in checklist.checklist:\n",
    "    print(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1e2f2ef0-d86c-4217-bb6a-1930a44a06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_log_messages(messages):\n",
    "    log_simplified = []\n",
    "\n",
    "    for m in messages:\n",
    "        parts = []\n",
    "    \n",
    "        for original_part in m['parts']:\n",
    "            part = original_part.copy()\n",
    "            kind = part['part_kind']\n",
    "    \n",
    "            if kind == 'user-prompt':\n",
    "                del part['timestamp']\n",
    "            if kind == 'tool-call':\n",
    "                del part['tool_call_id']\n",
    "            if kind == 'tool-return':\n",
    "                del part['tool_call_id']\n",
    "                del part['metadata']\n",
    "                del part['timestamp']\n",
    "                # Replace actual search results with placeholder to save tokens\n",
    "                part['content'] = 'RETURN_RESULTS_REDACTED'\n",
    "            if kind == 'text':\n",
    "                del part['id']\n",
    "    \n",
    "            parts.append(part)\n",
    "    \n",
    "        message = {\n",
    "            'kind': m['kind'],\n",
    "            'parts': parts\n",
    "        }\n",
    "    \n",
    "        log_simplified.append(message)\n",
    "    return log_simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dffccf0c-cdb7-4bfe-93f6-a9114aa0e3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def evaluate_log_record(eval_agent, log_record):\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    instructions = log_record['system_prompt']\n",
    "    question = messages[0]['parts'][0]['content']\n",
    "    answer = messages[-1]['parts'][0]['content']\n",
    "\n",
    "    log_simplified = simplify_log_messages(messages)\n",
    "    log = json.dumps(log_simplified)\n",
    "\n",
    "    user_prompt = user_prompt_format.format(\n",
    "        instructions=instructions,\n",
    "        question=question,\n",
    "        answer=answer,\n",
    "        log=log\n",
    "    )\n",
    "\n",
    "    result = await eval_agent.run(user_prompt, output_type=EvaluationChecklist)\n",
    "    return result.output \n",
    "\n",
    "\n",
    "log_record = load_log_file('./logs/faq_agent_v2_20251225_032927_164b47.json')\n",
    "eval1 = await evaluate_log_record(eval_agent, log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4fd41549-0e0d-4975-ac7b-ed3e673a313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_generation_prompt = \"\"\"\n",
    "You are helping to create test questions for an AI agent that answers questions about a data engineering course.\n",
    "\n",
    "Based on the provided FAQ content, generate realistic questions that students might ask.\n",
    "\n",
    "The questions should:\n",
    "\n",
    "- Be natural and varied in style\n",
    "- Range from simple to complex\n",
    "- Include both specific technical questions and general course questions\n",
    "\n",
    "Generate one question for each record.\n",
    "\"\"\".strip()\n",
    "\n",
    "class QuestionsList(BaseModel):\n",
    "    questions: list[str]\n",
    "\n",
    "question_generator = Agent(\n",
    "    name=\"question_generator\",\n",
    "    instructions=question_generation_prompt,\n",
    "    model='gpt-4o-mini',\n",
    "    output_type=QuestionsList\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4eb2b616-49b9-4d83-848d-b7b562e3ffad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "sample = random.sample(de_dtc_faq, 10)\n",
    "prompt_docs = [d['content'] for d in sample]\n",
    "prompt = json.dumps(prompt_docs)\n",
    "\n",
    "result = await question_generator.run(prompt)\n",
    "questions = result.output.questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5507f66a-0383-4169-b3fb-148ae8caa4da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719b8439ec4740b19c18f7518733cdaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How can I authenticate my Spark application with Google Cloud Storage using credentials?\n",
      "To authenticate your Spark application with Google Cloud Storage (GCS) using credentials, you can follow these steps:\n",
      "\n",
      "1. **Create a Service Account and Obtain Key**:\n",
      "   - Go to the Google Cloud Console.\n",
      "   - Navigate to \"IAM & Admin\" > \"Service accounts\".\n",
      "   - Create a new service account and assign it the necessary roles (e.g., Storage Object Admin).\n",
      "   - Generate a JSON key for this service account.\n",
      "\n",
      "2. **Set Environment Variables**:\n",
      "   - You can set the environment variable `GOOGLE_APPLICATION_CREDENTIALS` to point to your service account key JSON file. This will allow your Spark application to locate the credentials for authentication.\n",
      "   ```bash\n",
      "   export GOOGLE_APPLICATION_CREDENTIALS=\"/path/to/your/service-account-file.json\"\n",
      "   ```\n",
      "\n",
      "3. **Use Spark Configuration** (if applicable):\n",
      "   - When initializing your Spark session, you might also need to set configuration properties to use GCS, especially if you are working in a distributed environment:\n",
      "   ```python\n",
      "   from pyspark.sql import SparkSession\n",
      "\n",
      "   spark = SparkSession.builder \\\n",
      "       .appName(\"YourAppName\") \\\n",
      "       .config(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\") \\\n",
      "       .config(\"fs.gs.auth.file\", \"/path/to/your/service-account-file.json\") \\\n",
      "       .getOrCreate()\n",
      "   ```\n",
      "\n",
      "Make sure to replace `/path/to/your/service-account-file.json` with the actual path to your service account key file.\n",
      "\n",
      "4. **Verify Credentials**:\n",
      "   - Ensure that the service account has the correct permissions to access the GCS bucket.\n",
      "   - You can test access by performing basic operations such as listing buckets or uploading files to ensure your authentication is set up correctly.\n",
      "\n",
      "For detailed steps on logging in and permissions, you might refer to documents that discuss handling permissions and logging into Google Cloud services ([source](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/047_fdef2dfcc4_lserviceexception-401-anonymous-caller-does-not-ha.md)). \n",
      "\n",
      "If you require further customization or if you're deploying in specific environments, consider using configuration files or environment variables as mentioned in the general setup guidelines ([source](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/workshop-1-dlthub/009_d2add610e3_how-do-i-set-up-credentials-to-run-dlt-in-my-envir.md)).\n",
      "\n",
      "What steps should I follow to create an SSH config file in WSL2?\n",
      "To create an SSH config file in WSL2, you can follow these steps:\n",
      "\n",
      "1. **Open WSL2**: Start your WSL2 terminal.\n",
      "\n",
      "2. **Navigate to Your Home Directory**: \n",
      "   ```bash\n",
      "   cd ~\n",
      "   ```\n",
      "   \n",
      "3. **Create the `.ssh` Directory**: If the directory does not exist, create it using:\n",
      "   ```bash\n",
      "   mkdir .ssh\n",
      "   ```\n",
      "\n",
      "4. **Create the `config` File**: Navigate into the `.ssh` directory and create a `config` file:\n",
      "   ```bash\n",
      "   cd .ssh\n",
      "   touch config\n",
      "   ```\n",
      "\n",
      "5. **Edit the `config` File**: Open the `config` file in a text editor, such as nano:\n",
      "   ```bash\n",
      "   nano config\n",
      "   ```\n",
      "\n",
      "6. **Add Your SSH Configuration**: In the `config` file, you need to specify the connection details. Here’s a sample configuration you might include:\n",
      "   ```\n",
      "   Host [alias]\n",
      "       HostName [GPC VM external IP]\n",
      "       User [username]\n",
      "       IdentityFile ~/.ssh/[private key]\n",
      "   ```\n",
      "   Replace `[alias]`, `[GPC VM external IP]`, `[username]`, and `[private key]` with your actual details.\n",
      "\n",
      "7. **Save and Exit**: If you are using nano, you can save the changes by pressing `CTRL + X`, then `Y`, and `Enter`.\n",
      "\n",
      "8. **Set Proper Permissions**: Ensure your SSH folder and files have appropriate permissions:\n",
      "   ```bash\n",
      "   chmod 700 ~/.ssh\n",
      "   chmod 600 ~/.ssh/config\n",
      "   ```\n",
      "\n",
      "This setup allows you to easily connect to your SSH servers using the details specified in your `config` file.\n",
      "\n",
      "For reference, this process is outlined in the course material, specifically in this file: [WSL: Could not resolve host name](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/058_16d0d756c3_wsl-could-not-resolve-host-name.md).\n",
      "\n",
      "Since the Faust library is deprecated, what resources can I use for learning streaming concepts in Python?\n",
      "Here are some recommended resources for learning about streaming concepts in Python, especially after the deprecation of the Faust library:\n",
      "\n",
      "1. **Python Generators**:\n",
      "   - Generators are an essential feature in Python for handling streaming data. They allow for memory-efficient iteration, which is crucial when working with large datasets. You can learn about generators in Python in more detail [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-3/042_0f2a26772e_python-generators-in-python.md).\n",
      "\n",
      "2. **Kafka in Python**:\n",
      "   - You can set up streaming with Kafka using Python libraries. To get started, you can follow the instructions for installing necessary dependencies like `confluent-kafka` and `fastavro` for Kafka-based streaming. More information is available [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-6/021_2763850d3e_python-kafka-installing-dependencies-for-python3-0.md).\n",
      "\n",
      "3. **Spark Streaming**:\n",
      "   - If you're interested in using Apache Spark for streaming, you can learn how to read from multiple topics in Spark Streaming. The process includes initiating a Spark session and using the `readStream` method to handle data streams. Details can be found [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/project/007_ce77e05d24_spark-streaming-how-do-i-read-from-multiple-topics.md).\n",
      "\n",
      "4. **Alternative to Faust**:\n",
      "   - Since Faust is deprecated, an alternative approach is suggested through the use of RedPanda, which is a Kafka API-compatible streaming platform. You can check out an example of using RedPanda with Python [here](https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example) as well as watch video resources on streaming concepts in Python [here](https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80).\n",
      "\n",
      "These resources should give you a solid foundation to start learning streaming concepts in Python effectively.\n",
      "\n",
      "What should I do if I encounter an error when restarting a Docker container with a network name?\n",
      "If you encounter an error when restarting a Docker container that is configured with a network name, follow these steps to troubleshoot and resolve the issue:\n",
      "\n",
      "1. **Identify the running container**: First, check if the container is still running:\n",
      "   ```bash\n",
      "   docker ps\n",
      "   ```\n",
      "\n",
      "2. **Stop the container**: If the container is in a running state, you may need to stop it before restarting:\n",
      "   ```bash\n",
      "   docker stop <container_name>\n",
      "   ```\n",
      "\n",
      "3. **Remove the container**: If the error persists, you might need to remove the container and recreate it:\n",
      "   ```bash\n",
      "   docker rm <container_name>\n",
      "   ```\n",
      "\n",
      "4. **Restart the container**: Instead of using `docker run`, consider using `docker start` if you want to restart a stopped container without creating a new one:\n",
      "   ```bash\n",
      "   docker start <container_name>\n",
      "   ```\n",
      "\n",
      "5. **Check the network settings**: Make sure that the container is connected to the correct Docker network. You can use:\n",
      "   ```bash\n",
      "   docker network ls\n",
      "   ```\n",
      "   to list the available networks and verify that the expected network is present.\n",
      "\n",
      "6. **Use correct network name**: If you are using `docker-compose`, ensure that you are using the correct network name as defined in the `docker-compose.yaml` file. If a network name isn’t specified in the compose file, Docker automatically generates it based on the directory name where the file is located followed by `_default`.\n",
      "\n",
      "By following these steps, you should be able to address most issues relating to errors when restarting Docker containers associated with network names.\n",
      "\n",
      "For more detailed instructions and troubleshooting measures, you can refer to the following resources:\n",
      "- [Docker network ls documentation](https://docs.docker.com/engine/reference/commandline/network_ls/) - from the file `docker-docker-network-name.md` [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/029_79e44b9c07_docker-docker-network-name.md).\n",
      "- More on Docker container conflicts can be found in `docker-error-response-from-daemon-conflict-the-con.md` [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-1/030_381dfe5145_docker-error-response-from-daemon-conflict-the-con.md).\n",
      "\n",
      "Is it possible to partition a BigQuery table by more than one column?\n",
      "No, BigQuery does not support partitioning a table by more than one column. According to the documentation, you can only partition a table by a single column in BigQuery. \n",
      "\n",
      "For more information, you can refer to the [source documentation](https://cloud.google.com/bigquery/docs/partitioned-tables#limitations) [here](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-3/025_45b587e597_gcp-bq-does-bigquery-support-multiple-columns-part.md).\n",
      "\n",
      "What should I do if my YAML file isn't working as expected?\n",
      "If your YAML file isn't working as expected, there are a few common troubleshooting steps you can take:\n",
      "\n",
      "1. **Check Syntax**: Ensure that your YAML file has the correct syntax. YAML is sensitive to indentation and formatting, so make sure you are using spaces (not tabs) for indentation.\n",
      "\n",
      "2. **Validate the YAML**: Use a YAML validator tool online to check for any syntax errors. This can help identify issues that you might not see at a glance.\n",
      "\n",
      "3. **Check Specific Configurations**: If you’re working with a specific framework (like Docker or dbt), confirm that all required configurations are included. For example:\n",
      "   - In a Docker Compose file, ensure you have defined all the necessary services, volumes, and networks correctly. If your service refers to an undefined volume, you might see errors; you can add the volume definition under the `volumes` section.\n",
      "\n",
      "   Example:\n",
      "   ```yaml\n",
      "   volumes:\n",
      "     dtc_postgres_volume_local:\n",
      "   ```\n",
      "\n",
      "   [Reference for Docker Volumes](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-1/051_5f056e236c_docker-compose-error-undefined-volume-in-windowsws.md)\n",
      "\n",
      "4. **Check Logs for Errors**: If you're using a tool that logs errors (like dbt or Docker), check the error messages closely as they can point you toward the problem. For instance, dbt provides specific links in the error log that show which query has issues.\n",
      "\n",
      "   [Reference for dbt Troubleshooting](https://github.com/DataTalksClub/faq/blob/main/_questions/data-engineering-zoomcamp/module-4/040_2d05496ddf_troubleshooting-in-dbt.md)\n",
      "\n",
      "5. **Test Incrementally**: If possible, start with a minimal configuration that works, and incrementally add complexity back into the YAML file. This can help isolate the problematic section.\n",
      "\n",
      "If you're still having issues after trying these steps, please provide more details about the specific error messages or behaviors you're encountering.\n",
      "\n",
      "What memory settings should I use in Spark if I encounter memory allocation errors?\n",
      "If you encounter memory allocation errors in Spark, like exceeding total allocation limits, you can address these issues by adjusting the memory settings for your Spark session. Here are recommended settings to do so:\n",
      "\n",
      "1. **Increase Executor and Driver Memory**: When creating your Spark session, you can increase the memory allocated to the executor and driver. For example, you can set both to 4GB:\n",
      "\n",
      "   ```python\n",
      "   spark = SparkSession.builder \\\n",
      "       .master(\"local[*]\") \\\n",
      "       .appName('test') \\\n",
      "       .config(\"spark.executor.memory\", \"4g\") \\\n",
      "       .config(\"spark.driver.memory\", \"4g\") \\\n",
      "       .getOrCreate()\n",
      "   ```\n",
      "\n",
      "   Adjust these values based on your available resources and the requirements of your application. \n",
      "\n",
      "2. **Ensure Off-Heap Memory is Configured**: You might also consider enabling off-heap memory if your application requires more memory than is available:\n",
      "\n",
      "   ```python\n",
      "   .config(\"spark.memory.offHeap.enabled\", True) \\\n",
      "   .config(\"spark.memory.offHeap.size\", \"5g\") \\\n",
      "   ```\n",
      "\n",
      "3. **Restart the Environment**: After making these changes, be sure to restart your Spark environment (or Jupyter session) to allow the new configurations to take effect.\n",
      "\n",
      "Implementing these adjustments should help mitigate memory allocation errors in Spark. Always tailor the memory settings to match your actual use case and the resources available to you.\n",
      "\n",
      "Reference: [MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/039_14cc6e2060_memorymanager-total-allocation-exceeds-9500-102005.md)\n",
      "\n",
      "Do I need to have a paid account to use Google Cloud services for this course?\n",
      "You do not need to have a paid account to use Google Cloud services for this course; you can take advantage of their free trial. The free trial allows you to explore GCP's features without any initial costs, making it suitable for your educational needs. \n",
      "\n",
      "For additional details, you can refer to the content from the course material: [Should I pay for cloud services?](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/general/028_e7738f47c8_should-i-pay-for-cloud-services.md)\n",
      "\n",
      "What are some possible solutions if I am facing issues with the `ehail_fee` column in Parquet files?\n",
      "If you are facing issues with the `ehail_fee` column in Parquet files, there are several possible solutions that you can consider:\n",
      "\n",
      "1. **Drop the `ehail_fee` Column**: If the column is not needed, you can simply exclude it from your queries. For example, when creating a partitioned table from an external table in BigQuery, use:\n",
      "   ```sql\n",
      "   SELECT * EXCEPT (ehail_fee) FROM...\n",
      "   ```\n",
      "\n",
      "2. **Modify SQL Model**: You can adjust your SQL model to cast the `ehail_fee` column to a numeric type. For example:\n",
      "   ```sql\n",
      "   CAST(0 AS NUMERIC) AS ehail_fee\n",
      "   ```\n",
      "\n",
      "3. **Modify the Airflow DAG**: If you are using Airflow for data processing, you can modify the Data Access Gateway (DAG) to handle type conversions, like this:\n",
      "   ```python\n",
      "   pv.read_csv(src_file, convert_options=pv.ConvertOptions(column_types={'ehail_fee': 'float64'}))\n",
      "   ```\n",
      "\n",
      "4. **Using Pandas**: When importing your CSV into a DataFrame, you can specify the data types, ensuring that null values can be handled properly. Here's an example:\n",
      "   ```python\n",
      "   pd.from_csv(..., dtype=type_dict)\n",
      "   ```\n",
      "   Make sure to use Pandas' nullable integer type (`Int64`) if you expect any NaN values.\n",
      "\n",
      "5. **DBT Modification**: If using DBT (Data Build Tool), you can replace the original `ehail_fee` column in your transformations to ensure that the conversion aligns with expected types:\n",
      "   ```sql\n",
      "   {{ dbt.safe_cast('ehail_fee', api.Column.translate_type(\"numeric\")) }} as ehail_fee,\n",
      "   ```\n",
      "\n",
      "These solutions can help you address the type mismatch and other issues related to the `ehail_fee` column in your Parquet files. \n",
      "\n",
      "For further details and examples, you can refer to the following sources:  \n",
      "- [DBT: Error on building fact_trips.sql - Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/056_0fbc18e0f2_dbt-error-on-building-fact_tripssql-parquet-column.md)  \n",
      "- [Parquet: “Parquet column 'ehail_fee' has type DOUBLE which does not match the target cpp_type INT64”](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-4/013_45bd267149_parquet-parquet-column-ehail_fee-has-type-double-w.md)\n",
      "\n",
      "How do I properly use `repartition` and `coalesce` in Spark for writing Parquet files?\n",
      "To properly use `repartition` and `coalesce` in Spark for writing Parquet files, here's how you can apply both functions effectively:\n",
      "\n",
      "### Repartition and Coalesce\n",
      "\n",
      "1. **Repartition:** Use `repartition` when you want to increase the number of partitions. It will perform a full shuffle of the data. For example:\n",
      "   ```python\n",
      "   df = df.repartition(6)\n",
      "   ```\n",
      "\n",
      "2. **Coalesce:** Use `coalesce` when you want to decrease the number of partitions, particularly when reducing the number of partitions is needed to avoid excessive shuffling, which improves write performance. For example:\n",
      "   ```python\n",
      "   df = df.coalesce(6)\n",
      "   ```\n",
      "\n",
      "3. **Writing to Parquet:** After setting the number of partitions, you can write the DataFrame to Parquet like this:\n",
      "   ```python\n",
      "   df.write.parquet('fhv/2019/10', mode='overwrite')\n",
      "   ```\n",
      "\n",
      "### Example Code\n",
      "\n",
      "Here's how you can combine this in practice:\n",
      "\n",
      "```python\n",
      "df = ...  # Your Spark DataFrame\n",
      "\n",
      "# Optionally repartition before filtering or transformation if data volume is large\n",
      "df = df.repartition(6)  # Increase partitions if necessary\n",
      "\n",
      "# After processing the DataFrame (e.g., filtering, transformation)\n",
      "df = df.coalesce(6)  # Reduce partitions\n",
      "\n",
      "# Write the DataFrame to Parquet\n",
      "df.write.parquet('fhv/2019/10', mode='overwrite')\n",
      "```\n",
      "\n",
      "### Reference\n",
      "This approach is aligned with best practices for performance when writing Parquet files in Spark, as mentioned in the course materials. For more detailed information, you can refer to this [resource](https://github.com/DataTalksClub/faq/blob/main/faq-main/_questions/data-engineering-zoomcamp/module-5/049_fa58733a98_repartition-the-dataframe-to-6-partitions-using-df.md).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "for q in tqdm(questions):\n",
    "    print(q)\n",
    "\n",
    "    result = await agent.run(user_prompt=q)\n",
    "    print(result.output)\n",
    "\n",
    "    log_interaction_to_file(\n",
    "        agent,\n",
    "        result.new_messages(),\n",
    "        source='ai-generated'\n",
    "    )\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "741f3e1d-4e97-405b-979c-f2f17929ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_set = []\n",
    "\n",
    "for log_file in LOG_DIR.glob('*.json'):\n",
    "    if 'faq_agent_v2' not in log_file.name:\n",
    "        continue\n",
    "\n",
    "    log_record = load_log_file(log_file)\n",
    "    if log_record['source'] != 'ai-generated':\n",
    "        continue\n",
    "\n",
    "    eval_set.append(log_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0ef77dd4-9b78-465f-97f9-e4aed574f935",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761d895ac5574210a3158895d7b8e309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "eval_results = []\n",
    "\n",
    "for log_record in tqdm(eval_set):\n",
    "    eval_result = await evaluate_log_record(eval_agent, log_record)\n",
    "    eval_results.append((log_record, eval_result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ede80395-4af9-421c-94f1-6c256846bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "\n",
    "for log_record, eval_result in eval_results:\n",
    "    messages = log_record['messages']\n",
    "\n",
    "    row = {\n",
    "        'file': log_record['log_file'].name,\n",
    "        'question': messages[0]['parts'][0]['content'],\n",
    "        'answer': messages[-1]['parts'][0]['content'],\n",
    "    }\n",
    "\n",
    "    checks = {c.check_name: c.check_pass for c in eval_result.checklist}\n",
    "    row.update(checks)\n",
    "\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5c2a9464-bc6d-403e-ae87-547d6fac6fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_evals = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6bf9c194-4cbc-44ca-b4b7-36d5d9fab9b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "instructions_follow    1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_evals.mean(numeric_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "17b303b2-e19c-4e2a-9473-24db8f5b1794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_search_quality(search_function, test_queries):\n",
    "    results = []\n",
    "    \n",
    "    for query, expected_docs in test_queries:\n",
    "        search_results = search_function(query, num_results=5)\n",
    "        \n",
    "        # Calculate hit rate\n",
    "        relevant_found = any(doc['filename'] in expected_docs for doc in search_results)\n",
    "        \n",
    "        # Calculate MRR\n",
    "        for i, doc in enumerate(search_results):\n",
    "            if doc['filename'] in expected_docs:\n",
    "                mrr = 1 / (i + 1)\n",
    "                break\n",
    "        else:\n",
    "            mrr = 0\n",
    "            \n",
    "        results.append({\n",
    "            'query': query,\n",
    "            'hit': relevant_found,\n",
    "            'mrr': mrr\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a2417b-0fac-47c9-ba26-03d248e6d9ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
